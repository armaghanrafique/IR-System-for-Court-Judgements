{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbbd6c79-ceb2-4a62-b495-9968ed2680ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ” WILDCARD QUERY SEARCH SYSTEM\n",
      "================================================================================\n",
      "\n",
      "ğŸ“ Corpus folder: C:\\Users\\Armaghan Rafique\\Desktop\\AI Project\\cleaned_corpus\n",
      "ğŸ“‚ Loading index from: C:\\Users\\Armaghan Rafique\\Desktop\\AI Project\\cleaned_corpus\\wildcard_index\\wildcard_index.pkl\n",
      "âœ… Wildcard index loaded successfully!\n",
      "   Documents: 1,460\n",
      "   Unique terms: 15,240\n",
      "   K-gram size: 3\n",
      "\n",
      "ğŸ¯ WILDCARD SEARCH READY\n",
      "   Documents: 1,460\n",
      "   Unique terms: 15,240\n",
      "\n",
      "================================================================================\n",
      "ğŸ§ª QUICK TEST EXAMPLES\n",
      "================================================================================\n",
      "\n",
      "ğŸ” Testing: 'court*'\n",
      "\n",
      "ğŸ” Wildcard search: 'court*'\n",
      "   Method: k-gram indexing\n",
      "   âœ“ Found 1 matching term(s)\n",
      "   Terms: court\n",
      "   â±ï¸  Search time: 0.000s\n",
      "   ğŸ“„ Documents found: 346\n",
      "   âœ… Found 346 documents\n",
      "   Sample terms: ['court']\n",
      "\n",
      "ğŸ” Testing: '*evidence'\n",
      "\n",
      "ğŸ” Wildcard search: '*evidence'\n",
      "   Method: k-gram indexing\n",
      "   âœ“ Found 1 matching term(s)\n",
      "   Terms: evidence\n",
      "   â±ï¸  Search time: 0.000s\n",
      "   ğŸ“„ Documents found: 19\n",
      "   âœ… Found 19 documents\n",
      "   Sample terms: ['evidence']\n",
      "\n",
      "================================================================================\n",
      "ğŸ” WILDCARD SEARCH SYSTEM\n",
      "================================================================================\n",
      "\n",
      "ğŸ“‹ WILDCARD SYNTAX:\n",
      "  â€¢ *        = Matches any sequence of characters\n",
      "  â€¢ ?        = Matches any single character\n",
      "  â€¢ term*    = Terms starting with 'term'\n",
      "  â€¢ *term    = Terms ending with 'term'\n",
      "  â€¢ *term*   = Terms containing 'term'\n",
      "  â€¢ c?urt    = Terms like 'court', 'curt', etc.\n",
      "  â€¢ m*rder   = Terms like 'murder', 'morder', etc.\n",
      "\n",
      "ğŸ“‹ AVAILABLE COMMANDS:\n",
      "  â€¢ search <pattern>    - Wildcard search\n",
      "  â€¢ prefix <text>       - Prefix search (text*)\n",
      "  â€¢ suffix <text>       - Suffix search (*text)\n",
      "  â€¢ contains <text>     - Contains search (*text*)\n",
      "  â€¢ phrase <pattern>    - Phrase with wildcards\n",
      "  â€¢ similar <term>      - Find similar terms\n",
      "  â€¢ terms [pattern]     - Show term statistics\n",
      "  â€¢ stats               - Show index statistics\n",
      "  â€¢ example             - Show examples\n",
      "  â€¢ quit                - Exit\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ¯ Enter command:  search murd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” Wildcard search: 'murd'\n",
      "   Method: k-gram indexing\n",
      "   âœ— No matching terms found\n",
      "   â±ï¸  Search time: 0.000s\n",
      "\n",
      "âŒ No results found\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ¯ Enter command:  search\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” Wildcard search: 'search'\n",
      "   Method: k-gram indexing\n",
      "   âœ“ Found 1 matching term(s)\n",
      "   Terms: search\n",
      "   â±ï¸  Search time: 0.000s\n",
      "   ğŸ“„ Documents found: 32\n",
      "\n",
      "âœ… Search: 'search'\n",
      "ğŸ“Š Found 32 document(s)\n",
      "================================================================================\n",
      "\n",
      " 1. ğŸ“„ Cr_PLA_No_022015_Ashrat.txt\n",
      "    ğŸ“ Length: 413 tokens\n",
      "    â­ Score: 30.0\n",
      "    ğŸ” Matched terms: search\n",
      "    ğŸ“ˆ Terms matched: 1\n",
      "\n",
      " 2. ğŸ“„ Cr_Appeal_No_032011_in_CrPLA_No_102010_Naveed_Hussain_versus_the_State.txt\n",
      "    ğŸ“ Length: 1,650 tokens\n",
      "    â­ Score: 30.0\n",
      "    ğŸ” Matched terms: search\n",
      "    ğŸ“ˆ Terms matched: 1\n",
      "\n",
      " 3. ğŸ“„ Provincial_Government_Vs_Mst_Masnona_Shezadi_Provincial_20Govt_20Vr_20Mst._20Masnona.txt\n",
      "    ğŸ“ Length: 442 tokens\n",
      "    â­ Score: 30.0\n",
      "    ğŸ” Matched terms: search\n",
      "    ğŸ“ˆ Terms matched: 1\n",
      "\n",
      " 4. ğŸ“„ Cr_Misc_No072010_assistant_collector_custom_vs_syed_m._riaz.txt\n",
      "    ğŸ“ Length: 793 tokens\n",
      "    â­ Score: 30.0\n",
      "    ğŸ” Matched terms: search\n",
      "    ğŸ“ˆ Terms matched: 1\n",
      "\n",
      " 5. ğŸ“„ Cr_Appeal_No_022015_in_CrPLA_No_062013_Sher_20Wali_20versus_20Sakhawat_20Dar1.txt\n",
      "    ğŸ“ Length: 897 tokens\n",
      "    â­ Score: 30.0\n",
      "    ğŸ” Matched terms: search\n",
      "    ğŸ“ˆ Terms matched: 1\n",
      "\n",
      " 6. ğŸ“„ The_State_Versus_Ashfaq_Hussain_another_The_20State_20versus_20Ashfaq_20Hussain_20__20another_20Bail_20matter.txt\n",
      "    ğŸ“ Length: 357 tokens\n",
      "    â­ Score: 30.0\n",
      "    ğŸ” Matched terms: search\n",
      "    ğŸ“ˆ Terms matched: 1\n",
      "\n",
      " 7. ğŸ“„ Cr_Appeal_No_062017_in_Cr_PLA_No_402016_The_20State_20through_20NAF_20versus_20Ishaq_20AHmed.txt\n",
      "    ğŸ“ Length: 901 tokens\n",
      "    â­ Score: 30.0\n",
      "    ğŸ” Matched terms: search\n",
      "    ğŸ“ˆ Terms matched: 1\n",
      "\n",
      " 8. ğŸ“„ The_State_Versus_Sufi_Ali_so_Abdul_Karim_and_othe__10_._Sufi_Ali_.txt\n",
      "    ğŸ“ Length: 1,803 tokens\n",
      "    â­ Score: 30.0\n",
      "    ğŸ” Matched terms: search\n",
      "    ğŸ“ˆ Terms matched: 1\n",
      "\n",
      " 9. ğŸ“„ Assistant_Collector_Custom_Sust_Gojal_Versus_Syed_assistant_collector_custom_vs_syed_m._riaz.txt\n",
      "    ğŸ“ Length: 793 tokens\n",
      "    â­ Score: 30.0\n",
      "    ğŸ” Matched terms: search\n",
      "    ğŸ“ˆ Terms matched: 1\n",
      "\n",
      "10. ğŸ“„ Cr_PLA_No_312016_DFO_20Wildlife_20Gilgit_20versus_20Hashim_20Wali_20leave_20refused.txt\n",
      "    ğŸ“ Length: 217 tokens\n",
      "    â­ Score: 30.0\n",
      "    ğŸ” Matched terms: search\n",
      "    ğŸ“ˆ Terms matched: 1\n",
      "\n",
      "11. ğŸ“„ 2025LHC7732.txt\n",
      "    ğŸ“ Length: 1,143 tokens\n",
      "    â­ Score: 30.0\n",
      "    ğŸ” Matched terms: search\n",
      "    ğŸ“ˆ Terms matched: 1\n",
      "\n",
      "12. ğŸ“„ The_State_through_ANF_Versus_Ishaq_Ahmed_The_20State_20through_20NAF_20versus_20Ishaq_20AHmed.txt\n",
      "    ğŸ“ Length: 901 tokens\n",
      "    â­ Score: 30.0\n",
      "    ğŸ” Matched terms: search\n",
      "    ğŸ“ˆ Terms matched: 1\n",
      "\n",
      "13. ğŸ“„ Provincial_Government_through_Chief_Secretary_ot_Provincial_Govt_Vr_Mst._Masnona.txt\n",
      "    ğŸ“ Length: 438 tokens\n",
      "    â­ Score: 30.0\n",
      "    ğŸ” Matched terms: search\n",
      "    ğŸ“ˆ Terms matched: 1\n",
      "\n",
      "14. ğŸ“„ Naveed_Hussain_Versus_The_State_Naveed_Hussain_versus_the_State.txt\n",
      "    ğŸ“ Length: 1,650 tokens\n",
      "    â­ Score: 30.0\n",
      "    ğŸ” Matched terms: search\n",
      "    ğŸ“ˆ Terms matched: 1\n",
      "\n",
      "15. ğŸ“„ DFO_Gilgit_Versus_Hashim_Wali_DFO_20Wildlife_20Gilgit_20versus_20Hashim_20Wali_20leave_20refused.txt\n",
      "    ğŸ“ Length: 217 tokens\n",
      "    â­ Score: 30.0\n",
      "    ğŸ” Matched terms: search\n",
      "    ğŸ“ˆ Terms matched: 1\n",
      "\n",
      "... and 17 more documents\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“– Preview a document? (enter number or 'n'):  *murder*\n",
      "\n",
      "ğŸ¯ Enter command:  court*\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” Wildcard search: 'court*'\n",
      "   Method: k-gram indexing\n",
      "   âœ“ Found 1 matching term(s)\n",
      "   Terms: court\n",
      "   â±ï¸  Search time: 0.000s\n",
      "   ğŸ“„ Documents found: 346\n",
      "\n",
      "âœ… Search: 'court*'\n",
      "ğŸ“Š Found 346 document(s)\n",
      "================================================================================\n",
      "\n",
      " 1. ğŸ“„ CPLA_Under_Objection_No1452019_Provincial_Govt_17._20judgment_20of_20mehboob_20hussain.txt\n",
      "    ğŸ“ Length: 948 tokens\n",
      "    â­ Score: 10.0\n",
      "    ğŸ” Matched terms: court\n",
      "    ğŸ“ˆ Terms matched: 1\n",
      "\n",
      " 2. ğŸ“„ Mst_Roqia_Begum_and_others_versus_Safdar_Ali_and_Mst._20Roqia_20Begum_20versus_20Safdar_20Ali.txt\n",
      "    ğŸ“ Length: 1,037 tokens\n",
      "    â­ Score: 10.0\n",
      "    ğŸ” Matched terms: court\n",
      "    ğŸ“ˆ Terms matched: 1\n",
      "\n",
      " 3. ğŸ“„ Cr_Appeal_No_082017_in_Cr_PLA_No_312015_the_20State_20versus_20Aziz-ur-Rehman_20bail_20granted.txt\n",
      "    ğŸ“ Length: 430 tokens\n",
      "    â­ Score: 10.0\n",
      "    ğŸ” Matched terms: court\n",
      "    ğŸ“ˆ Terms matched: 1\n",
      "\n",
      " 4. ğŸ“„ Civil_Review_No_122017_in_Civil_Appeal_No_6620_Abdul_20Murad_20Khan_20vs_20RO_20of_20Gojalti_20Review.txt\n",
      "    ğŸ“ Length: 73 tokens\n",
      "    â­ Score: 10.0\n",
      "    ğŸ” Matched terms: court\n",
      "    ğŸ“ˆ Terms matched: 1\n",
      "\n",
      " 5. ğŸ“„ Provincial_Government_others_Versus_Muhammad_Gul_Prov._20Govt_20versus_20Muhammad_20Gul.txt\n",
      "    ğŸ“ Length: 456 tokens\n",
      "    â­ Score: 10.0\n",
      "    ğŸ” Matched terms: court\n",
      "    ğŸ“ˆ Terms matched: 1\n",
      "\n",
      " 6. ğŸ“„ Abbas_and_others_versus_Raza_others_Abbas_20__20others_20versus_20Raza_20__20others_20SKD_20Branch.txt\n",
      "    ğŸ“ Length: 500 tokens\n",
      "    â­ Score: 10.0\n",
      "    ğŸ” Matched terms: court\n",
      "    ğŸ“ˆ Terms matched: 1\n",
      "\n",
      " 7. ğŸ“„ CPLA_NO182010_All_20Residents_20of_20Mausa_20Shella.txt\n",
      "    ğŸ“ Length: 398 tokens\n",
      "    â­ Score: 10.0\n",
      "    ğŸ” Matched terms: court\n",
      "    ğŸ“ˆ Terms matched: 1\n",
      "\n",
      " 8. ğŸ“„ CPLA_No502011_Pronvincial_20Government_20Vs_20Faqir_20Shah.txt\n",
      "    ğŸ“ Length: 515 tokens\n",
      "    â­ Score: 10.0\n",
      "    ğŸ” Matched terms: court\n",
      "    ğŸ“ˆ Terms matched: 1\n",
      "\n",
      " 9. ğŸ“„ Syed_Faiz_ALi_Shah_versus_Iqbal_Aman_and_others_Syed_20Faiz_20ALi_20Shah-.txt\n",
      "    ğŸ“ Length: 449 tokens\n",
      "    â­ Score: 10.0\n",
      "    ğŸ” Matched terms: court\n",
      "    ğŸ“ˆ Terms matched: 1\n",
      "\n",
      "10. ğŸ“„ SMC_No_042013_smc_20no._204-2013.txt\n",
      "    ğŸ“ Length: 536 tokens\n",
      "    â­ Score: 10.0\n",
      "    ğŸ” Matched terms: court\n",
      "    ğŸ“ˆ Terms matched: 1\n",
      "\n",
      "11. ğŸ“„ Civil_Appeal_No_092018_in_CPAL_No_11_2017_Ali_20Gohar_20through_20LRs_20vs_20Yousuf_20Ali.txt\n",
      "    ğŸ“ Length: 432 tokens\n",
      "    â­ Score: 10.0\n",
      "    ğŸ” Matched terms: court\n",
      "    ğŸ“ˆ Terms matched: 1\n",
      "\n",
      "12. ğŸ“„ Cr_Appeal_No_292017_in_Cr_PLA_No_322017_Hazrat_20Hussain_20versus_20The_20State.txt\n",
      "    ğŸ“ Length: 698 tokens\n",
      "    â­ Score: 10.0\n",
      "    ğŸ” Matched terms: court\n",
      "    ğŸ“ˆ Terms matched: 1\n",
      "\n",
      "13. ğŸ“„ CPLA_No_432011_Judgment_20in_20Ajab_20Khan_20Case.txt\n",
      "    ğŸ“ Length: 1,356 tokens\n",
      "    â­ Score: 10.0\n",
      "    ğŸ” Matched terms: court\n",
      "    ğŸ“ˆ Terms matched: 1\n",
      "\n",
      "14. ğŸ“„ Wajid_Ali_so_Ghulam_Nabi_and_09_others_Versus_Mur_judgment_20of_20wajid_20ali_20versus_20murtaza_20dated_20_17-4-2017_.txt\n",
      "    ğŸ“ Length: 401 tokens\n",
      "    â­ Score: 10.0\n",
      "    ğŸ” Matched terms: court\n",
      "    ğŸ“ˆ Terms matched: 1\n",
      "\n",
      "15. ğŸ“„ Hamidullah_son_of_Safar_Ali_Versus_Lal_Din_12_ot_Hamidullah_20versus_20Lal_20Din_207_20others.txt\n",
      "    ğŸ“ Length: 350 tokens\n",
      "    â­ Score: 10.0\n",
      "    ğŸ” Matched terms: court\n",
      "    ğŸ“ˆ Terms matched: 1\n",
      "\n",
      "... and 331 more documents\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“– Preview a document? (enter number or 'n'):  17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ“„ DOCUMENT: Cr_PLA_No_092017_The_20State_20versus_20Ghafoor_20__20others.txt\n",
      "ğŸ“ Tokens: 254\n",
      "================================================================================\n",
      "  1. page in the supreme appellate court gilgit-baltistan, gilgit. chief judge. judge. cr. pla. no. the state through depu...\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ¯ Enter command:  quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ‘‹ Goodbye!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List, Dict, Set, Tuple\n",
    "import fnmatch\n",
    "\n",
    "class WildcardSearchSystem:\n",
    "    def __init__(self, corpus_folder: str):\n",
    "        \"\"\"\n",
    "        Initialize Wildcard Search System\n",
    "        \n",
    "        Args:\n",
    "            corpus_folder: Path to the cleaned_corpus folder\n",
    "        \"\"\"\n",
    "        self.corpus_folder = corpus_folder\n",
    "        self.index_folder = os.path.join(corpus_folder, \"wildcard_index\")\n",
    "        \n",
    "        # Create index folder if it doesn't exist\n",
    "        if not os.path.exists(self.index_folder):\n",
    "            os.makedirs(self.index_folder)\n",
    "        \n",
    "        # Data structures\n",
    "        self.documents = {}  # doc_id -> document info\n",
    "        self.term_index = defaultdict(set)  # term -> set of doc_ids\n",
    "        self.kgram_index = defaultdict(set)  # k-gram -> set of terms\n",
    "        self.term_docs = defaultdict(set)  # term -> set of doc_ids\n",
    "        self.doc_terms = defaultdict(set)  # doc_id -> set of terms\n",
    "        \n",
    "        # Statistics\n",
    "        self.stats = {\n",
    "            'total_documents': 0,\n",
    "            'total_terms': 0,\n",
    "            'unique_terms': 0,\n",
    "            'kgram_size': 3  # For k-gram indexing\n",
    "        }\n",
    "    \n",
    "    def build_kgram_index(self, term: str, k: int = 3) -> Set[str]:\n",
    "        \"\"\"\n",
    "        Generate k-grams for a term\n",
    "        \n",
    "        Args:\n",
    "            term: The term to generate k-grams for\n",
    "            k: Size of k-grams (default 3)\n",
    "        \n",
    "        Returns:\n",
    "            Set of k-grams\n",
    "        \"\"\"\n",
    "        kgrams = set()\n",
    "        # Add special characters to handle wildcards properly\n",
    "        padded_term = f\"${term}$\"\n",
    "        \n",
    "        for i in range(len(padded_term) - k + 1):\n",
    "            kgram = padded_term[i:i+k]\n",
    "            kgrams.add(kgram)\n",
    "        \n",
    "        return kgrams\n",
    "    \n",
    "    def build_index(self):\n",
    "        \"\"\"Build wildcard search index from corpus\"\"\"\n",
    "        print(\"ğŸ”¨ Building Wildcard Search Index...\")\n",
    "        \n",
    "        # Load document tokens\n",
    "        doc_tokens_file = os.path.join(self.corpus_folder, \"document_tokens.json\")\n",
    "        \n",
    "        if not os.path.exists(doc_tokens_file):\n",
    "            print(\"âŒ document_tokens.json not found!\")\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            with open(doc_tokens_file, 'r', encoding='utf-8') as f:\n",
    "                doc_data = json.load(f)\n",
    "            print(f\"âœ… Loaded {len(doc_data)} documents\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error loading document tokens: {e}\")\n",
    "            return False\n",
    "        \n",
    "        # Process each document\n",
    "        doc_id = 0\n",
    "        all_terms = set()\n",
    "        \n",
    "        for doc_name, doc_info in doc_data.items():\n",
    "            doc_id += 1\n",
    "            tokens = doc_info.get('tokens', [])\n",
    "            token_count = doc_info.get('token_count', 0)\n",
    "            \n",
    "            if tokens and token_count > 0:\n",
    "                doc_key = f\"doc_{doc_id:05d}\"\n",
    "                \n",
    "                # Store document info\n",
    "                self.documents[doc_key] = {\n",
    "                    'name': doc_name,\n",
    "                    'token_count': token_count\n",
    "                }\n",
    "                \n",
    "                # Process unique terms in document\n",
    "                unique_terms = set(tokens)\n",
    "                self.doc_terms[doc_key] = unique_terms\n",
    "                all_terms.update(unique_terms)\n",
    "                \n",
    "                # Add to term index\n",
    "                for term in unique_terms:\n",
    "                    self.term_docs[term].add(doc_key)\n",
    "        \n",
    "        # Build k-gram index for all terms\n",
    "        print(\"ğŸ“Š Building k-gram index for wildcard support...\")\n",
    "        k = self.stats['kgram_size']\n",
    "        \n",
    "        for term in all_terms:\n",
    "            kgrams = self.build_kgram_index(term, k)\n",
    "            for kgram in kgrams:\n",
    "                self.kgram_index[kgram].add(term)\n",
    "        \n",
    "        # Update statistics\n",
    "        self.stats['total_documents'] = len(self.documents)\n",
    "        self.stats['unique_terms'] = len(all_terms)\n",
    "        self.stats['total_terms'] = sum(len(terms) for terms in self.doc_terms.values())\n",
    "        \n",
    "        # Save index\n",
    "        self.save_index()\n",
    "        \n",
    "        print(f\"\\nâœ… Wildcard index built successfully!\")\n",
    "        print(f\"   Documents: {self.stats['total_documents']:,}\")\n",
    "        print(f\"   Unique terms: {self.stats['unique_terms']:,}\")\n",
    "        print(f\"   Total term occurrences: {self.stats['total_terms']:,}\")\n",
    "        print(f\"   K-gram size: {k}\")\n",
    "        print(f\"   Unique k-grams: {len(self.kgram_index):,}\")\n",
    "        \n",
    "        # Show sample terms and k-grams\n",
    "        sample_terms = list(all_terms)[:10]\n",
    "        print(f\"   Sample terms: {', '.join(sample_terms)}\")\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def save_index(self):\n",
    "        \"\"\"Save index to disk\"\"\"\n",
    "        index_data = {\n",
    "            'documents': self.documents,\n",
    "            'kgram_index': {k: list(v) for k, v in self.kgram_index.items()},\n",
    "            'term_docs': {k: list(v) for k, v in self.term_docs.items()},\n",
    "            'doc_terms': {k: list(v) for k, v in self.doc_terms.items()},\n",
    "            'stats': self.stats\n",
    "        }\n",
    "        \n",
    "        index_file = os.path.join(self.index_folder, \"wildcard_index.pkl\")\n",
    "        with open(index_file, 'wb') as f:\n",
    "            pickle.dump(index_data, f)\n",
    "        \n",
    "        print(f\"\\nğŸ’¾ Index saved to: {index_file}\")\n",
    "    \n",
    "    def load_index(self):\n",
    "        \"\"\"Load index from disk\"\"\"\n",
    "        index_file = os.path.join(self.index_folder, \"wildcard_index.pkl\")\n",
    "        \n",
    "        if not os.path.exists(index_file):\n",
    "            print(\"âŒ Index not found. Building index...\")\n",
    "            return self.build_index()\n",
    "        \n",
    "        try:\n",
    "            print(f\"ğŸ“‚ Loading index from: {index_file}\")\n",
    "            with open(index_file, 'rb') as f:\n",
    "                index_data = pickle.load(f)\n",
    "            \n",
    "            self.documents = index_data['documents']\n",
    "            self.stats = index_data['stats']\n",
    "            \n",
    "            # Reconstruct kgram_index\n",
    "            self.kgram_index = defaultdict(set)\n",
    "            for kgram, terms in index_data['kgram_index'].items():\n",
    "                self.kgram_index[kgram] = set(terms)\n",
    "            \n",
    "            # Reconstruct term_docs\n",
    "            self.term_docs = defaultdict(set)\n",
    "            for term, docs in index_data['term_docs'].items():\n",
    "                self.term_docs[term] = set(docs)\n",
    "            \n",
    "            # Reconstruct doc_terms\n",
    "            self.doc_terms = defaultdict(set)\n",
    "            for doc_id, terms in index_data['doc_terms'].items():\n",
    "                self.doc_terms[doc_id] = set(terms)\n",
    "            \n",
    "            print(f\"âœ… Wildcard index loaded successfully!\")\n",
    "            print(f\"   Documents: {self.stats['total_documents']:,}\")\n",
    "            print(f\"   Unique terms: {self.stats['unique_terms']:,}\")\n",
    "            print(f\"   K-gram size: {self.stats['kgram_size']}\")\n",
    "            \n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error loading index: {e}\")\n",
    "            return self.build_index()\n",
    "    \n",
    "    def parse_wildcard_pattern(self, pattern: str) -> Tuple[str, str]:\n",
    "        \"\"\"\n",
    "        Parse wildcard pattern to extract prefix and suffix\n",
    "        \n",
    "        Args:\n",
    "            pattern: Wildcard pattern (e.g., \"court*\", \"*evidence\", \"m*rder\")\n",
    "        \n",
    "        Returns:\n",
    "            (pattern_type, regex_pattern)\n",
    "        \"\"\"\n",
    "        pattern = pattern.lower().strip()\n",
    "        \n",
    "        if not pattern:\n",
    "            return \"empty\", \"\"\n",
    "        \n",
    "        # Convert wildcard pattern to regex\n",
    "        # Escape regex special characters except * and ?\n",
    "        regex_pattern = re.escape(pattern)\n",
    "        \n",
    "        # Convert * to .*\n",
    "        regex_pattern = regex_pattern.replace(r'\\*', '.*')\n",
    "        \n",
    "        # Convert ? to .\n",
    "        regex_pattern = regex_pattern.replace(r'\\?', '.')\n",
    "        \n",
    "        # Add word boundaries for exact matching\n",
    "        regex_pattern = f\"^{regex_pattern}$\"\n",
    "        \n",
    "        return \"regex\", regex_pattern\n",
    "    \n",
    "    def kgram_wildcard_search(self, pattern: str) -> Set[str]:\n",
    "        \"\"\"\n",
    "        Wildcard search using k-gram indexing (fast)\n",
    "        \n",
    "        Args:\n",
    "            pattern: Wildcard pattern\n",
    "        \n",
    "        Returns:\n",
    "            Set of matching terms\n",
    "        \"\"\"\n",
    "        pattern = pattern.lower().strip()\n",
    "        \n",
    "        if '*' not in pattern and '?' not in pattern:\n",
    "            # No wildcards, exact match\n",
    "            if pattern in self.term_docs:\n",
    "                return {pattern}\n",
    "            return set()\n",
    "        \n",
    "        # Generate k-grams from pattern\n",
    "        k = self.stats['kgram_size']\n",
    "        \n",
    "        # Remove wildcards for k-gram generation\n",
    "        clean_pattern = pattern.replace('*', '').replace('?', '')\n",
    "        if not clean_pattern:\n",
    "            # Pattern is only wildcards, return all terms\n",
    "            return set(self.term_docs.keys())\n",
    "        \n",
    "        # Generate k-grams from clean pattern\n",
    "        pattern_kgrams = self.build_kgram_index(clean_pattern, k)\n",
    "        \n",
    "        # Find candidate terms that share k-grams with pattern\n",
    "        candidate_terms = None\n",
    "        \n",
    "        for kgram in pattern_kgrams:\n",
    "            terms_with_kgram = self.kgram_index.get(kgram, set())\n",
    "            \n",
    "            if candidate_terms is None:\n",
    "                candidate_terms = terms_with_kgram.copy()\n",
    "            else:\n",
    "                candidate_terms = candidate_terms.intersection(terms_with_kgram)\n",
    "            \n",
    "            if not candidate_terms:\n",
    "                break\n",
    "        \n",
    "        if not candidate_terms:\n",
    "            return set()\n",
    "        \n",
    "        # Filter candidate terms using regex\n",
    "        _, regex_pattern = self.parse_wildcard_pattern(pattern)\n",
    "        \n",
    "        matching_terms = set()\n",
    "        for term in candidate_terms:\n",
    "            if re.match(regex_pattern, term):\n",
    "                matching_terms.add(term)\n",
    "        \n",
    "        return matching_terms\n",
    "    \n",
    "    def regex_wildcard_search(self, pattern: str) -> Set[str]:\n",
    "        \"\"\"\n",
    "        Wildcard search using regex (more accurate but slower)\n",
    "        \n",
    "        Args:\n",
    "            pattern: Wildcard pattern\n",
    "        \n",
    "        Returns:\n",
    "            Set of matching terms\n",
    "        \"\"\"\n",
    "        pattern = pattern.lower().strip()\n",
    "        \n",
    "        if '*' not in pattern and '?' not in pattern:\n",
    "            # No wildcards, exact match\n",
    "            if pattern in self.term_docs:\n",
    "                return {pattern}\n",
    "            return set()\n",
    "        \n",
    "        # Convert to regex\n",
    "        _, regex_pattern = self.parse_wildcard_pattern(pattern)\n",
    "        \n",
    "        # Search through all terms\n",
    "        matching_terms = set()\n",
    "        \n",
    "        for term in self.term_docs.keys():\n",
    "            if re.match(regex_pattern, term):\n",
    "                matching_terms.add(term)\n",
    "        \n",
    "        return matching_terms\n",
    "    \n",
    "    def wildcard_search(self, pattern: str, use_kgram: bool = True) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Wildcard search for documents\n",
    "        \n",
    "        Args:\n",
    "            pattern: Wildcard pattern\n",
    "            use_kgram: Use k-gram indexing (faster) or regex (more accurate)\n",
    "        \n",
    "        Returns:\n",
    "            List of matching documents\n",
    "        \"\"\"\n",
    "        if not pattern or not pattern.strip():\n",
    "            print(\"âŒ Empty pattern\")\n",
    "            return []\n",
    "        \n",
    "        print(f\"\\nğŸ” Wildcard search: '{pattern}'\")\n",
    "        print(f\"   Method: {'k-gram indexing' if use_kgram else 'regex scanning'}\")\n",
    "        \n",
    "        # Find matching terms\n",
    "        start_time = time.time()\n",
    "        \n",
    "        if use_kgram:\n",
    "            matching_terms = self.kgram_wildcard_search(pattern)\n",
    "        else:\n",
    "            matching_terms = self.regex_wildcard_search(pattern)\n",
    "        \n",
    "        search_time = time.time() - start_time\n",
    "        \n",
    "        if not matching_terms:\n",
    "            print(f\"   âœ— No matching terms found\")\n",
    "            print(f\"   â±ï¸  Search time: {search_time:.3f}s\")\n",
    "            return []\n",
    "        \n",
    "        print(f\"   âœ“ Found {len(matching_terms)} matching term(s)\")\n",
    "        print(f\"   Terms: {', '.join(list(matching_terms)[:10])}\")\n",
    "        if len(matching_terms) > 10:\n",
    "            print(f\"   ... and {len(matching_terms) - 10} more\")\n",
    "        print(f\"   â±ï¸  Search time: {search_time:.3f}s\")\n",
    "        \n",
    "        # Find documents containing these terms\n",
    "        matching_docs = set()\n",
    "        term_doc_counts = []\n",
    "        \n",
    "        for term in matching_terms:\n",
    "            docs = self.term_docs.get(term, set())\n",
    "            term_doc_counts.append((term, len(docs)))\n",
    "            matching_docs.update(docs)\n",
    "        \n",
    "        print(f\"   ğŸ“„ Documents found: {len(matching_docs)}\")\n",
    "        \n",
    "        # Get document details\n",
    "        results = []\n",
    "        for doc_id in matching_docs:\n",
    "            doc_info = self.documents[doc_id]\n",
    "            \n",
    "            # Find which terms from the pattern appear in this document\n",
    "            doc_terms = self.doc_terms[doc_id]\n",
    "            matched_terms = matching_terms.intersection(doc_terms)\n",
    "            \n",
    "            # Calculate score based on term frequency and pattern match\n",
    "            score = len(matched_terms) * 10\n",
    "            \n",
    "            # Bonus for exact pattern match if it's a single term\n",
    "            if len(matching_terms) == 1 and pattern.replace('*', '').replace('?', ''):\n",
    "                if pattern in matched_terms:\n",
    "                    score += 20\n",
    "            \n",
    "            results.append({\n",
    "                'doc_id': doc_id,\n",
    "                'name': doc_info['name'],\n",
    "                'token_count': doc_info['token_count'],\n",
    "                'match_score': score,\n",
    "                'matched_terms': list(matched_terms),\n",
    "                'matched_count': len(matched_terms),\n",
    "                'pattern': pattern\n",
    "            })\n",
    "        \n",
    "        # Sort by score\n",
    "        results.sort(key=lambda x: x['match_score'], reverse=True)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def phrase_wildcard_search(self, phrase_pattern: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Wildcard search for phrases (multiple words with wildcards)\n",
    "        \n",
    "        Args:\n",
    "            phrase_pattern: Phrase with wildcards (e.g., \"supreme c*\", \"* of *\")\n",
    "        \n",
    "        Returns:\n",
    "            List of matching documents\n",
    "        \"\"\"\n",
    "        if not phrase_pattern or not phrase_pattern.strip():\n",
    "            return []\n",
    "        \n",
    "        print(f\"\\nğŸ” Phrase wildcard search: '{phrase_pattern}'\")\n",
    "        \n",
    "        # Split phrase into words\n",
    "        words = phrase_pattern.lower().strip().split()\n",
    "        \n",
    "        if len(words) == 1:\n",
    "            # Single word, use regular wildcard search\n",
    "            return self.wildcard_search(words[0])\n",
    "        \n",
    "        # For each word in phrase, find matching terms\n",
    "        word_term_sets = []\n",
    "        \n",
    "        for word in words:\n",
    "            if '*' in word or '?' in word:\n",
    "                matching_terms = self.wildcard_search(word, use_kgram=True)\n",
    "                word_term_sets.append(matching_terms)\n",
    "            else:\n",
    "                # Exact word\n",
    "                if word in self.term_docs:\n",
    "                    word_term_sets.append({word})\n",
    "                else:\n",
    "                    word_term_sets.append(set())\n",
    "        \n",
    "        # Check if any word has no matches\n",
    "        for i, term_set in enumerate(word_term_sets):\n",
    "            if not term_set:\n",
    "                print(f\"   âœ— No matches for word: '{words[i]}'\")\n",
    "                return []\n",
    "        \n",
    "        print(f\"   âœ“ All words have matching terms\")\n",
    "        \n",
    "        # Find documents containing sequences of these terms\n",
    "        # This is simplified - in reality would need position information\n",
    "        all_docs = set(self.documents.keys())\n",
    "        \n",
    "        # Find documents that contain at least one matching term for each word position\n",
    "        candidate_docs = all_docs.copy()\n",
    "        \n",
    "        for term_set in word_term_sets:\n",
    "            docs_with_any_term = set()\n",
    "            for term in term_set:\n",
    "                docs_with_any_term.update(self.term_docs.get(term, set()))\n",
    "            \n",
    "            candidate_docs = candidate_docs.intersection(docs_with_any_term)\n",
    "            \n",
    "            if not candidate_docs:\n",
    "                print(\"   âœ— No documents contain all required terms\")\n",
    "                return []\n",
    "        \n",
    "        print(f\"   ğŸ“„ Candidate documents: {len(candidate_docs)}\")\n",
    "        \n",
    "        # Score documents\n",
    "        results = []\n",
    "        for doc_id in candidate_docs:\n",
    "            doc_info = self.documents[doc_id]\n",
    "            doc_terms = self.doc_terms[doc_id]\n",
    "            \n",
    "            # Count how many words from phrase have matches in document\n",
    "            matched_words = 0\n",
    "            matched_terms_list = []\n",
    "            \n",
    "            for i, term_set in enumerate(word_term_sets):\n",
    "                # Check if any term from this word's set is in document\n",
    "                if term_set.intersection(doc_terms):\n",
    "                    matched_words += 1\n",
    "                    matched_terms = term_set.intersection(doc_terms)\n",
    "                    matched_terms_list.append(list(matched_terms)[0])\n",
    "            \n",
    "            score = (matched_words / len(words)) * 100\n",
    "            \n",
    "            results.append({\n",
    "                'doc_id': doc_id,\n",
    "                'name': doc_info['name'],\n",
    "                'token_count': doc_info['token_count'],\n",
    "                'match_score': score,\n",
    "                'matched_terms': matched_terms_list,\n",
    "                'matched_words': matched_words,\n",
    "                'total_words': len(words),\n",
    "                'pattern': phrase_pattern\n",
    "            })\n",
    "        \n",
    "        results.sort(key=lambda x: x['match_score'], reverse=True)\n",
    "        return results\n",
    "    \n",
    "    def prefix_search(self, prefix: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Prefix search (special case of wildcard: term*)\n",
    "        \n",
    "        Args:\n",
    "            prefix: Prefix to search for\n",
    "        \n",
    "        Returns:\n",
    "            List of matching documents\n",
    "        \"\"\"\n",
    "        pattern = f\"{prefix}*\"\n",
    "        return self.wildcard_search(pattern)\n",
    "    \n",
    "    def suffix_search(self, suffix: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Suffix search (special case of wildcard: *term)\n",
    "        \n",
    "        Args:\n",
    "            suffix: Suffix to search for\n",
    "        \n",
    "        Returns:\n",
    "            List of matching documents\n",
    "        \"\"\"\n",
    "        pattern = f\"*{suffix}\"\n",
    "        return self.wildcard_search(pattern)\n",
    "    \n",
    "    def contains_search(self, substring: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Contains search (special case of wildcard: *term*)\n",
    "        \n",
    "        Args:\n",
    "            substring: Substring to search for\n",
    "        \n",
    "        Returns:\n",
    "            List of matching documents\n",
    "        \"\"\"\n",
    "        pattern = f\"*{substring}*\"\n",
    "        return self.wildcard_search(pattern)\n",
    "    \n",
    "    def show_term_statistics(self, pattern: str = None):\n",
    "        \"\"\"Show statistics about terms matching a pattern\"\"\"\n",
    "        if pattern:\n",
    "            matching_terms = self.regex_wildcard_search(pattern)\n",
    "            if not matching_terms:\n",
    "                print(f\"âŒ No terms match pattern: '{pattern}'\")\n",
    "                return\n",
    "            \n",
    "            print(f\"\\nğŸ“Š TERM STATISTICS for pattern: '{pattern}'\")\n",
    "            print(\"=\" * 80)\n",
    "            print(f\"   Matching terms: {len(matching_terms)}\")\n",
    "            \n",
    "            # Show term frequencies\n",
    "            term_freqs = []\n",
    "            for term in matching_terms:\n",
    "                doc_count = len(self.term_docs.get(term, set()))\n",
    "                term_freqs.append((term, doc_count))\n",
    "            \n",
    "            # Sort by frequency\n",
    "            term_freqs.sort(key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            print(f\"\\nğŸ“ˆ TOP 20 MATCHING TERMS BY DOCUMENT FREQUENCY:\")\n",
    "            print(\"-\" * 80)\n",
    "            for i, (term, freq) in enumerate(term_freqs[:20], 1):\n",
    "                percentage = (freq / self.stats['total_documents']) * 100\n",
    "                print(f\"{i:3d}. {term:<25s} {freq:5d} docs ({percentage:.1f}%)\")\n",
    "        \n",
    "        else:\n",
    "            # Show general statistics\n",
    "            print(f\"\\nğŸ“Š TERM STATISTICS\")\n",
    "            print(\"=\" * 80)\n",
    "            print(f\"   Total unique terms: {self.stats['unique_terms']:,}\")\n",
    "            print(f\"   Total term occurrences: {self.stats['total_terms']:,}\")\n",
    "            \n",
    "            # Show most common terms\n",
    "            term_freqs = []\n",
    "            for term, docs in self.term_docs.items():\n",
    "                term_freqs.append((term, len(docs)))\n",
    "            \n",
    "            term_freqs.sort(key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            print(f\"\\nğŸ“ˆ TOP 20 MOST COMMON TERMS:\")\n",
    "            print(\"-\" * 80)\n",
    "            for i, (term, freq) in enumerate(term_freqs[:20], 1):\n",
    "                percentage = (freq / self.stats['total_documents']) * 100\n",
    "                print(f\"{i:3d}. {term:<25s} {freq:5d} docs ({percentage:.1f}%)\")\n",
    "        \n",
    "        print(\"=\" * 80)\n",
    "    \n",
    "    def find_similar_terms(self, term: str, max_distance: int = 2) -> List[str]:\n",
    "        \"\"\"\n",
    "        Find terms similar to given term using wildcard patterns\n",
    "        \n",
    "        Args:\n",
    "            term: Base term\n",
    "            max_distance: Maximum edit distance\n",
    "        \n",
    "        Returns:\n",
    "            List of similar terms\n",
    "        \"\"\"\n",
    "        if term not in self.term_docs:\n",
    "            print(f\"âŒ Term not found: '{term}'\")\n",
    "            return []\n",
    "        \n",
    "        similar_terms = set()\n",
    "        \n",
    "        # Generate wildcard patterns for similar terms\n",
    "        patterns = []\n",
    "        \n",
    "        # 1. Prefix variations\n",
    "        for i in range(1, min(max_distance + 1, len(term))):\n",
    "            pattern = term[:i] + \"*\"\n",
    "            patterns.append(pattern)\n",
    "        \n",
    "        # 2. Suffix variations\n",
    "        for i in range(1, min(max_distance + 1, len(term))):\n",
    "            pattern = \"*\" + term[-i:]\n",
    "            patterns.append(pattern)\n",
    "        \n",
    "        # 3. Contains variations\n",
    "        if len(term) > 3:\n",
    "            for i in range(1, len(term) - 1):\n",
    "                for j in range(i + 1, len(term)):\n",
    "                    substring = term[i:j]\n",
    "                    if len(substring) >= 2:\n",
    "                        pattern = f\"*{substring}*\"\n",
    "                        patterns.append(pattern)\n",
    "        \n",
    "        # Search for terms matching these patterns\n",
    "        for pattern in set(patterns):  # Remove duplicates\n",
    "            matching_terms = self.regex_wildcard_search(pattern)\n",
    "            similar_terms.update(matching_terms)\n",
    "        \n",
    "        # Remove the original term\n",
    "        similar_terms.discard(term)\n",
    "        \n",
    "        if similar_terms:\n",
    "            print(f\"\\nğŸ” Terms similar to '{term}':\")\n",
    "            sorted_terms = sorted(similar_terms)\n",
    "            for i, similar_term in enumerate(sorted_terms[:20], 1):\n",
    "                doc_count = len(self.term_docs.get(similar_term, set()))\n",
    "                print(f\"  {i:2d}. {similar_term} ({doc_count} docs)\")\n",
    "            \n",
    "            if len(sorted_terms) > 20:\n",
    "                print(f\"  ... and {len(sorted_terms) - 20} more\")\n",
    "        \n",
    "        return list(similar_terms)\n",
    "    \n",
    "    def display_results(self, results: List[Dict], title: str):\n",
    "        \"\"\"Display search results\"\"\"\n",
    "        if not results:\n",
    "            print(f\"\\nâŒ No results found\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\nâœ… {title}\")\n",
    "        print(f\"ğŸ“Š Found {len(results)} document(s)\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        for i, result in enumerate(results[:15], 1):\n",
    "            print(f\"\\n{i:2d}. ğŸ“„ {result['name']}\")\n",
    "            print(f\"    ğŸ“ Length: {result['token_count']:,} tokens\")\n",
    "            print(f\"    â­ Score: {result['match_score']:.1f}\")\n",
    "            \n",
    "            if 'matched_terms' in result and result['matched_terms']:\n",
    "                terms_str = ', '.join(result['matched_terms'][:5])\n",
    "                print(f\"    ğŸ” Matched terms: {terms_str}\")\n",
    "                if len(result['matched_terms']) > 5:\n",
    "                    print(f\"    ... and {len(result['matched_terms']) - 5} more\")\n",
    "            \n",
    "            if 'matched_count' in result:\n",
    "                print(f\"    ğŸ“ˆ Terms matched: {result['matched_count']}\")\n",
    "        \n",
    "        if len(results) > 15:\n",
    "            print(f\"\\n... and {len(results) - 15} more documents\")\n",
    "        \n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # Ask for document preview\n",
    "        if results:\n",
    "            choice = input(\"\\nğŸ“– Preview a document? (enter number or 'n'): \").strip()\n",
    "            if choice.lower() != 'n' and choice.isdigit():\n",
    "                idx = int(choice) - 1\n",
    "                if 0 <= idx < len(results):\n",
    "                    self.show_document_preview(results[idx]['doc_id'])\n",
    "    \n",
    "    def show_document_preview(self, doc_id: str, context_lines: int = 10):\n",
    "        \"\"\"Show document preview with matching terms highlighted\"\"\"\n",
    "        if doc_id not in self.doc_terms:\n",
    "            print(f\"âŒ Document not found: {doc_id}\")\n",
    "            return\n",
    "        \n",
    "        doc_info = self.documents[doc_id]\n",
    "        \n",
    "        # Try to load the actual document text\n",
    "        doc_path = os.path.join(self.corpus_folder, \"cleaned_docs\", doc_info['name'])\n",
    "        \n",
    "        if not os.path.exists(doc_path):\n",
    "            # Try alternative location\n",
    "            doc_path = os.path.join(self.corpus_folder, doc_info['name'])\n",
    "        \n",
    "        if os.path.exists(doc_path):\n",
    "            try:\n",
    "                with open(doc_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                    content = f.read()\n",
    "                \n",
    "                print(f\"\\n\" + \"=\" * 80)\n",
    "                print(f\"ğŸ“„ DOCUMENT: {doc_info['name']}\")\n",
    "                print(f\"ğŸ“ Tokens: {doc_info['token_count']:,}\")\n",
    "                print(\"=\" * 80)\n",
    "                \n",
    "                # Show first N lines\n",
    "                lines = content.split('\\n')[:context_lines]\n",
    "                for i, line in enumerate(lines, 1):\n",
    "                    if line.strip():\n",
    "                        # Truncate long lines\n",
    "                        if len(line) > 120:\n",
    "                            line = line[:117] + \"...\"\n",
    "                        print(f\"{i:3d}. {line}\")\n",
    "                \n",
    "                if len(content.split('\\n')) > context_lines:\n",
    "                    print(f\"\\n... and {len(content.split('\\n')) - context_lines} more lines\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Error reading document: {e}\")\n",
    "                print(f\"\\nğŸ“„ Document: {doc_info['name']}\")\n",
    "                print(f\"ğŸ“ Tokens: {doc_info['token_count']:,}\")\n",
    "                print(f\"ğŸ” Terms in document ({len(self.doc_terms[doc_id])}):\")\n",
    "                terms = sorted(self.doc_terms[doc_id])\n",
    "                for i in range(0, len(terms), 10):\n",
    "                    print(f\"  {', '.join(terms[i:i+10])}\")\n",
    "        else:\n",
    "            print(f\"\\nğŸ“„ Document: {doc_info['name']} (file not found)\")\n",
    "            print(f\"ğŸ“ Tokens: {doc_info['token_count']:,}\")\n",
    "            print(f\"ğŸ” Terms in document ({len(self.doc_terms[doc_id])}):\")\n",
    "            terms = sorted(self.doc_terms[doc_id])\n",
    "            for i in range(0, len(terms), 10):\n",
    "                print(f\"  {', '.join(terms[i:i+10])}\")\n",
    "        \n",
    "        print(\"=\" * 80)\n",
    "    \n",
    "    def interactive_search(self):\n",
    "        \"\"\"Interactive wildcard search interface\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"ğŸ” WILDCARD SEARCH SYSTEM\")\n",
    "        print(\"=\" * 80)\n",
    "        print(\"\\nğŸ“‹ WILDCARD SYNTAX:\")\n",
    "        print(\"  â€¢ *        = Matches any sequence of characters\")\n",
    "        print(\"  â€¢ ?        = Matches any single character\")\n",
    "        print(\"  â€¢ term*    = Terms starting with 'term'\")\n",
    "        print(\"  â€¢ *term    = Terms ending with 'term'\")\n",
    "        print(\"  â€¢ *term*   = Terms containing 'term'\")\n",
    "        print(\"  â€¢ c?urt    = Terms like 'court', 'curt', etc.\")\n",
    "        print(\"  â€¢ m*rder   = Terms like 'murder', 'morder', etc.\")\n",
    "        \n",
    "        print(\"\\nğŸ“‹ AVAILABLE COMMANDS:\")\n",
    "        print(\"  â€¢ search <pattern>    - Wildcard search\")\n",
    "        print(\"  â€¢ prefix <text>       - Prefix search (text*)\")\n",
    "        print(\"  â€¢ suffix <text>       - Suffix search (*text)\")\n",
    "        print(\"  â€¢ contains <text>     - Contains search (*text*)\")\n",
    "        print(\"  â€¢ phrase <pattern>    - Phrase with wildcards\")\n",
    "        print(\"  â€¢ similar <term>      - Find similar terms\")\n",
    "        print(\"  â€¢ terms [pattern]     - Show term statistics\")\n",
    "        print(\"  â€¢ stats               - Show index statistics\")\n",
    "        print(\"  â€¢ example             - Show examples\")\n",
    "        print(\"  â€¢ quit                - Exit\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        while True:\n",
    "            user_input = input(\"\\nğŸ¯ Enter command: \").strip()\n",
    "            \n",
    "            if not user_input:\n",
    "                continue\n",
    "            \n",
    "            if user_input.lower() == 'quit':\n",
    "                print(\"ğŸ‘‹ Goodbye!\")\n",
    "                break\n",
    "            \n",
    "            elif user_input.lower() == 'stats':\n",
    "                print(f\"\\nğŸ“Š WILDCARD INDEX STATISTICS\")\n",
    "                print(\"=\" * 60)\n",
    "                print(f\"Documents: {self.stats['total_documents']:,}\")\n",
    "                print(f\"Unique terms: {self.stats['unique_terms']:,}\")\n",
    "                print(f\"K-gram size: {self.stats['kgram_size']}\")\n",
    "                print(f\"Unique k-grams: {len(self.kgram_index):,}\")\n",
    "                print(\"=\" * 60)\n",
    "            \n",
    "            elif user_input.lower() == 'example':\n",
    "                self.show_examples()\n",
    "            \n",
    "            elif user_input.lower().startswith('search '):\n",
    "                pattern = user_input[7:].strip()\n",
    "                if pattern:\n",
    "                    results = self.wildcard_search(pattern, use_kgram=True)\n",
    "                    self.display_results(results, f\"Wildcard search: '{pattern}'\")\n",
    "                else:\n",
    "                    print(\"âŒ Please enter a wildcard pattern\")\n",
    "            \n",
    "            elif user_input.lower().startswith('prefix '):\n",
    "                text = user_input[7:].strip()\n",
    "                if text:\n",
    "                    results = self.prefix_search(text)\n",
    "                    self.display_results(results, f\"Prefix search: '{text}*'\")\n",
    "                else:\n",
    "                    print(\"âŒ Please enter prefix text\")\n",
    "            \n",
    "            elif user_input.lower().startswith('suffix '):\n",
    "                text = user_input[7:].strip()\n",
    "                if text:\n",
    "                    results = self.suffix_search(text)\n",
    "                    self.display_results(results, f\"Suffix search: '*{text}'\")\n",
    "                else:\n",
    "                    print(\"âŒ Please enter suffix text\")\n",
    "            \n",
    "            elif user_input.lower().startswith('contains '):\n",
    "                text = user_input[9:].strip()\n",
    "                if text:\n",
    "                    results = self.contains_search(text)\n",
    "                    self.display_results(results, f\"Contains search: '*{text}*'\")\n",
    "                else:\n",
    "                    print(\"âŒ Please enter text to search for\")\n",
    "            \n",
    "            elif user_input.lower().startswith('phrase '):\n",
    "                pattern = user_input[7:].strip()\n",
    "                if pattern:\n",
    "                    results = self.phrase_wildcard_search(pattern)\n",
    "                    self.display_results(results, f\"Phrase wildcard: '{pattern}'\")\n",
    "                else:\n",
    "                    print(\"âŒ Please enter a phrase pattern\")\n",
    "            \n",
    "            elif user_input.lower().startswith('similar '):\n",
    "                term = user_input[8:].strip()\n",
    "                if term:\n",
    "                    self.find_similar_terms(term)\n",
    "                else:\n",
    "                    print(\"âŒ Please enter a term\")\n",
    "            \n",
    "            elif user_input.lower().startswith('terms '):\n",
    "                pattern = user_input[6:].strip()\n",
    "                self.show_term_statistics(pattern)\n",
    "            \n",
    "            elif user_input.lower() == 'terms':\n",
    "                self.show_term_statistics()\n",
    "            \n",
    "            else:\n",
    "                # Default to wildcard search\n",
    "                results = self.wildcard_search(user_input, use_kgram=True)\n",
    "                self.display_results(results, f\"Search: '{user_input}'\")\n",
    "    \n",
    "    def show_examples(self):\n",
    "        \"\"\"Show example wildcard searches\"\"\"\n",
    "        print(\"\\nğŸ“ WILDCARD SEARCH EXAMPLES\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        examples = [\n",
    "            (\"Prefix search\", \"court*\", \"Finds: court, courts, courtroom, etc.\"),\n",
    "            (\"Suffix search\", \"*ment\", \"Finds: judgment, document, agreement, etc.\"),\n",
    "            (\"Contains search\", \"*evid*\", \"Finds: evidence, evident, evidentiary, etc.\"),\n",
    "            (\"Single character wildcard\", \"c?urt\", \"Finds: court, curt\"),\n",
    "            (\"Multiple wildcards\", \"m*rder\", \"Finds: murder, morder, murderer\"),\n",
    "            (\"Combined wildcards\", \"*cour*ment*\", \"Finds terms containing both 'cour' and 'ment'\"),\n",
    "            (\"Phrase with wildcards\", \"supreme c* of *\", \"Finds phrases like 'supreme court of pakistan'\"),\n",
    "        ]\n",
    "        \n",
    "        for category, pattern, explanation in examples:\n",
    "            print(f\"\\n{category}:\")\n",
    "            print(f\"  Pattern: {pattern}\")\n",
    "            print(f\"  {explanation}\")\n",
    "        \n",
    "        print(\"\\nğŸ’¡ LEGAL TERM EXAMPLES:\")\n",
    "        legal_examples = [\n",
    "            (\"judg*\", \"judgment, judge, judging, judgements\"),\n",
    "            (\"*appeal*\", \"appeal, appeals, appealing, appealable\"),\n",
    "            (\"evid*\", \"evidence, evident, evidentiary\"),\n",
    "            (\"crimin*\", \"criminal, crime, criminology\"),\n",
    "            (\"const*\", \"constitutional, constitution, constituted\"),\n",
    "            (\"*right*\", \"right, rights, rightful, birthright\"),\n",
    "        ]\n",
    "        \n",
    "        for pattern, finds in legal_examples:\n",
    "            print(f\"  {pattern:<15} â†’ {finds}\")\n",
    "        \n",
    "        print(\"=\" * 80)\n",
    "\n",
    "import time  # Added import for timing\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function\"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"ğŸ” WILDCARD QUERY SEARCH SYSTEM\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Try to find corpus folder\n",
    "    corpus_folder = r\"C:\\Users\\Armaghan Rafique\\Desktop\\AI Project\\cleaned_corpus\"\n",
    "    \n",
    "    if not os.path.exists(corpus_folder):\n",
    "        print(f\"âŒ Corpus folder not found: {corpus_folder}\")\n",
    "        \n",
    "        # Try alternative\n",
    "        alt_folders = [\n",
    "            r\"C:\\Users\\Armaghan Rafique\\Desktop\\AI Project\\supreme_court_judgements_txt\",\n",
    "            r\"C:\\Users\\Armaghan Rafique\\Desktop\\AI Project\",\n",
    "            os.path.join(os.path.expanduser(\"~\"), \"Desktop\", \"AI Project\", \"cleaned_corpus\")\n",
    "        ]\n",
    "        \n",
    "        found = False\n",
    "        for folder in alt_folders:\n",
    "            if os.path.exists(folder):\n",
    "                corpus_folder = folder\n",
    "                print(f\"âœ… Using folder: {corpus_folder}\")\n",
    "                found = True\n",
    "                break\n",
    "        \n",
    "        if not found:\n",
    "            corpus_folder = input(\"ğŸ“ Enter corpus folder path: \").strip()\n",
    "            if not os.path.exists(corpus_folder):\n",
    "                print(f\"âŒ Folder does not exist: {corpus_folder}\")\n",
    "                return\n",
    "    \n",
    "    print(f\"\\nğŸ“ Corpus folder: {corpus_folder}\")\n",
    "    \n",
    "    # Create wildcard search system\n",
    "    wildcard_system = WildcardSearchSystem(corpus_folder)\n",
    "    \n",
    "    # Load or build index\n",
    "    if not wildcard_system.load_index():\n",
    "        print(\"âŒ Failed to initialize wildcard search system\")\n",
    "        return\n",
    "    \n",
    "    # Show welcome message\n",
    "    print(f\"\\nğŸ¯ WILDCARD SEARCH READY\")\n",
    "    print(f\"   Documents: {wildcard_system.stats['total_documents']:,}\")\n",
    "    print(f\"   Unique terms: {wildcard_system.stats['unique_terms']:,}\")\n",
    "    \n",
    "    # Test some example searches\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ğŸ§ª QUICK TEST EXAMPLES\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    test_patterns = [\n",
    "        \"court*\",\n",
    "        \"*evidence\",\n",
    "        \"*murder*\",\n",
    "        \"judg*\",\n",
    "    ]\n",
    "    \n",
    "    for pattern in test_patterns[:2]:  # Test first 2\n",
    "        print(f\"\\nğŸ” Testing: '{pattern}'\")\n",
    "        results = wildcard_system.wildcard_search(pattern, use_kgram=True)\n",
    "        if results:\n",
    "            print(f\"   âœ… Found {len(results)} documents\")\n",
    "            print(f\"   Sample terms: {results[0].get('matched_terms', [])[:3]}\")\n",
    "        else:\n",
    "            print(f\"   âš ï¸  No matches found\")\n",
    "    \n",
    "    # Start interactive search\n",
    "    wildcard_system.interactive_search()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754b8de5-5a92-4b82-a1a3-fe55d919f90d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
