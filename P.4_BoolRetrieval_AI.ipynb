{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59c9f492-0765-48c2-ad26-d72a21f6f76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "‚öñÔ∏è  SUPREME COURT - BOOLEAN RETRIEVAL SYSTEM\n",
      "================================================================================\n",
      "‚úÖ Found corpus folder: C:\\Users\\Armaghan Rafique\\Desktop\\AI Project\\cleaned_corpus\n",
      "\n",
      "üìÅ Using corpus folder: C:\\Users\\Armaghan Rafique\\Desktop\\AI Project\\cleaned_corpus\n",
      "‚úÖ Index loaded successfully!\n",
      "   Documents: 1,460\n",
      "   Vocabulary: 15,240\n",
      "\n",
      "üìä SYSTEM READY\n",
      "   Documents: 1,460\n",
      "   Vocabulary: 15,240\n",
      "   Total terms: 1,048,901\n",
      "\n",
      "================================================================================\n",
      "üéØ What would you like to do?\n",
      "  1. Test predefined queries\n",
      "  2. Start interactive search\n",
      "  3. Find documents for a specific term\n",
      "  4. Get statistics for a term\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Select option (1-4):  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üîç BOOLEAN RETRIEVAL SYSTEM - INTERACTIVE SEARCH\n",
      "================================================================================\n",
      "\n",
      "üìã Available Commands:\n",
      "  ‚Ä¢ search <query>      - Search for documents\n",
      "  ‚Ä¢ stats <term>        - Get statistics for a term\n",
      "  ‚Ä¢ find <term>         - Find documents containing a term\n",
      "  ‚Ä¢ preview <doc_name>  - Preview a specific document\n",
      "  ‚Ä¢ terms               - Show most common terms\n",
      "  ‚Ä¢ quit                - Exit the program\n",
      "\n",
      "üìù Query Examples:\n",
      "  ‚Ä¢ evidence\n",
      "  ‚Ä¢ murder AND evidence\n",
      "  ‚Ä¢ murder OR homicide\n",
      "  ‚Ä¢ murder AND NOT evidence\n",
      "  ‚Ä¢ (murder OR killing) AND weapon\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ Enter command:  murder\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Query: 'murder'\n",
      "   Operation: TERM\n",
      "   Documents found: 96\n",
      "\n",
      "‚úÖ Found 96 document(s):\n",
      "    1. 2025LHC7277.txt (1,051 tokens)\n",
      "    2. 2025LHC7389.txt (6,103 tokens)\n",
      "    3. 2025LHC7437.txt (6,891 tokens)\n",
      "    4. 2025LHC7508.txt (5,160 tokens)\n",
      "    5. 2025LHC7583.txt (4,272 tokens)\n",
      "    6. 2025LHC7631.txt (10,326 tokens)\n",
      "    7. Ateeq_Hussain_Versus_The_State_The_State_Versus_S_Ateeq_hussain_vs_the_state.txt (2,008 tokens)\n",
      "    8. Bulbul_Aman_Shah_Versus_The_State_Bulbul_20Aman_20Shah_20versus_20The_20State.txt (1,083 tokens)\n",
      "    9. Civil_Appeal_No022018_in_CrPLA_No_342017_The_20State_20versus_20Muhammad_20Nadeem.txt (649 tokens)\n",
      "   10. CrA_No_062013__8_._Cr._A_No._6-2013_Naheed_Akhtar_v._The_State.txt (2,998 tokens)\n",
      "   11. CrAppeal_No022011_in_CrPLA_No152010_The_20State_20Versus_20Muhammad_20Afzal.txt (5,445 tokens)\n",
      "   12. CrAppeal_No022011_in_CrPLA_No152010_The_Stat_Muhammad_20Afzal_20Versus_20The_20State.txt (5,443 tokens)\n",
      "   13. CrPLANo292015_Sufiyan_20versus_20the_20STate.txt (73 tokens)\n",
      "   14. Cr_Appeal_No062008_The_20state_20vs_20muhamma_20sadiq.txt (1,154 tokens)\n",
      "   15. Cr_Appeal_No102015_in_CrPLA_NO212015_The_20State_20Versus_20Abdul_20Ghafar.txt (600 tokens)\n",
      "   16. Cr_Appeal_No_012018_in_Cr_PLA_No_02_2018_The_20State_20versus_20Ashfaq_20Hussain_20__20another_20Bail_20matter.txt (357 tokens)\n",
      "   17. Cr_Appeal_No_022015_in_CrPLA_No_062013_Sher_20Wali_20versus_20Sakhawat_20Dar1.txt (897 tokens)\n",
      "   18. Cr_Appeal_No_032011_in_CrPLA_No_102010_Naveed_Hussain_versus_the_State.txt (1,650 tokens)\n",
      "   19. Cr_Appeal_No_032016_in_Cr_PLA_No_012015_The_20State_20versus_20Sulieman_20_2_.txt (1,350 tokens)\n",
      "   20. Cr_Appeal_No_032018_in_CrPLANo332016_The_20State_20versus_20Naveed_20Akhtar_20Alias_20Jani.txt (854 tokens)\n",
      "  ... and 76 more documents\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ Enter command:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Query: 'exit'\n",
      "   Operation: TERM\n",
      "   Documents found: 1\n",
      "\n",
      "‚úÖ Found 1 document(s):\n",
      "    1. PLD2016Sindh238.txt (5,040 tokens)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ Enter command:  end\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Query: 'end'\n",
      "   Operation: TERM\n",
      "   Documents found: 215\n",
      "\n",
      "‚úÖ Found 215 document(s):\n",
      "    1. 1_Tajuddin_2_Gul_Alam_sons_of_Mir_Alam_VERSUS_M__5_._20tajuddin_20etc_20vs_20Mst_20Zainab.txt (760 tokens)\n",
      "    2. 2025LHC7336.txt (1,139 tokens)\n",
      "    3. Abdu-Rahim_Shah_Versus_Provincial_Government_oth_Abdur-Rahim_20Shah_20versus_20the_20Prov_20Govt.txt (417 tokens)\n",
      "    4. Abdul_Bari_Versus_Government_of_GB_others_Abdul_20Bari_20versus_20Government_20of_20GB.txt (217 tokens)\n",
      "    5. Abdul_Bari_Vs_Provincial_Governmet_Abdul_Bari_Vs_Provincial_Govt.txt (278 tokens)\n",
      "    6. Abdul_Wahid_Versus_The_State_Cr.Misc._No._07-210.txt (251 tokens)\n",
      "    7. Abdur-ur-Rahim_versus_The_State_abdur-ur-rahim_vs_the_state.txt (500 tokens)\n",
      "    8. Abdur-ur-rahim_vs_The_state_abdur-ur-rahim_20vs_20the_20state.txt (500 tokens)\n",
      "    9. All_Gilgit-Baltistan_workers_federation_all_gilgit_baltistan_workers_federation.txt (4,452 tokens)\n",
      "   10. All_Residents_of_Fultux_Versus_Muhammad_Ali_judgement_20of_20all_20residents_20of_20fultux.txt (282 tokens)\n",
      "   11. Ashfaq_So_Ghulam_Muhammad_Versus_The_State_cr.Misc._No._26-2009.txt (636 tokens)\n",
      "   12. Ashfaq_Versus_The_State_ashfaq_vs_state.txt (625 tokens)\n",
      "   13. Ateeq_Hussain_Versus_The_State_The_State_Versus_S_Ateeq_hussain_vs_the_state.txt (2,008 tokens)\n",
      "   14. Barat_Ali_Versus_Mst_Maher_Banu_Barat_20Ali.txt (265 tokens)\n",
      "   15. CIVIL_APPEAL_NO_032015_Rozi_20khan.txt (333 tokens)\n",
      "   16. CIVIL_APPEAL_NO_062015_in_CPLA_No732014_Ghullam_20Nabi_20Versus_20Mst._20Gull_20Najaf.txt (419 tokens)\n",
      "   17. CPLA_NO032010_islamic_investment_bank.txt (710 tokens)\n",
      "   18. CPLA_NO362009_Meherban_Ali_vs_shakoor_Khan.txt (727 tokens)\n",
      "   19. CPLA_NO412014_Suleman_20Versus_20Maroof.txt (382 tokens)\n",
      "   20. CPLA_NO442014_Fatime_20Bi_20versus_20Abdur_20Rahim.txt (419 tokens)\n",
      "  ... and 195 more documents\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ Enter command:  \n",
      "\n",
      "üéØ Enter command:  \n",
      "\n",
      "üéØ Enter command:  quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üëã Goodbye!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "from typing import Set, List, Dict, Tuple\n",
    "\n",
    "class BooleanRetrievalSystem:\n",
    "    def __init__(self, corpus_folder: str):\n",
    "        \"\"\"\n",
    "        Initialize Boolean Retrieval System\n",
    "        \n",
    "        Args:\n",
    "            corpus_folder: Path to the cleaned_corpus folder\n",
    "        \"\"\"\n",
    "        self.corpus_folder = corpus_folder\n",
    "        self.index_folder = os.path.join(corpus_folder, \"boolean_index\")\n",
    "        \n",
    "        # Create index folder if it doesn't exist\n",
    "        if not os.path.exists(self.index_folder):\n",
    "            os.makedirs(self.index_folder)\n",
    "        \n",
    "        # Data structures\n",
    "        self.inverted_index = defaultdict(set)  # term -> set of doc_ids\n",
    "        self.documents = {}  # doc_id -> document info\n",
    "        self.term_frequencies = defaultdict(Counter)  # doc_id -> term -> frequency\n",
    "        self.doc_lengths = {}  # doc_id -> number of tokens\n",
    "        \n",
    "        # Statistics\n",
    "        self.stats = {\n",
    "            'total_documents': 0,\n",
    "            'total_terms': 0,\n",
    "            'vocabulary_size': 0\n",
    "        }\n",
    "        \n",
    "        # Query parsing cache\n",
    "        self.query_cache = {}\n",
    "        \n",
    "        # Document name to ID mapping for easier lookup\n",
    "        self.doc_name_to_id = {}\n",
    "    \n",
    "    def build_index(self):\n",
    "        \"\"\"Build inverted index from corpus\"\"\"\n",
    "        print(\"üî® Building Boolean Retrieval Index...\")\n",
    "        \n",
    "        # First, check what files are available\n",
    "        print(f\"üìÅ Checking corpus folder: {self.corpus_folder}\")\n",
    "        \n",
    "        # Look for document tokens file in different locations\n",
    "        possible_token_files = [\n",
    "            os.path.join(self.corpus_folder, \"document_tokens.json\"),\n",
    "            os.path.join(self.corpus_folder, \"..\", \"document_tokens.json\"),\n",
    "            os.path.join(self.corpus_folder, \"statistics\", \"document_tokens.json\"),\n",
    "        ]\n",
    "        \n",
    "        doc_tokens_file = None\n",
    "        for file_path in possible_token_files:\n",
    "            if os.path.exists(file_path):\n",
    "                doc_tokens_file = file_path\n",
    "                print(f\"‚úÖ Found document tokens: {file_path}\")\n",
    "                break\n",
    "        \n",
    "        if not doc_tokens_file:\n",
    "            print(\"‚ùå document_tokens.json not found!\")\n",
    "            print(\"Available files in corpus folder:\")\n",
    "            for root, dirs, files in os.walk(self.corpus_folder):\n",
    "                for file in files[:20]:  # Show first 20 files\n",
    "                    print(f\"  ‚Ä¢ {os.path.join(root, file)}\")\n",
    "                if len(files) > 20:\n",
    "                    print(f\"  ... and {len(files) - 20} more files\")\n",
    "            return False\n",
    "        \n",
    "        # Load document tokens\n",
    "        try:\n",
    "            with open(doc_tokens_file, 'r', encoding='utf-8') as f:\n",
    "                doc_data = json.load(f)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading document tokens: {e}\")\n",
    "            return False\n",
    "        \n",
    "        # Process each document\n",
    "        doc_id = 0\n",
    "        for doc_name, doc_info in doc_data.items():\n",
    "            tokens = doc_info.get('tokens', [])\n",
    "            token_count = doc_info.get('token_count', 0)\n",
    "            \n",
    "            if tokens and token_count > 0:\n",
    "                doc_id += 1\n",
    "                doc_key = f\"doc_{doc_id:04d}\"  # Format as doc_0001, doc_0002, etc.\n",
    "                \n",
    "                # Store document info\n",
    "                self.documents[doc_key] = {\n",
    "                    'name': doc_name,\n",
    "                    'token_count': token_count\n",
    "                }\n",
    "                \n",
    "                # Store name to ID mapping for quick lookup\n",
    "                self.doc_name_to_id[doc_name] = doc_key\n",
    "                \n",
    "                # Build inverted index and term frequencies\n",
    "                term_counter = Counter(tokens)\n",
    "                self.term_frequencies[doc_key] = term_counter\n",
    "                self.doc_lengths[doc_key] = token_count\n",
    "                \n",
    "                # Add to inverted index\n",
    "                for term in term_counter:\n",
    "                    self.inverted_index[term].add(doc_key)\n",
    "        \n",
    "        # Update statistics\n",
    "        self.stats['total_documents'] = len(self.documents)\n",
    "        self.stats['vocabulary_size'] = len(self.inverted_index)\n",
    "        self.stats['total_terms'] = sum(self.doc_lengths.values())\n",
    "        \n",
    "        # Save index\n",
    "        self.save_index()\n",
    "        \n",
    "        print(f\"\\n‚úÖ Index built successfully!\")\n",
    "        print(f\"   Documents: {self.stats['total_documents']:,}\")\n",
    "        print(f\"   Vocabulary: {self.stats['vocabulary_size']:,}\")\n",
    "        print(f\"   Total terms: {self.stats['total_terms']:,}\")\n",
    "        \n",
    "        # Show sample terms\n",
    "        sample_terms = list(self.inverted_index.keys())[:10]\n",
    "        print(f\"   Sample terms: {', '.join(sample_terms)}...\")\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def save_index(self):\n",
    "        \"\"\"Save index to disk\"\"\"\n",
    "        index_data = {\n",
    "            'inverted_index': {k: list(v) for k, v in self.inverted_index.items()},\n",
    "            'documents': self.documents,\n",
    "            'term_frequencies': {k: dict(v) for k, v in self.term_frequencies.items()},\n",
    "            'doc_lengths': self.doc_lengths,\n",
    "            'stats': self.stats,\n",
    "            'doc_name_to_id': self.doc_name_to_id\n",
    "        }\n",
    "        \n",
    "        index_file = os.path.join(self.index_folder, \"boolean_index.pkl\")\n",
    "        with open(index_file, 'wb') as f:\n",
    "            pickle.dump(index_data, f)\n",
    "        \n",
    "        print(f\"\\nüíæ Index saved to: {index_file}\")\n",
    "    \n",
    "    def load_index(self):\n",
    "        \"\"\"Load index from disk\"\"\"\n",
    "        index_file = os.path.join(self.index_folder, \"boolean_index.pkl\")\n",
    "        \n",
    "        if not os.path.exists(index_file):\n",
    "            print(\"‚ùå Index not found. Building index...\")\n",
    "            return self.build_index()\n",
    "        \n",
    "        try:\n",
    "            with open(index_file, 'rb') as f:\n",
    "                index_data = pickle.load(f)\n",
    "            \n",
    "            self.inverted_index = defaultdict(set)\n",
    "            for k, v in index_data['inverted_index'].items():\n",
    "                self.inverted_index[k] = set(v)\n",
    "            \n",
    "            self.documents = index_data['documents']\n",
    "            self.term_frequencies = defaultdict(Counter)\n",
    "            for k, v in index_data['term_frequencies'].items():\n",
    "                self.term_frequencies[k] = Counter(v)\n",
    "            \n",
    "            self.doc_lengths = index_data['doc_lengths']\n",
    "            self.stats = index_data['stats']\n",
    "            self.doc_name_to_id = index_data.get('doc_name_to_id', {})\n",
    "            \n",
    "            print(f\"‚úÖ Index loaded successfully!\")\n",
    "            print(f\"   Documents: {self.stats['total_documents']:,}\")\n",
    "            print(f\"   Vocabulary: {self.stats['vocabulary_size']:,}\")\n",
    "            \n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading index: {e}\")\n",
    "            print(\"Building fresh index...\")\n",
    "            return self.build_index()\n",
    "    \n",
    "    def boolean_and(self, term1_docs: Set[str], term2_docs: Set[str]) -> Set[str]:\n",
    "        \"\"\"Boolean AND operation (intersection)\"\"\"\n",
    "        if not term1_docs or not term2_docs:\n",
    "            return set()\n",
    "        return term1_docs.intersection(term2_docs)\n",
    "    \n",
    "    def boolean_or(self, term1_docs: Set[str], term2_docs: Set[str]) -> Set[str]:\n",
    "        \"\"\"Boolean OR operation (union)\"\"\"\n",
    "        if not term1_docs:\n",
    "            return term2_docs.copy() if term2_docs else set()\n",
    "        if not term2_docs:\n",
    "            return term1_docs.copy() if term1_docs else set()\n",
    "        return term1_docs.union(term2_docs)\n",
    "    \n",
    "    def boolean_not(self, term_docs: Set[str]) -> Set[str]:\n",
    "        \"\"\"Boolean NOT operation (complement)\"\"\"\n",
    "        all_docs = set(self.documents.keys())\n",
    "        return all_docs.difference(term_docs)\n",
    "    \n",
    "    def get_docs_for_term(self, term: str) -> Set[str]:\n",
    "        \"\"\"Get documents containing a term\"\"\"\n",
    "        term_lower = term.lower().strip()\n",
    "        if term_lower in self.inverted_index:\n",
    "            return self.inverted_index[term_lower].copy()\n",
    "        return set()\n",
    "    \n",
    "    def simple_parse_query(self, query: str) -> Tuple[str, Set[str]]:\n",
    "        \"\"\"\n",
    "        Simple query parser for boolean operations\n",
    "        Supports: AND, OR, NOT, parentheses\n",
    "        \"\"\"\n",
    "        # Clean query\n",
    "        query = query.strip().lower()\n",
    "        \n",
    "        # Check cache\n",
    "        if query in self.query_cache:\n",
    "            return self.query_cache[query]\n",
    "        \n",
    "        # Handle parentheses first\n",
    "        if '(' in query and ')' in query:\n",
    "            # Find innermost parentheses\n",
    "            start = query.find('(')\n",
    "            end = query.rfind(')')\n",
    "            if start < end:\n",
    "                inner_query = query[start+1:end]\n",
    "                # Parse inner query\n",
    "                inner_op, inner_result = self.simple_parse_query(inner_query)\n",
    "                \n",
    "                # Replace parentheses with placeholder\n",
    "                placeholder = f\"__result_{len(inner_result)}__\"\n",
    "                new_query = query[:start] + placeholder + query[end+1:]\n",
    "                \n",
    "                # Parse the new query\n",
    "                return self.simple_parse_query(new_query)\n",
    "        \n",
    "        # Handle NOT operations\n",
    "        if query.startswith('not '):\n",
    "            term = query[4:].strip()\n",
    "            docs = self.get_docs_for_term(term)\n",
    "            result = self.boolean_not(docs)\n",
    "            self.query_cache[query] = (\"NOT\", result)\n",
    "            return \"NOT\", result\n",
    "        \n",
    "        # Handle AND operations\n",
    "        if ' and ' in query:\n",
    "            parts = [p.strip() for p in query.split(' and ')]\n",
    "            result = None\n",
    "            for part in parts:\n",
    "                if part.startswith('not '):\n",
    "                    # Handle AND NOT\n",
    "                    term = part[4:].strip()\n",
    "                    term_docs = self.get_docs_for_term(term)\n",
    "                    not_docs = self.boolean_not(term_docs)\n",
    "                    if result is None:\n",
    "                        result = not_docs\n",
    "                    else:\n",
    "                        result = self.boolean_and(result, not_docs)\n",
    "                else:\n",
    "                    term_docs = self.get_docs_for_term(part)\n",
    "                    if result is None:\n",
    "                        result = term_docs\n",
    "                    else:\n",
    "                        result = self.boolean_and(result, term_docs)\n",
    "            self.query_cache[query] = (\"AND\", result if result else set())\n",
    "            return \"AND\", result if result else set()\n",
    "        \n",
    "        # Handle OR operations\n",
    "        if ' or ' in query:\n",
    "            parts = [p.strip() for p in query.split(' or ')]\n",
    "            result = set()\n",
    "            for part in parts:\n",
    "                if part.startswith('not '):\n",
    "                    term = part[4:].strip()\n",
    "                    term_docs = self.get_docs_for_term(term)\n",
    "                    not_docs = self.boolean_not(term_docs)\n",
    "                    result = self.boolean_or(result, not_docs)\n",
    "                else:\n",
    "                    term_docs = self.get_docs_for_term(part)\n",
    "                    result = self.boolean_or(result, term_docs)\n",
    "            self.query_cache[query] = (\"OR\", result)\n",
    "            return \"OR\", result\n",
    "        \n",
    "        # Single term query\n",
    "        docs = self.get_docs_for_term(query)\n",
    "        self.query_cache[query] = (\"TERM\", docs)\n",
    "        return \"TERM\", docs\n",
    "    \n",
    "    def search(self, query: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Execute boolean search query\n",
    "        \n",
    "        Returns:\n",
    "            List of document information for matching documents\n",
    "        \"\"\"\n",
    "        if not query or not query.strip():\n",
    "            print(\"‚ùå Empty query\")\n",
    "            return []\n",
    "        \n",
    "        operation, result_docs = self.simple_parse_query(query)\n",
    "        \n",
    "        print(f\"\\nüìä Query: '{query}'\")\n",
    "        print(f\"   Operation: {operation}\")\n",
    "        print(f\"   Documents found: {len(result_docs)}\")\n",
    "        \n",
    "        if not result_docs:\n",
    "            return []\n",
    "        \n",
    "        # Convert to list of document info\n",
    "        results = []\n",
    "        for doc_id in result_docs:\n",
    "            doc_info = self.documents.get(doc_id)\n",
    "            if doc_info:\n",
    "                results.append({\n",
    "                    'doc_id': doc_id,\n",
    "                    'name': doc_info['name'],\n",
    "                    'token_count': doc_info['token_count']\n",
    "                })\n",
    "        \n",
    "        # Sort by document name\n",
    "        results.sort(key=lambda x: x['name'])\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def show_document_text(self, doc_name: str, preview_lines: int = 10):\n",
    "        \"\"\"Show text of a specific document\"\"\"\n",
    "        # First, check if we have cleaned_docs folder\n",
    "        cleaned_docs_folder = os.path.join(self.corpus_folder, \"cleaned_docs\")\n",
    "        \n",
    "        if not os.path.exists(cleaned_docs_folder):\n",
    "            # Try alternative locations\n",
    "            possible_locations = [\n",
    "                os.path.join(self.corpus_folder, \"..\", \"cleaned_docs\"),\n",
    "                os.path.join(self.corpus_folder, \"..\", \"..\", \"cleaned_corpus\", \"cleaned_docs\"),\n",
    "                os.path.join(os.path.dirname(self.corpus_folder), \"cleaned_docs\"),\n",
    "            ]\n",
    "            \n",
    "            for location in possible_locations:\n",
    "                if os.path.exists(location):\n",
    "                    cleaned_docs_folder = location\n",
    "                    break\n",
    "        \n",
    "        doc_path = os.path.join(cleaned_docs_folder, doc_name)\n",
    "        \n",
    "        if not os.path.exists(doc_path):\n",
    "            # Try to find the document in the original text folder\n",
    "            print(f\"‚ùå Document not found in cleaned_docs: {doc_name}\")\n",
    "            print(\"Looking for document in alternative locations...\")\n",
    "            \n",
    "            # Try original text folder\n",
    "            original_folder = r\"C:\\Users\\Armaghan Rafique\\Desktop\\AI Project\\supreme_court_judgements_txt\"\n",
    "            if os.path.exists(original_folder):\n",
    "                original_path = os.path.join(original_folder, doc_name)\n",
    "                if os.path.exists(original_path):\n",
    "                    doc_path = original_path\n",
    "                    print(f\"‚úÖ Found document in original folder\")\n",
    "                else:\n",
    "                    print(f\"‚ùå Document not found in: {original_folder}\")\n",
    "                    return\n",
    "            else:\n",
    "                print(f\"‚ùå Original folder not found: {original_folder}\")\n",
    "                return\n",
    "        \n",
    "        try:\n",
    "            with open(doc_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                content = f.read()\n",
    "            \n",
    "            print(f\"\\n\" + \"=\" * 80)\n",
    "            print(f\"üìÑ DOCUMENT PREVIEW: {doc_name}\")\n",
    "            print(\"=\" * 80)\n",
    "            \n",
    "            # Show metadata if present\n",
    "            if content.startswith('='):\n",
    "                # Extract metadata\n",
    "                lines = content.split('\\n')\n",
    "                meta_end = 0\n",
    "                for i, line in enumerate(lines):\n",
    "                    if 'TEXT CONTENT:' in line or 'TEXT:' in line:\n",
    "                        meta_end = i + 1\n",
    "                        break\n",
    "                \n",
    "                if meta_end > 0:\n",
    "                    print(\"\\nüìã METADATA:\")\n",
    "                    for line in lines[:min(meta_end, 8)]:\n",
    "                        if line.strip():\n",
    "                            print(f\"  {line}\")\n",
    "            \n",
    "            # Show actual text content\n",
    "            print(\"\\nüìù TEXT CONTENT (first {} lines):\".format(preview_lines))\n",
    "            print(\"-\" * 80)\n",
    "            \n",
    "            # Split into lines and show content\n",
    "            lines = content.split('\\n')\n",
    "            text_start = 0\n",
    "            for i, line in enumerate(lines):\n",
    "                if 'TEXT CONTENT:' in line or 'TEXT:' in line:\n",
    "                    text_start = i + 2  # Skip the separator line\n",
    "                    break\n",
    "            \n",
    "            lines_to_show = lines[text_start:text_start + preview_lines]\n",
    "            for i, line in enumerate(lines_to_show):\n",
    "                if line.strip():\n",
    "                    # Clean and truncate line\n",
    "                    clean_line = re.sub(r'\\s+', ' ', line.strip())\n",
    "                    if len(clean_line) > 120:\n",
    "                        print(f\"{i+1:3d}. {clean_line[:117]}...\")\n",
    "                    else:\n",
    "                        print(f\"{i+1:3d}. {clean_line}\")\n",
    "            \n",
    "            if len(lines) > text_start + preview_lines:\n",
    "                print(f\"\\n... and {len(lines) - (text_start + preview_lines)} more lines\")\n",
    "            \n",
    "            print(\"=\" * 80)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error reading document: {e}\")\n",
    "    \n",
    "    def get_term_statistics(self, term: str):\n",
    "        \"\"\"Get statistics for a specific term\"\"\"\n",
    "        term_lower = term.lower()\n",
    "        \n",
    "        if term_lower not in self.inverted_index:\n",
    "            print(f\"\\n‚ùå Term '{term}' not found in vocabulary\")\n",
    "            print(f\"   Similar terms: {[t for t in self.inverted_index.keys() if term_lower in t][:5]}\")\n",
    "            return\n",
    "        \n",
    "        docs = self.inverted_index[term_lower]\n",
    "        total_freq = 0\n",
    "        \n",
    "        print(f\"\\nüìä STATISTICS FOR TERM: '{term}'\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"   Document frequency: {len(docs):,} documents\")\n",
    "        \n",
    "        # Calculate total frequency across all documents\n",
    "        for doc_id in docs:\n",
    "            total_freq += self.term_frequencies[doc_id].get(term_lower, 0)\n",
    "        \n",
    "        print(f\"   Total frequency: {total_freq:,} occurrences\")\n",
    "        print(f\"   Average per document: {total_freq/len(docs):.1f} occurrences\")\n",
    "        \n",
    "        # Show top documents containing the term\n",
    "        print(f\"\\nüìÑ TOP DOCUMENTS CONTAINING '{term}':\")\n",
    "        doc_freqs = []\n",
    "        for doc_id in docs:\n",
    "            freq = self.term_frequencies[doc_id].get(term_lower, 0)\n",
    "            doc_name = self.documents[doc_id]['name']\n",
    "            doc_freqs.append((doc_name, freq, doc_id))\n",
    "        \n",
    "        # Sort by frequency (descending)\n",
    "        doc_freqs.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        for i, (doc_name, freq, doc_id) in enumerate(doc_freqs[:10], 1):\n",
    "            doc_info = self.documents[doc_id]\n",
    "            print(f\"   {i:2d}. {doc_name}\")\n",
    "            print(f\"       Frequency: {freq} | Total tokens: {doc_info['token_count']:,}\")\n",
    "        \n",
    "        if len(doc_freqs) > 10:\n",
    "            print(f\"   ... and {len(doc_freqs) - 10} more documents\")\n",
    "        \n",
    "        print(\"=\" * 50)\n",
    "    \n",
    "    def find_documents_with_term(self, term: str, limit: int = 20):\n",
    "        \"\"\"Find documents containing a specific term\"\"\"\n",
    "        term_lower = term.lower()\n",
    "        \n",
    "        if term_lower not in self.inverted_index:\n",
    "            print(f\"‚ùå Term '{term}' not found\")\n",
    "            return []\n",
    "        \n",
    "        docs = list(self.inverted_index[term_lower])\n",
    "        print(f\"\\nüîç Found {len(docs)} documents containing '{term}':\")\n",
    "        \n",
    "        results = []\n",
    "        for i, doc_id in enumerate(docs[:limit], 1):\n",
    "            doc_info = self.documents[doc_id]\n",
    "            freq = self.term_frequencies[doc_id].get(term_lower, 0)\n",
    "            print(f\"  {i:3d}. {doc_info['name']} ({freq} occurrences)\")\n",
    "            results.append({\n",
    "                'name': doc_info['name'],\n",
    "                'frequency': freq,\n",
    "                'token_count': doc_info['token_count']\n",
    "            })\n",
    "        \n",
    "        if len(docs) > limit:\n",
    "            print(f\"  ... and {len(docs) - limit} more documents\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def interactive_search(self):\n",
    "        \"\"\"Interactive search interface\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"üîç BOOLEAN RETRIEVAL SYSTEM - INTERACTIVE SEARCH\")\n",
    "        print(\"=\" * 80)\n",
    "        print(\"\\nüìã Available Commands:\")\n",
    "        print(\"  ‚Ä¢ search <query>      - Search for documents\")\n",
    "        print(\"  ‚Ä¢ stats <term>        - Get statistics for a term\")\n",
    "        print(\"  ‚Ä¢ find <term>         - Find documents containing a term\")\n",
    "        print(\"  ‚Ä¢ preview <doc_name>  - Preview a specific document\")\n",
    "        print(\"  ‚Ä¢ terms               - Show most common terms\")\n",
    "        print(\"  ‚Ä¢ quit                - Exit the program\")\n",
    "        print(\"\\nüìù Query Examples:\")\n",
    "        print(\"  ‚Ä¢ evidence\")\n",
    "        print(\"  ‚Ä¢ murder AND evidence\")\n",
    "        print(\"  ‚Ä¢ murder OR homicide\")\n",
    "        print(\"  ‚Ä¢ murder AND NOT evidence\")\n",
    "        print(\"  ‚Ä¢ (murder OR killing) AND weapon\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        while True:\n",
    "            user_input = input(\"\\nüéØ Enter command: \").strip()\n",
    "            \n",
    "            if not user_input:\n",
    "                continue\n",
    "            \n",
    "            if user_input.lower() == 'quit':\n",
    "                print(\"üëã Goodbye!\")\n",
    "                break\n",
    "            \n",
    "            elif user_input.lower() == 'terms':\n",
    "                # Show most common terms\n",
    "                common_terms = []\n",
    "                for term, docs in self.inverted_index.items():\n",
    "                    total_freq = 0\n",
    "                    for doc_id in docs:\n",
    "                        total_freq += self.term_frequencies[doc_id].get(term, 0)\n",
    "                    common_terms.append((term, len(docs), total_freq))\n",
    "                \n",
    "                # Sort by document frequency\n",
    "                common_terms.sort(key=lambda x: x[1], reverse=True)\n",
    "                \n",
    "                print(f\"\\nüìä TOP 20 MOST COMMON TERMS:\")\n",
    "                print(\"=\" * 60)\n",
    "                print(f\"{'Term':<20} {'Docs':<10} {'Total Freq':<12}\")\n",
    "                print(\"-\" * 60)\n",
    "                for term, doc_count, total_freq in common_terms[:20]:\n",
    "                    print(f\"{term:<20} {doc_count:<10,} {total_freq:<12,}\")\n",
    "                print(\"=\" * 60)\n",
    "            \n",
    "            elif user_input.lower().startswith('search '):\n",
    "                query = user_input[7:].strip()\n",
    "                if query:\n",
    "                    results = self.search(query)\n",
    "                    if results:\n",
    "                        print(f\"\\n‚úÖ Found {len(results)} document(s):\")\n",
    "                        for i, doc in enumerate(results, 1):\n",
    "                            print(f\"  {i:3d}. {doc['name']} ({doc['token_count']:,} tokens)\")\n",
    "                        \n",
    "                        # Ask for preview\n",
    "                        if results:\n",
    "                            preview_choice = input(\"\\nüìñ Preview a document? (enter number or 'n'): \").strip()\n",
    "                            if preview_choice.lower() != 'n' and preview_choice.isdigit():\n",
    "                                idx = int(preview_choice) - 1\n",
    "                                if 0 <= idx < len(results):\n",
    "                                    self.show_document_text(results[idx]['name'])\n",
    "                    else:\n",
    "                        print(\"‚ùå No documents found\")\n",
    "                else:\n",
    "                    print(\"‚ùå Please enter a search query\")\n",
    "            \n",
    "            elif user_input.lower().startswith('stats '):\n",
    "                term = user_input[6:].strip()\n",
    "                if term:\n",
    "                    self.get_term_statistics(term)\n",
    "                else:\n",
    "                    print(\"‚ùå Please enter a term\")\n",
    "            \n",
    "            elif user_input.lower().startswith('find '):\n",
    "                term = user_input[5:].strip()\n",
    "                if term:\n",
    "                    self.find_documents_with_term(term)\n",
    "                else:\n",
    "                    print(\"‚ùå Please enter a term\")\n",
    "            \n",
    "            elif user_input.lower().startswith('preview '):\n",
    "                doc_name = user_input[8:].strip()\n",
    "                if doc_name:\n",
    "                    self.show_document_text(doc_name)\n",
    "                else:\n",
    "                    print(\"‚ùå Please enter a document name\")\n",
    "            \n",
    "            else:\n",
    "                # Try as a search query\n",
    "                results = self.search(user_input)\n",
    "                if results:\n",
    "                    print(f\"\\n‚úÖ Found {len(results)} document(s):\")\n",
    "                    for i, doc in enumerate(results[:20], 1):\n",
    "                        print(f\"  {i:3d}. {doc['name']} ({doc['token_count']:,} tokens)\")\n",
    "                    if len(results) > 20:\n",
    "                        print(f\"  ... and {len(results) - 20} more documents\")\n",
    "                else:\n",
    "                    print(f\"‚ùå No results found for: {user_input}\")\n",
    "                    print(\"   Type 'help' to see available commands\")\n",
    "    \n",
    "    def test_queries(self):\n",
    "        \"\"\"Test various boolean queries\"\"\"\n",
    "        test_queries = [\n",
    "            (\"Single term\", \"evidence\"),\n",
    "            (\"AND operation\", \"murder AND evidence\"),\n",
    "            (\"OR operation\", \"murder OR homicide\"),\n",
    "            (\"NOT operation\", \"NOT murder\"),\n",
    "            (\"Complex query\", \"(murder OR homicide) AND evidence\"),\n",
    "        ]\n",
    "        \n",
    "        print(\"\\nüß™ TESTING BOOLEAN QUERIES\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        for test_name, query in test_queries:\n",
    "            print(f\"\\nüìã Test: {test_name}\")\n",
    "            print(f\"   Query: {query}\")\n",
    "            \n",
    "            results = self.search(query)\n",
    "            \n",
    "            if results:\n",
    "                print(f\"   ‚úÖ Found {len(results)} documents\")\n",
    "                print(f\"   Sample documents:\")\n",
    "                for doc in results[:3]:\n",
    "                    print(f\"     ‚Ä¢ {doc['name']} ({doc['token_count']:,} tokens)\")\n",
    "                if len(results) > 3:\n",
    "                    print(f\"     ... and {len(results) - 3} more\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå No documents found\")\n",
    "            \n",
    "            print(\"-\" * 40)\n",
    "\n",
    "def find_corpus_folder():\n",
    "    \"\"\"Find the corpus folder automatically\"\"\"\n",
    "    possible_paths = [\n",
    "        # Primary location\n",
    "        r\"C:\\Users\\Armaghan Rafique\\Desktop\\AI Project\\cleaned_corpus\",\n",
    "        # Alternative locations\n",
    "        r\"C:\\Users\\Armaghan Rafique\\Desktop\\AI Project\\supreme_court_judgements_txt\\cleaned_corpus\",\n",
    "        # If cleaned_corpus doesn't exist, try the text folder\n",
    "        r\"C:\\Users\\Armaghan Rafique\\Desktop\\AI Project\\supreme_court_judgements_txt\",\n",
    "    ]\n",
    "    \n",
    "    for path in possible_paths:\n",
    "        if os.path.exists(path):\n",
    "            print(f\"‚úÖ Found corpus folder: {path}\")\n",
    "            \n",
    "            # Check if it has the necessary structure\n",
    "            if \"document_tokens.json\" in os.listdir(path) or \\\n",
    "               os.path.exists(os.path.join(path, \"document_tokens.json\")):\n",
    "                return path\n",
    "            else:\n",
    "                # Check subdirectories\n",
    "                for root, dirs, files in os.walk(path):\n",
    "                    if \"document_tokens.json\" in files:\n",
    "                        print(f\"‚úÖ Found document_tokens.json in: {root}\")\n",
    "                        return root\n",
    "    \n",
    "    print(\"‚ùå Could not find corpus folder automatically\")\n",
    "    return None\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function\"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"‚öñÔ∏è  SUPREME COURT - BOOLEAN RETRIEVAL SYSTEM\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Try to find corpus folder automatically\n",
    "    corpus_folder = find_corpus_folder()\n",
    "    \n",
    "    if not corpus_folder:\n",
    "        # Manual input\n",
    "        corpus_folder = input(\"\\nüìÅ Enter corpus folder path: \").strip()\n",
    "        if not os.path.exists(corpus_folder):\n",
    "            print(f\"‚ùå Folder does not exist: {corpus_folder}\")\n",
    "            return\n",
    "    \n",
    "    print(f\"\\nüìÅ Using corpus folder: {corpus_folder}\")\n",
    "    \n",
    "    # Create and initialize retrieval system\n",
    "    retrieval_system = BooleanRetrievalSystem(corpus_folder)\n",
    "    \n",
    "    # Load or build index\n",
    "    if not retrieval_system.load_index():\n",
    "        print(\"‚ùå Failed to initialize retrieval system\")\n",
    "        return\n",
    "    \n",
    "    # Show system info\n",
    "    print(f\"\\nüìä SYSTEM READY\")\n",
    "    print(f\"   Documents: {retrieval_system.stats['total_documents']:,}\")\n",
    "    print(f\"   Vocabulary: {retrieval_system.stats['vocabulary_size']:,}\")\n",
    "    print(f\"   Total terms: {retrieval_system.stats['total_terms']:,}\")\n",
    "    \n",
    "    # Ask user what they want to do\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"üéØ What would you like to do?\")\n",
    "    print(\"  1. Test predefined queries\")\n",
    "    print(\"  2. Start interactive search\")\n",
    "    print(\"  3. Find documents for a specific term\")\n",
    "    print(\"  4. Get statistics for a term\")\n",
    "    \n",
    "    choice = input(\"\\nSelect option (1-4): \").strip()\n",
    "    \n",
    "    if choice == '1':\n",
    "        retrieval_system.test_queries()\n",
    "        # Then go to interactive\n",
    "        retrieval_system.interactive_search()\n",
    "    elif choice == '2':\n",
    "        retrieval_system.interactive_search()\n",
    "    elif choice == '3':\n",
    "        term = input(\"Enter term to find documents: \").strip()\n",
    "        if term:\n",
    "            retrieval_system.find_documents_with_term(term)\n",
    "        retrieval_system.interactive_search()\n",
    "    elif choice == '4':\n",
    "        term = input(\"Enter term for statistics: \").strip()\n",
    "        if term:\n",
    "            retrieval_system.get_term_statistics(term)\n",
    "        retrieval_system.interactive_search()\n",
    "    else:\n",
    "        # Default to interactive search\n",
    "        retrieval_system.interactive_search()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7038a248-3fd4-4e99-9aaf-2aa3629b5df9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
