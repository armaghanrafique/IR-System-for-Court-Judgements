{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "438f17cf-b3fa-4f5d-b7db-060f3dc53040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ¥‡ SUPREME COURT - TF-IDF DOCUMENT RANKING SYSTEM\n",
      "================================================================================\n",
      "\n",
      "ğŸ“ Using corpus folder: C:\\Users\\Armaghan Rafique\\Desktop\\AI Project\\cleaned_corpus\n",
      "\n",
      "ğŸ“‚ Initializing TF-IDF ranking system...\n",
      "âŒ Index not found at: C:\\Users\\Armaghan Rafique\\Desktop\\AI Project\\cleaned_corpus\\tfidf_ranking\\tfidf_ranking.pkl\n",
      "Building new index...\n",
      "ğŸ”¨ Calculating TF-IDF Scores for Ranking...\n",
      "âŒ No documents loaded. Loading documents first...\n",
      "ğŸ“‚ Loading documents...\n",
      "âœ… Loaded 1460 documents\n",
      "ğŸ“Š Total terms: 1,048,901\n",
      "ğŸ“Š Vectorizer parameters: {'max_features': 10000, 'min_df': 2, 'max_df': 0.95, 'stop_words': 'english', 'ngram_range': (1, 2), 'sublinear_tf': False, 'norm': None, 'use_idf': True, 'smooth_idf': True}\n",
      "\n",
      "ğŸ’¾ Index saved to: C:\\Users\\Armaghan Rafique\\Desktop\\AI Project\\cleaned_corpus\\tfidf_ranking\\tfidf_ranking.pkl\n",
      "ğŸ“Š Statistics saved to: C:\\Users\\Armaghan Rafique\\Desktop\\AI Project\\cleaned_corpus\\tfidf_ranking\\tfidf_ranking_stats.json\n",
      "\n",
      "âœ… TF-IDF Scores calculated successfully!\n",
      "   Documents: 1,460\n",
      "   Vocabulary: 10,000\n",
      "   TF-IDF matrix shape: (1460, 10000)\n",
      "\n",
      "ğŸ“ˆ TF-IDF STATISTICS:\n",
      "--------------------------------------------------\n",
      "Average TF-IDF score per document: 0.1904\n",
      "Maximum TF-IDF score: 120.1605\n",
      "Minimum TF-IDF score: 0.0000\n",
      "\n",
      "ğŸ“Š Top 10 terms by IDF (most discriminating):\n",
      "   1. aata                 IDF: 7.188\n",
      "   2. abbott               IDF: 7.188\n",
      "   3. abbott motor         IDF: 7.188\n",
      "   4. academy              IDF: 7.188\n",
      "   5. accused ehsan        IDF: 7.188\n",
      "   6. acre                 IDF: 7.188\n",
      "   7. action program       IDF: 7.188\n",
      "   8. adbp                 IDF: 7.188\n",
      "   9. admin finance        IDF: 7.188\n",
      "  10. affectees jsr        IDF: 7.188\n",
      "--------------------------------------------------\n",
      "\n",
      "ğŸ“Š TF-IDF RANKING SYSTEM READY\n",
      "   Documents: 1,460\n",
      "   Vocabulary: 10,000\n",
      "   Total terms: 1,048,901\n",
      "\n",
      "================================================================================\n",
      "ğŸ¥‡ TF-IDF DOCUMENT RANKING SYSTEM\n",
      "================================================================================\n",
      "\n",
      "ğŸ“‹ Available Commands:\n",
      "  â€¢ rank <query>           - Rank documents by query relevance\n",
      "  â€¢ keywords <word1 word2> - Rank documents by keywords\n",
      "  â€¢ terms <doc_name>       - Show top TF-IDF terms for document\n",
      "  â€¢ analyze <doc_name>     - Show TF-IDF analysis for document\n",
      "  â€¢ stats                  - Show ranking statistics\n",
      "  â€¢ rebuild                - Recalculate TF-IDF scores\n",
      "  â€¢ quit                   - Exit\n",
      "\n",
      "ğŸ“ Example commands:\n",
      "  â€¢ rank murder evidence\n",
      "  â€¢ keywords supreme court appeal\n",
      "  â€¢ terms 2025LHC7277.txt\n",
      "  â€¢ analyze 2025LHC7389.txt\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ¯ Enter command:  quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ‘‹ Goodbye!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List, Dict, Set, Tuple\n",
    "import math\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer as SklearnTfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import scipy.sparse as sp\n",
    "\n",
    "class SupremeCourtTFIDFSystem:\n",
    "    \"\"\"\n",
    "    TF-IDF Vectorization and Search System for Supreme Court Documents\n",
    "    Integrates with existing Boolean and N-gram systems\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, corpus_folder: str):\n",
    "        \"\"\"\n",
    "        Initialize TF-IDF System\n",
    "        \n",
    "        Args:\n",
    "            corpus_folder: Path to the cleaned_corpus folder\n",
    "        \"\"\"\n",
    "        self.corpus_folder = corpus_folder\n",
    "        self.index_folder = os.path.join(corpus_folder, \"tfidf_index\")\n",
    "        \n",
    "        # Create index folder if it doesn't exist\n",
    "        if not os.path.exists(self.index_folder):\n",
    "            os.makedirs(self.index_folder)\n",
    "        \n",
    "        # Data structures\n",
    "        self.documents = {}  # doc_id -> document info\n",
    "        self.doc_texts = {}  # doc_id -> full text\n",
    "        self.doc_tokens = {}  # doc_id -> list of tokens\n",
    "        \n",
    "        # TF-IDF matrices\n",
    "        self.tfidf_matrix = None  # Sparse TF-IDF matrix\n",
    "        self.feature_names = []  # Vocabulary\n",
    "        self.vectorizer = None   # Scikit-learn vectorizer\n",
    "        \n",
    "        # Statistics\n",
    "        self.stats = {\n",
    "            'total_documents': 0,\n",
    "            'vocabulary_size': 0,\n",
    "            'total_terms': 0\n",
    "        }\n",
    "        \n",
    "        # Document mapping\n",
    "        self.doc_id_to_index = {}\n",
    "        self.index_to_doc_id = {}\n",
    "        self.doc_names = []\n",
    "    \n",
    "    def load_documents(self):\n",
    "        \"\"\"Load documents from the corpus\"\"\"\n",
    "        print(\"ğŸ“‚ Loading documents...\")\n",
    "        \n",
    "        # Try to find document tokens file\n",
    "        doc_tokens_file = os.path.join(self.corpus_folder, \"document_tokens.json\")\n",
    "        \n",
    "        if not os.path.exists(doc_tokens_file):\n",
    "            print(f\"âŒ Document tokens file not found: {doc_tokens_file}\")\n",
    "            \n",
    "            # Try alternative locations\n",
    "            alt_locations = [\n",
    "                os.path.join(self.corpus_folder, \"..\", \"document_tokens.json\"),\n",
    "                os.path.join(self.corpus_folder, \"statistics\", \"document_tokens.json\"),\n",
    "            ]\n",
    "            \n",
    "            for location in alt_locations:\n",
    "                if os.path.exists(location):\n",
    "                    doc_tokens_file = location\n",
    "                    print(f\"âœ… Found at: {doc_tokens_file}\")\n",
    "                    break\n",
    "        \n",
    "        if not os.path.exists(doc_tokens_file):\n",
    "            print(\"âŒ Could not find document tokens file\")\n",
    "            return False\n",
    "        \n",
    "        # Load document tokens\n",
    "        try:\n",
    "            with open(doc_tokens_file, 'r', encoding='utf-8') as f:\n",
    "                doc_data = json.load(f)\n",
    "            \n",
    "            doc_id = 0\n",
    "            for doc_name, doc_info in doc_data.items():\n",
    "                doc_id += 1\n",
    "                doc_key = f\"doc_{doc_id:05d}\"\n",
    "                \n",
    "                tokens = doc_info.get('tokens', [])\n",
    "                token_count = doc_info.get('token_count', 0)\n",
    "                \n",
    "                if tokens and token_count > 0:\n",
    "                    self.documents[doc_key] = {\n",
    "                        'name': doc_name,\n",
    "                        'token_count': token_count\n",
    "                    }\n",
    "                    \n",
    "                    # Store tokens as text for vectorization\n",
    "                    self.doc_texts[doc_key] = \" \".join(tokens)\n",
    "                    self.doc_tokens[doc_key] = tokens\n",
    "            \n",
    "            self.stats['total_documents'] = len(self.documents)\n",
    "            self.stats['total_terms'] = sum(doc['token_count'] for doc in self.documents.values())\n",
    "            \n",
    "            print(f\"âœ… Loaded {self.stats['total_documents']} documents\")\n",
    "            print(f\"ğŸ“Š Total terms: {self.stats['total_terms']:,}\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error loading documents: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def build_tfidf_index(self, max_features: int = 10000, **kwargs):\n",
    "        \"\"\"\n",
    "        Build TF-IDF index using scikit-learn\n",
    "        \n",
    "        Args:\n",
    "            max_features: Maximum number of features to keep\n",
    "            **kwargs: Additional parameters for TfidfVectorizer\n",
    "        \"\"\"\n",
    "        print(\"ğŸ”¨ Building TF-IDF Index...\")\n",
    "        \n",
    "        if not self.doc_texts:\n",
    "            print(\"âŒ No documents loaded. Loading documents first...\")\n",
    "            if not self.load_documents():\n",
    "                return False\n",
    "        \n",
    "        # Prepare document texts\n",
    "        doc_ids = sorted(self.doc_texts.keys())\n",
    "        doc_names = [self.documents[doc_id]['name'] for doc_id in doc_ids]\n",
    "        doc_texts = [self.doc_texts[doc_id] for doc_id in doc_ids]\n",
    "        \n",
    "        # Configure TF-IDF vectorizer\n",
    "        vectorizer_kwargs = {\n",
    "            'max_features': max_features,\n",
    "            'min_df': 2,  # Ignore terms that appear in less than 2 documents\n",
    "            'max_df': 0.95,  # Ignore terms that appear in more than 95% of documents\n",
    "            'stop_words': 'english',\n",
    "            'ngram_range': (1, 2),  # Use unigrams and bigrams\n",
    "            'sublinear_tf': True,  # Use 1 + log(tf)\n",
    "            'norm': 'l2',  # Normalize vectors to unit length\n",
    "            'use_idf': True,  # Use IDF weighting\n",
    "            'smooth_idf': True,  # Smooth IDF weights\n",
    "            **kwargs\n",
    "        }\n",
    "        \n",
    "        print(f\"ğŸ“Š Vectorizer parameters: {vectorizer_kwargs}\")\n",
    "        \n",
    "        # Create and fit vectorizer\n",
    "        self.vectorizer = SklearnTfidfVectorizer(**vectorizer_kwargs)\n",
    "        self.tfidf_matrix = self.vectorizer.fit_transform(doc_texts)\n",
    "        \n",
    "        # Get feature names\n",
    "        self.feature_names = self.vectorizer.get_feature_names_out()\n",
    "        \n",
    "        # Update statistics\n",
    "        self.stats['vocabulary_size'] = len(self.feature_names)\n",
    "        \n",
    "        # Store document mapping\n",
    "        self.doc_id_to_index = {doc_id: idx for idx, doc_id in enumerate(doc_ids)}\n",
    "        self.index_to_doc_id = {idx: doc_id for idx, doc_id in enumerate(doc_ids)}\n",
    "        self.doc_names = doc_names\n",
    "        \n",
    "        # Save index\n",
    "        self.save_index()\n",
    "        \n",
    "        print(f\"\\nâœ… TF-IDF Index built successfully!\")\n",
    "        print(f\"   Documents: {self.stats['total_documents']:,}\")\n",
    "        print(f\"   Vocabulary: {self.stats['vocabulary_size']:,}\")\n",
    "        print(f\"   TF-IDF matrix shape: {self.tfidf_matrix.shape}\")\n",
    "        print(f\"   Sparsity: {(1 - self.tfidf_matrix.nnz / (self.tfidf_matrix.shape[0] * self.tfidf_matrix.shape[1])) * 100:.1f}%\")\n",
    "        \n",
    "        # Show sample features\n",
    "        sample_features = self.feature_names[:10]\n",
    "        print(f\"   Sample features: {', '.join(sample_features)}\")\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def save_index(self):\n",
    "        \"\"\"Save TF-IDF index to disk\"\"\"\n",
    "        if self.vectorizer is None or self.tfidf_matrix is None:\n",
    "            print(\"âŒ No index to save\")\n",
    "            return\n",
    "        \n",
    "        # Save vectorizer and matrix\n",
    "        index_data = {\n",
    "            'vectorizer': self.vectorizer,\n",
    "            'tfidf_matrix': self.tfidf_matrix,\n",
    "            'documents': self.documents,\n",
    "            'doc_texts': self.doc_texts,\n",
    "            'doc_tokens': self.doc_tokens,\n",
    "            'feature_names': self.feature_names,\n",
    "            'stats': self.stats,\n",
    "            'doc_id_to_index': self.doc_id_to_index,\n",
    "            'index_to_doc_id': self.index_to_doc_id,\n",
    "            'doc_names': self.doc_names\n",
    "        }\n",
    "        \n",
    "        index_file = os.path.join(self.index_folder, \"tfidf_index.pkl\")\n",
    "        with open(index_file, 'wb') as f:\n",
    "            pickle.dump(index_data, f)\n",
    "        \n",
    "        # Also save a human-readable version\n",
    "        readable_file = os.path.join(self.index_folder, \"tfidf_stats.json\")\n",
    "        readable_data = {\n",
    "            'stats': self.stats,\n",
    "            'matrix_shape': self.tfidf_matrix.shape,\n",
    "            'sample_features': self.feature_names[:50].tolist() if hasattr(self.feature_names, 'tolist') else self.feature_names[:50],\n",
    "            'document_count': len(self.documents)\n",
    "        }\n",
    "        \n",
    "        with open(readable_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(readable_data, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"\\nğŸ’¾ Index saved to: {index_file}\")\n",
    "        print(f\"ğŸ“Š Statistics saved to: {readable_file}\")\n",
    "    \n",
    "    def load_index(self):\n",
    "        \"\"\"Load TF-IDF index from disk\"\"\"\n",
    "        index_file = os.path.join(self.index_folder, \"tfidf_index.pkl\")\n",
    "        \n",
    "        if not os.path.exists(index_file):\n",
    "            print(f\"âŒ Index not found at: {index_file}\")\n",
    "            print(\"Building new index...\")\n",
    "            return self.build_tfidf_index()\n",
    "        \n",
    "        try:\n",
    "            print(f\"ğŸ“‚ Loading TF-IDF index from: {index_file}\")\n",
    "            with open(index_file, 'rb') as f:\n",
    "                index_data = pickle.load(f)\n",
    "            \n",
    "            self.vectorizer = index_data['vectorizer']\n",
    "            self.tfidf_matrix = index_data['tfidf_matrix']\n",
    "            self.documents = index_data['documents']\n",
    "            self.doc_texts = index_data['doc_texts']\n",
    "            self.doc_tokens = index_data['doc_tokens']\n",
    "            self.feature_names = index_data['feature_names']\n",
    "            self.stats = index_data['stats']\n",
    "            self.doc_id_to_index = index_data['doc_id_to_index']\n",
    "            self.index_to_doc_id = index_data['index_to_doc_id']\n",
    "            self.doc_names = index_data['doc_names']\n",
    "            \n",
    "            print(f\"âœ… TF-IDF Index loaded successfully!\")\n",
    "            print(f\"   Documents: {self.stats['total_documents']:,}\")\n",
    "            print(f\"   Vocabulary: {self.stats['vocabulary_size']:,}\")\n",
    "            print(f\"   Matrix shape: {self.tfidf_matrix.shape}\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error loading index: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return self.build_tfidf_index()\n",
    "    \n",
    "    def search_similar_documents(self, query: str, top_k: int = 10) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Search for documents similar to query using cosine similarity\n",
    "        \n",
    "        Args:\n",
    "            query: Search query string\n",
    "            top_k: Number of top results to return\n",
    "            \n",
    "        Returns:\n",
    "            List of similar documents with similarity scores\n",
    "        \"\"\"\n",
    "        if self.vectorizer is None or self.tfidf_matrix is None:\n",
    "            print(\"âŒ TF-IDF index not loaded\")\n",
    "            return []\n",
    "        \n",
    "        # Transform query to TF-IDF vector\n",
    "        query_vector = self.vectorizer.transform([query])\n",
    "        \n",
    "        # Calculate cosine similarity with all documents\n",
    "        similarities = cosine_similarity(query_vector, self.tfidf_matrix).flatten()\n",
    "        \n",
    "        # Get top K similar documents\n",
    "        top_indices = similarities.argsort()[::-1][:top_k]\n",
    "        \n",
    "        results = []\n",
    "        for rank, idx in enumerate(top_indices, 1):\n",
    "            doc_id = self.index_to_doc_id[idx]\n",
    "            doc_info = self.documents[doc_id]\n",
    "            similarity = similarities[idx]\n",
    "            \n",
    "            # Calculate query term relevance\n",
    "            query_terms = query.lower().split()\n",
    "            doc_text = self.doc_texts[doc_id].lower()\n",
    "            query_terms_found = [term for term in query_terms if term in doc_text]\n",
    "            \n",
    "            results.append({\n",
    "                'doc_id': doc_id,\n",
    "                'name': doc_info['name'],\n",
    "                'token_count': doc_info['token_count'],\n",
    "                'similarity_score': similarity,\n",
    "                'query_terms_found': query_terms_found,\n",
    "                'terms_found_count': len(query_terms_found),\n",
    "                'rank': rank,\n",
    "                'relevance_score': self.calculate_relevance_score(similarity, len(query_terms_found), len(query_terms))\n",
    "            })\n",
    "        \n",
    "        # Sort by relevance score (higher is better)\n",
    "        results.sort(key=lambda x: x['relevance_score'], reverse=True)\n",
    "        \n",
    "        # Re-rank based on final relevance score\n",
    "        for i, result in enumerate(results, 1):\n",
    "            result['rank'] = i\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def calculate_relevance_score(self, similarity: float, terms_found: int, total_terms: int) -> float:\n",
    "        \"\"\"\n",
    "        Calculate comprehensive relevance score\n",
    "        \n",
    "        Args:\n",
    "            similarity: Cosine similarity score\n",
    "            terms_found: Number of query terms found in document\n",
    "            total_terms: Total number of query terms\n",
    "            \n",
    "        Returns:\n",
    "            Combined relevance score\n",
    "        \"\"\"\n",
    "        if total_terms == 0:\n",
    "            return similarity\n",
    "        \n",
    "        # Term coverage score (0-1)\n",
    "        term_coverage = terms_found / total_terms\n",
    "        \n",
    "        # Weighted combination: 70% similarity + 30% term coverage\n",
    "        relevance = (0.7 * similarity) + (0.3 * term_coverage)\n",
    "        \n",
    "        return relevance\n",
    "    \n",
    "    def find_similar_to_document(self, doc_name: str, top_k: int = 10) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Find documents similar to a specific document\n",
    "        \n",
    "        Args:\n",
    "            doc_name: Name of the document\n",
    "            top_k: Number of top results to return\n",
    "            \n",
    "        Returns:\n",
    "            List of similar documents\n",
    "        \"\"\"\n",
    "        if self.vectorizer is None or self.tfidf_matrix is None:\n",
    "            print(\"âŒ TF-IDF index not loaded\")\n",
    "            return []\n",
    "        \n",
    "        # Find the document\n",
    "        target_doc_id = None\n",
    "        for doc_id, info in self.documents.items():\n",
    "            if info['name'] == doc_name:\n",
    "                target_doc_id = doc_id\n",
    "                break\n",
    "        \n",
    "        if target_doc_id is None:\n",
    "            print(f\"âŒ Document not found: {doc_name}\")\n",
    "            return []\n",
    "        \n",
    "        # Get document index\n",
    "        if target_doc_id not in self.doc_id_to_index:\n",
    "            print(f\"âŒ Document not in TF-IDF matrix: {doc_name}\")\n",
    "            return []\n",
    "        \n",
    "        doc_idx = self.doc_id_to_index[target_doc_id]\n",
    "        \n",
    "        # Calculate cosine similarity with all other documents\n",
    "        doc_vector = self.tfidf_matrix[doc_idx:doc_idx+1]\n",
    "        similarities = cosine_similarity(doc_vector, self.tfidf_matrix).flatten()\n",
    "        \n",
    "        # Set self-similarity to 0 to avoid returning the same document\n",
    "        similarities[doc_idx] = 0\n",
    "        \n",
    "        # Get top K similar documents\n",
    "        top_indices = similarities.argsort()[::-1][:top_k]\n",
    "        \n",
    "        results = []\n",
    "        for rank, idx in enumerate(top_indices, 1):\n",
    "            if idx == doc_idx:\n",
    "                continue\n",
    "                \n",
    "            doc_id = self.index_to_doc_id[idx]\n",
    "            doc_info = self.documents[doc_id]\n",
    "            similarity = similarities[idx]\n",
    "            \n",
    "            # Calculate document overlap\n",
    "            target_tokens = set(self.doc_tokens[target_doc_id])\n",
    "            other_tokens = set(self.doc_tokens[doc_id])\n",
    "            overlap_ratio = len(target_tokens.intersection(other_tokens)) / len(target_tokens.union(other_tokens)) if target_tokens.union(other_tokens) else 0\n",
    "            \n",
    "            results.append({\n",
    "                'doc_id': doc_id,\n",
    "                'name': doc_info['name'],\n",
    "                'token_count': doc_info['token_count'],\n",
    "                'similarity_score': similarity,\n",
    "                'overlap_ratio': overlap_ratio,\n",
    "                'rank': rank\n",
    "            })\n",
    "        \n",
    "        # Sort by similarity score (already sorted, but ensure)\n",
    "        results.sort(key=lambda x: x['similarity_score'], reverse=True)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_top_terms_for_document(self, doc_name: str, top_n: int = 10) -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        Get top TF-IDF terms for a specific document\n",
    "        \n",
    "        Args:\n",
    "            doc_name: Name of the document\n",
    "            top_n: Number of top terms to return\n",
    "            \n",
    "        Returns:\n",
    "            List of (term, tfidf_score) tuples\n",
    "        \"\"\"\n",
    "        if self.vectorizer is None or self.tfidf_matrix is None:\n",
    "            print(\"âŒ TF-IDF index not loaded\")\n",
    "            return []\n",
    "        \n",
    "        # Find the document\n",
    "        target_doc_id = None\n",
    "        for doc_id, info in self.documents.items():\n",
    "            if info['name'] == doc_name:\n",
    "                target_doc_id = doc_id\n",
    "                break\n",
    "        \n",
    "        if target_doc_id is None:\n",
    "            print(f\"âŒ Document not found: {doc_name}\")\n",
    "            return []\n",
    "        \n",
    "        # Get document index\n",
    "        if target_doc_id not in self.doc_id_to_index:\n",
    "            print(f\"âŒ Document not in TF-IDF matrix: {doc_name}\")\n",
    "            return []\n",
    "        \n",
    "        doc_idx = self.doc_id_to_index[target_doc_id]\n",
    "        \n",
    "        # Get document vector\n",
    "        doc_vector = self.tfidf_matrix[doc_idx]\n",
    "        \n",
    "        # Convert to dense if sparse\n",
    "        if sp.issparse(doc_vector):\n",
    "            doc_vector = doc_vector.toarray().flatten()\n",
    "        \n",
    "        # Get top term indices\n",
    "        top_indices = doc_vector.argsort()[::-1][:top_n]\n",
    "        \n",
    "        # Get term names and scores\n",
    "        top_terms = []\n",
    "        for idx in top_indices:\n",
    "            if doc_vector[idx] > 0:\n",
    "                term = self.feature_names[idx]\n",
    "                score = doc_vector[idx]\n",
    "                top_terms.append((term, score))\n",
    "        \n",
    "        return top_terms\n",
    "    \n",
    "    def search_by_keywords(self, keywords: List[str], top_k: int = 10) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Search for documents containing specific keywords using TF-IDF weights\n",
    "        \n",
    "        Args:\n",
    "            keywords: List of keywords to search for\n",
    "            top_k: Number of top results to return\n",
    "            \n",
    "        Returns:\n",
    "            List of documents with relevance scores\n",
    "        \"\"\"\n",
    "        if self.vectorizer is None or self.tfidf_matrix is None:\n",
    "            print(\"âŒ TF-IDF index not loaded\")\n",
    "            return []\n",
    "        \n",
    "        # Create a query that emphasizes the keywords\n",
    "        query = \" \".join(keywords)\n",
    "        \n",
    "        # Transform query to TF-IDF vector\n",
    "        query_vector = self.vectorizer.transform([query])\n",
    "        \n",
    "        # Calculate cosine similarity\n",
    "        similarities = cosine_similarity(query_vector, self.tfidf_matrix).flatten()\n",
    "        \n",
    "        # Get top K documents\n",
    "        top_indices = similarities.argsort()[::-1][:top_k]\n",
    "        \n",
    "        results = []\n",
    "        for rank, idx in enumerate(top_indices, 1):\n",
    "            doc_id = self.index_to_doc_id[idx]\n",
    "            doc_info = self.documents[doc_id]\n",
    "            similarity = similarities[idx]\n",
    "            \n",
    "            # Check if document contains any of the keywords\n",
    "            keywords_found = []\n",
    "            doc_text = self.doc_texts[doc_id].lower()\n",
    "            for keyword in keywords:\n",
    "                if keyword.lower() in doc_text:\n",
    "                    keywords_found.append(keyword)\n",
    "            \n",
    "            # Calculate keyword density\n",
    "            keyword_density = 0\n",
    "            if doc_info['token_count'] > 0:\n",
    "                total_occurrences = sum(doc_text.count(keyword.lower()) for keyword in keywords)\n",
    "                keyword_density = total_occurrences / doc_info['token_count']\n",
    "            \n",
    "            results.append({\n",
    "                'doc_id': doc_id,\n",
    "                'name': doc_info['name'],\n",
    "                'token_count': doc_info['token_count'],\n",
    "                'similarity_score': similarity,\n",
    "                'keywords_found': keywords_found,\n",
    "                'keywords_count': len(keywords_found),\n",
    "                'keyword_density': keyword_density,\n",
    "                'rank': rank,\n",
    "                'relevance_score': self.calculate_keyword_relevance(similarity, len(keywords_found), len(keywords), keyword_density)\n",
    "            })\n",
    "        \n",
    "        # Sort by relevance score\n",
    "        results.sort(key=lambda x: x['relevance_score'], reverse=True)\n",
    "        \n",
    "        # Re-rank\n",
    "        for i, result in enumerate(results, 1):\n",
    "            result['rank'] = i\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def calculate_keyword_relevance(self, similarity: float, found_count: int, total_keywords: int, density: float) -> float:\n",
    "        \"\"\"\n",
    "        Calculate relevance score for keyword search\n",
    "        \n",
    "        Args:\n",
    "            similarity: Cosine similarity\n",
    "            found_count: Number of keywords found\n",
    "            total_keywords: Total number of keywords\n",
    "            density: Keyword density in document\n",
    "            \n",
    "        Returns:\n",
    "            Combined relevance score\n",
    "        \"\"\"\n",
    "        if total_keywords == 0:\n",
    "            return similarity\n",
    "        \n",
    "        # Keyword coverage score\n",
    "        coverage = found_count / total_keywords\n",
    "        \n",
    "        # Weighted combination\n",
    "        relevance = (0.5 * similarity) + (0.3 * coverage) + (0.2 * min(density * 10, 1.0))\n",
    "        \n",
    "        return relevance\n",
    "    \n",
    "    def show_document_preview(self, doc_name: str, preview_lines: int = 10):\n",
    "        \"\"\"Show preview of a document\"\"\"\n",
    "        # Try to find the document in cleaned_docs folder\n",
    "        cleaned_docs_folder = os.path.join(self.corpus_folder, \"cleaned_docs\")\n",
    "        \n",
    "        if not os.path.exists(cleaned_docs_folder):\n",
    "            # Try alternative locations\n",
    "            possible_locations = [\n",
    "                os.path.join(self.corpus_folder, \"..\", \"cleaned_docs\"),\n",
    "                os.path.join(os.path.dirname(self.corpus_folder), \"cleaned_docs\"),\n",
    "                r\"C:\\\\Users\\\\Armaghan Rafique\\\\Desktop\\\\AI Project\\\\cleaned_corpus\\\\cleaned_docs\",\n",
    "            ]\n",
    "            \n",
    "            for location in possible_locations:\n",
    "                if os.path.exists(location):\n",
    "                    cleaned_docs_folder = location\n",
    "                    break\n",
    "        \n",
    "        doc_path = os.path.join(cleaned_docs_folder, doc_name)\n",
    "        \n",
    "        if not os.path.exists(doc_path):\n",
    "            print(f\"âŒ Document not found: {doc_path}\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            with open(doc_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                content = f.read()\n",
    "            \n",
    "            print(f\"\\n\" + \"=\" * 80)\n",
    "            print(f\"ğŸ“„ DOCUMENT PREVIEW: {doc_name}\")\n",
    "            print(\"=\" * 80)\n",
    "            \n",
    "            # Extract and show text content\n",
    "            lines = content.split('\\n')\n",
    "            \n",
    "            # Find where actual text starts (skip metadata)\n",
    "            text_start = 0\n",
    "            for i, line in enumerate(lines):\n",
    "                if 'TEXT CONTENT:' in line or 'TEXT:' in line:\n",
    "                    text_start = i + 1\n",
    "                    break\n",
    "            \n",
    "            print(f\"\\nğŸ“ First {preview_lines} lines of content:\")\n",
    "            print(\"-\" * 80)\n",
    "            \n",
    "            for i, line in enumerate(lines[text_start:text_start + preview_lines]):\n",
    "                if line.strip():\n",
    "                    clean_line = re.sub(r'\\s+', ' ', line.strip())\n",
    "                    if len(clean_line) > 120:\n",
    "                        print(f\"{i+1:3d}. {clean_line[:117]}...\")\n",
    "                    else:\n",
    "                        print(f\"{i+1:3d}. {clean_line}\")\n",
    "            \n",
    "            if len(lines) > text_start + preview_lines:\n",
    "                print(f\"\\n... and {len(lines) - (text_start + preview_lines)} more lines\")\n",
    "            \n",
    "            print(\"=\" * 80)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error reading document: {e}\")\n",
    "    \n",
    "    def show_statistics(self):\n",
    "        \"\"\"Show TF-IDF system statistics\"\"\"\n",
    "        print(f\"\\nğŸ“Š TF-IDF SYSTEM STATISTICS\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"Documents: {self.stats['total_documents']:,}\")\n",
    "        print(f\"Vocabulary: {self.stats['vocabulary_size']:,}\")\n",
    "        print(f\"Total terms: {self.stats['total_terms']:,}\")\n",
    "        \n",
    "        if self.tfidf_matrix is not None:\n",
    "            print(f\"TF-IDF matrix shape: {self.tfidf_matrix.shape}\")\n",
    "            print(f\"Matrix density: {self.tfidf_matrix.nnz / (self.tfidf_matrix.shape[0] * self.tfidf_matrix.shape[1]) * 100:.2f}%\")\n",
    "        \n",
    "        # Show most common terms\n",
    "        if self.vectorizer is not None and hasattr(self.vectorizer, 'idf_'):\n",
    "            print(f\"\\nğŸ“ˆ Most important terms (highest IDF scores):\")\n",
    "            \n",
    "            # Get terms with highest IDF scores\n",
    "            feature_names = self.feature_names\n",
    "            idf_scores = self.vectorizer.idf_\n",
    "            \n",
    "            # Sort by IDF (descending)\n",
    "            top_indices = idf_scores.argsort()[::-1][:20]\n",
    "            \n",
    "            for i, idx in enumerate(top_indices[:10], 1):\n",
    "                term = feature_names[idx]\n",
    "                score = idf_scores[idx]\n",
    "                print(f\"  {i:2d}. {term:<20} IDF: {score:.3f}\")\n",
    "        \n",
    "        print(\"=\" * 80)\n",
    "    \n",
    "    def interactive_search(self):\n",
    "        \"\"\"Interactive TF-IDF search interface\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"ğŸ” TF-IDF VECTOR SPACE SEARCH SYSTEM\")\n",
    "        print(\"=\" * 80)\n",
    "        print(\"\\nğŸ“‹ Available Commands:\")\n",
    "        print(\"  â€¢ search <query>           - Search for documents similar to query\")\n",
    "        print(\"  â€¢ similar <doc_name>       - Find documents similar to a specific document\")\n",
    "        print(\"  â€¢ keywords <word1 word2>   - Search by keywords\")\n",
    "        print(\"  â€¢ terms <doc_name>         - Show top terms for a document\")\n",
    "        print(\"  â€¢ preview <doc_name>       - Preview a document\")\n",
    "        print(\"  â€¢ stats                    - Show system statistics\")\n",
    "        print(\"  â€¢ rebuild                  - Rebuild TF-IDF index\")\n",
    "        print(\"  â€¢ quit                     - Exit\")\n",
    "        print(\"\\nğŸ“ Example searches:\")\n",
    "        print(\"  â€¢ search murder evidence trial\")\n",
    "        print(\"  â€¢ similar 2025LHC7277.txt\")\n",
    "        print(\"  â€¢ keywords supreme court appeal\")\n",
    "        print(\"  â€¢ terms 2025LHC7389.txt\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        while True:\n",
    "            user_input = input(\"\\nğŸ¯ Enter command: \").strip()\n",
    "            \n",
    "            if not user_input:\n",
    "                continue\n",
    "            \n",
    "            if user_input.lower() == 'quit':\n",
    "                print(\"ğŸ‘‹ Goodbye!\")\n",
    "                break\n",
    "            \n",
    "            elif user_input.lower() == 'stats':\n",
    "                self.show_statistics()\n",
    "            \n",
    "            elif user_input.lower() == 'rebuild':\n",
    "                confirm = input(\"âš ï¸  Rebuild TF-IDF index? This may take time. (y/n): \").strip().lower()\n",
    "                if confirm == 'y':\n",
    "                    max_features = input(\"Enter max features (default 10000): \").strip()\n",
    "                    max_features = int(max_features) if max_features.isdigit() else 10000\n",
    "                    self.build_tfidf_index(max_features=max_features)\n",
    "            \n",
    "            elif user_input.lower().startswith('search '):\n",
    "                query = user_input[7:].strip()\n",
    "                if query:\n",
    "                    print(f\"\\nğŸ” Searching for: '{query}'\")\n",
    "                    results = self.search_similar_documents(query, top_k=15)\n",
    "                    self.display_results(results, \"Search Results\")\n",
    "                else:\n",
    "                    print(\"âŒ Please enter a search query\")\n",
    "            \n",
    "            elif user_input.lower().startswith('similar '):\n",
    "                doc_name = user_input[8:].strip()\n",
    "                if doc_name:\n",
    "                    print(f\"\\nğŸ” Finding documents similar to: '{doc_name}'\")\n",
    "                    results = self.find_similar_to_document(doc_name, top_k=15)\n",
    "                    self.display_results(results, f\"Similar to {doc_name}\")\n",
    "                else:\n",
    "                    print(\"âŒ Please enter a document name\")\n",
    "            \n",
    "            elif user_input.lower().startswith('keywords '):\n",
    "                keywords = user_input[9:].strip().split()\n",
    "                if keywords:\n",
    "                    print(f\"\\nğŸ” Searching by keywords: {keywords}\")\n",
    "                    results = self.search_by_keywords(keywords, top_k=15)\n",
    "                    self.display_results(results, f\"Keywords: {', '.join(keywords)}\")\n",
    "                else:\n",
    "                    print(\"âŒ Please enter keywords\")\n",
    "            \n",
    "            elif user_input.lower().startswith('terms '):\n",
    "                doc_name = user_input[6:].strip()\n",
    "                if doc_name:\n",
    "                    print(f\"\\nğŸ”¤ Top terms for: '{doc_name}'\")\n",
    "                    top_terms = self.get_top_terms_for_document(doc_name, top_n=15)\n",
    "                    \n",
    "                    if top_terms:\n",
    "                        print(\"\\n\" + \"=\" * 80)\n",
    "                        print(f\"ğŸ“Š TOP TERMS FOR: {doc_name}\")\n",
    "                        print(\"=\" * 80)\n",
    "                        \n",
    "                        for i, (term, score) in enumerate(top_terms, 1):\n",
    "                            print(f\"{i:2d}. {term:<25} TF-IDF: {score:.4f}\")\n",
    "                        \n",
    "                        print(\"=\" * 80)\n",
    "                    else:\n",
    "                        print(\"âŒ No terms found or document not in index\")\n",
    "                else:\n",
    "                    print(\"âŒ Please enter a document name\")\n",
    "            \n",
    "            elif user_input.lower().startswith('preview '):\n",
    "                doc_name = user_input[8:].strip()\n",
    "                if doc_name:\n",
    "                    self.show_document_preview(doc_name)\n",
    "                else:\n",
    "                    print(\"âŒ Please enter a document name\")\n",
    "            \n",
    "            else:\n",
    "                # Try as a search query\n",
    "                print(f\"\\nğŸ” Searching for: '{user_input}'\")\n",
    "                results = self.search_similar_documents(user_input, top_k=10)\n",
    "                self.display_results(results, \"Search Results\")\n",
    "    \n",
    "    def display_results(self, results: List[Dict], title: str):\n",
    "        \"\"\"Display search results\"\"\"\n",
    "        if not results:\n",
    "            print(f\"\\nâŒ No results found\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\nâœ… {title}\")\n",
    "        print(f\"ğŸ“Š Found {len(results)} document(s)\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        for i, result in enumerate(results[:15], 1):  # Show only top 15\n",
    "            print(f\"\\n{i:2d}. ğŸ“„ {result['name']}\")\n",
    "            print(f\"    ğŸ“ Length: {result['token_count']:,} tokens\")\n",
    "            print(f\"    ğŸ¥‡ Rank: #{result['rank']}\")\n",
    "            print(f\"    â­ Similarity: {result['similarity_score']:.4f}\")\n",
    "            \n",
    "            if 'relevance_score' in result:\n",
    "                print(f\"    ğŸ¯ Relevance: {result['relevance_score']:.3f}\")\n",
    "            \n",
    "            if 'query_terms_found' in result and result['query_terms_found']:\n",
    "                print(f\"    ğŸ” Query terms found: {', '.join(result['query_terms_found'][:5])}\")\n",
    "                if len(result['query_terms_found']) > 5:\n",
    "                    print(f\"       ... and {len(result['query_terms_found']) - 5} more\")\n",
    "            \n",
    "            if 'keywords_found' in result and result['keywords_found']:\n",
    "                print(f\"    ğŸ”‘ Keywords found: {', '.join(result['keywords_found'][:3])}\")\n",
    "                if len(result['keywords_found']) > 3:\n",
    "                    print(f\"       ... and {len(result['keywords_found']) - 3} more\")\n",
    "                if 'keyword_density' in result:\n",
    "                    print(f\"    ğŸ“ˆ Keyword density: {result['keyword_density']:.4f}\")\n",
    "        \n",
    "        if len(results) > 15:\n",
    "            print(f\"\\n... and {len(results) - 15} more documents\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        \n",
    "        # Ask for document preview\n",
    "        if results:\n",
    "            choice = input(\"\\nğŸ“– Preview a document? (enter number or 'n'): \").strip()\n",
    "            if choice.lower() != 'n' and choice.isdigit():\n",
    "                idx = int(choice) - 1\n",
    "                if 0 <= idx < len(results):\n",
    "                    self.show_document_preview(results[idx]['name'])\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function\"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"ğŸ” SUPREME COURT - TF-IDF VECTOR SPACE SEARCH SYSTEM\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Set corpus folder path\n",
    "    corpus_folder = r\"C:\\Users\\Armaghan Rafique\\Desktop\\AI Project\\cleaned_corpus\"\n",
    "    \n",
    "    # Check if folder exists\n",
    "    if not os.path.exists(corpus_folder):\n",
    "        print(f\"âŒ Corpus folder not found: {corpus_folder}\")\n",
    "        \n",
    "        # Try alternative locations\n",
    "        alt_folders = [\n",
    "            r\"C:\\Users\\Armaghan Rafique\\Desktop\\AI Project\\supreme_court_judgements_txt\",\n",
    "            r\"C:\\Users\\Armaghan Rafique\\Desktop\\AI Project\",\n",
    "            os.path.join(os.path.expanduser(\"~\"), \"Desktop\", \"AI Project\", \"cleaned_corpus\")\n",
    "        ]\n",
    "        \n",
    "        found = False\n",
    "        for folder in alt_folders:\n",
    "            if os.path.exists(folder):\n",
    "                corpus_folder = folder\n",
    "                print(f\"âœ… Using folder: {corpus_folder}\")\n",
    "                found = True\n",
    "                break\n",
    "        \n",
    "        if not found:\n",
    "            corpus_folder = input(\"ğŸ“ Enter corpus folder path: \").strip()\n",
    "            if not os.path.exists(corpus_folder):\n",
    "                print(f\"âŒ Folder does not exist: {corpus_folder}\")\n",
    "                return\n",
    "    \n",
    "    print(f\"\\nğŸ“ Using corpus folder: {corpus_folder}\")\n",
    "    \n",
    "    # Create TF-IDF system\n",
    "    tfidf_system = SupremeCourtTFIDFSystem(corpus_folder)\n",
    "    \n",
    "    # Load or build index\n",
    "    print(\"\\nğŸ“‚ Initializing TF-IDF system...\")\n",
    "    if not tfidf_system.load_index():\n",
    "        print(\"âŒ Failed to initialize TF-IDF system\")\n",
    "        return\n",
    "    \n",
    "    # Show system info\n",
    "    print(f\"\\nğŸ“Š TF-IDF SYSTEM READY\")\n",
    "    print(f\"   Documents: {tfidf_system.stats['total_documents']:,}\")\n",
    "    print(f\"   Vocabulary: {tfidf_system.stats['vocabulary_size']:,}\")\n",
    "    print(f\"   Total terms: {tfidf_system.stats['total_terms']:,}\")\n",
    "    \n",
    "    # Start interactive search\n",
    "    tfidf_system.interactive_search()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8d6f6a-f9e6-440b-969c-60be8195e4dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ¥‡ SUPREME COURT - TF-IDF DOCUMENT RANKING SYSTEM\n",
      "================================================================================\n",
      "\n",
      "ğŸ“ Using corpus folder: C:\\Users\\Armaghan Rafique\\Desktop\\AI Project\\cleaned_corpus\n",
      "\n",
      "ğŸ“‚ Initializing TF-IDF ranking system...\n",
      "ğŸ“‚ Loading TF-IDF ranking index from: C:\\Users\\Armaghan Rafique\\Desktop\\AI Project\\cleaned_corpus\\tfidf_ranking\\tfidf_ranking.pkl\n",
      "âœ… TF-IDF Ranking Index loaded successfully!\n",
      "   Documents: 1,460\n",
      "   Vocabulary: 10,000\n",
      "   Matrix shape: (1460, 10000)\n",
      "\n",
      "ğŸ“Š TF-IDF RANKING SYSTEM READY\n",
      "   Documents: 1,460\n",
      "   Vocabulary: 10,000\n",
      "   Total terms: 1,048,901\n",
      "\n",
      "================================================================================\n",
      "ğŸ¥‡ TF-IDF DOCUMENT RANKING SYSTEM\n",
      "================================================================================\n",
      "\n",
      "ğŸ“‹ Available Commands:\n",
      "  â€¢ rank <query>           - Rank documents by query relevance\n",
      "  â€¢ keywords <word1 word2> - Rank documents by keywords\n",
      "  â€¢ terms <doc_name>       - Show top TF-IDF terms for document\n",
      "  â€¢ analyze <doc_name>     - Show TF-IDF analysis for document\n",
      "  â€¢ stats                  - Show ranking statistics\n",
      "  â€¢ rebuild                - Recalculate TF-IDF scores\n",
      "  â€¢ quit                   - Exit\n",
      "\n",
      "ğŸ“ Example commands:\n",
      "  â€¢ rank murder evidence\n",
      "  â€¢ keywords supreme court appeal\n",
      "  â€¢ terms 2025LHC7277.txt\n",
      "  â€¢ analyze 2025LHC7389.txt\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ¯ Enter command:  police\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” Ranking documents for query: 'police'\n",
      "\n",
      "ğŸ” Ranking documents for query: 'police'\n",
      "\n",
      "âœ… Ranking for: 'police'\n",
      "ğŸ“Š Ranked 15 document(s) by TF-IDF relevance\n",
      "================================================================================\n",
      "\n",
      " 1. ğŸ“„ SMC_No_102017_Shoulder_Out_20of_20Turn_20Promotion.txt\n",
      "    ğŸ¥‡ Rank: #1\n",
      "    ğŸ“ Length: 19,904 tokens\n",
      "    â­ TF-IDF Score: 148.8222\n",
      "    ğŸ“Š Normalized Score: 15.0345\n",
      "    ğŸ” Terms matched: 0/1\n",
      "\n",
      " 2. ğŸ“„ SHOULDEROUT_OF_TURN_PROMOTION_IN_GB_POLICE_Shoulder_Out_20of_20Turn_20Promotion.txt\n",
      "    ğŸ¥‡ Rank: #2\n",
      "    ğŸ“ Length: 19,904 tokens\n",
      "    â­ TF-IDF Score: 148.8222\n",
      "    ğŸ“Š Normalized Score: 15.0345\n",
      "    ğŸ” Terms matched: 0/1\n",
      "\n",
      " 3. ğŸ“„ Civil_Appeal_No_802016_in_CPLA_No1172016_Prov._20Govt_20__20others_20vs_20Rehmat_20Jan_20DSP_20__20others.txt\n",
      "    ğŸ¥‡ Rank: #3\n",
      "    ğŸ“ Length: 871 tokens\n",
      "    â­ TF-IDF Score: 91.5829\n",
      "    ğŸ“Š Normalized Score: 13.5262\n",
      "    ğŸ” Terms matched: 0/1\n",
      "\n",
      " 4. ğŸ“„ Provincial_Government_through_Chief_Secretary_Gilg_Prov._20Govt_20__20others_20vs_20Rehmat_20Jan_20DSP_20__20others.txt\n",
      "    ğŸ¥‡ Rank: #4\n",
      "    ğŸ“ Length: 871 tokens\n",
      "    â­ TF-IDF Score: 91.5829\n",
      "    ğŸ“Š Normalized Score: 13.5262\n",
      "    ğŸ” Terms matched: 0/1\n",
      "\n",
      " 5. ğŸ“„ CPLA_Under_Objection_No_132019_Syed_Hamid_Hussai_26._20judgment_20of_20Syed_20Hamid_20Hussain.txt\n",
      "    ğŸ¥‡ Rank: #5\n",
      "    ğŸ“ Length: 1,320 tokens\n",
      "    â­ TF-IDF Score: 80.1350\n",
      "    ğŸ“Š Normalized Score: 11.1513\n",
      "    ğŸ” Terms matched: 0/1\n",
      "\n",
      " 6. ğŸ“„ SMC_NO222011_COMLAINT_AGAINST_PWD_smc_20no._2022-2011.txt\n",
      "    ğŸ¥‡ Rank: #6\n",
      "    ğŸ“ Length: 519 tokens\n",
      "    â­ TF-IDF Score: 68.6872\n",
      "    ğŸ“Š Normalized Score: 10.9832\n",
      "    ğŸ” Terms matched: 0/1\n",
      "\n",
      " 7. ğŸ“„ The_State_versus_Haider_The_20State_20versus_20Haider.txt\n",
      "    ğŸ¥‡ Rank: #7\n",
      "    ğŸ“ Length: 698 tokens\n",
      "    â­ TF-IDF Score: 68.6872\n",
      "    ğŸ“Š Normalized Score: 10.4871\n",
      "    ğŸ” Terms matched: 0/1\n",
      "\n",
      " 8. ğŸ“„ Cr_Appeal_No_042016_In_Cr_PLA_No_052016_The_20State_20versus_20Haider.txt\n",
      "    ğŸ¥‡ Rank: #8\n",
      "    ğŸ“ Length: 698 tokens\n",
      "    â­ TF-IDF Score: 68.6872\n",
      "    ğŸ“Š Normalized Score: 10.4871\n",
      "    ğŸ” Terms matched: 0/1\n",
      "\n",
      " 9. ğŸ“„ Cr_PLA_NO_202011__6_._Cr._PLA_No._20-2011_Noshad_v._Incharge_Police_Chowki_Nomal_Gilgit_and_others.txt\n",
      "    ğŸ¥‡ Rank: #9\n",
      "    ğŸ“ Length: 1,219 tokens\n",
      "    â­ TF-IDF Score: 68.6872\n",
      "    ğŸ“Š Normalized Score: 9.6653\n",
      "    ğŸ” Terms matched: 0/1\n",
      "\n",
      "10. ğŸ“„ Mst_Noshad_wo_Afsar_Jan_VERSUS_Incharge_Police_C__6_._Cr._PLA_No._20-2011_Noshad_v._Incharge_Police_Chowki_Nomal_Gilgit_and_others.txt\n",
      "    ğŸ¥‡ Rank: #10\n",
      "    ğŸ“ Length: 1,219 tokens\n",
      "    â­ TF-IDF Score: 68.6872\n",
      "    ğŸ“Š Normalized Score: 9.6653\n",
      "    ğŸ” Terms matched: 0/1\n",
      "\n",
      "11. ğŸ“„ CPLA_Under_Objection_No_852019_Govt_of_Gilgit-B_11._20judgement_20of_20Akhtar_20Hussain_20Changazi.txt\n",
      "    ğŸ¥‡ Rank: #11\n",
      "    ğŸ“ Length: 1,394 tokens\n",
      "    â­ TF-IDF Score: 68.6872\n",
      "    ğŸ“Š Normalized Score: 9.4863\n",
      "    ğŸ” Terms matched: 0/1\n",
      "\n",
      "12. ğŸ“„ Civil_Misc_No_032016_Civil_Misc_No_022016_Tahira_Yasub_vs_provincial_Governemnt.txt\n",
      "    ğŸ¥‡ Rank: #12\n",
      "    ğŸ“ Length: 3,108 tokens\n",
      "    â­ TF-IDF Score: 74.4111\n",
      "    ğŸ“Š Normalized Score: 9.2527\n",
      "    ğŸ” Terms matched: 0/1\n",
      "\n",
      "13. ğŸ“„ Tahira_Yasub_DSP_18_others_Versus_Government_of_Tahira_Yasub_vs_provincial_Governemnt.txt\n",
      "    ğŸ¥‡ Rank: #13\n",
      "    ğŸ“ Length: 3,108 tokens\n",
      "    â­ TF-IDF Score: 74.4111\n",
      "    ğŸ“Š Normalized Score: 9.2527\n",
      "    ğŸ” Terms matched: 0/1\n",
      "\n",
      "14. ğŸ“„ Cr_Appeal_No_052015Cr_Appeal_No_062015Cr_baba_20jan_20__20others.txt\n",
      "    ğŸ¥‡ Rank: #14\n",
      "    ğŸ“ Length: 13,424 tokens\n",
      "    â­ TF-IDF Score: 85.8589\n",
      "    ğŸ“Š Normalized Score: 9.0331\n",
      "    ğŸ” Terms matched: 0/1\n",
      "\n",
      "15. ğŸ“„ The_State_Versus_Iftikhar_Hussain_The_State_Versu_baba_20jan_20__20others.txt\n",
      "    ğŸ¥‡ Rank: #15\n",
      "    ğŸ“ Length: 13,424 tokens\n",
      "    â­ TF-IDF Score: 85.8589\n",
      "    ğŸ“Š Normalized Score: 9.0331\n",
      "    ğŸ” Terms matched: 0/1\n",
      "\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Analyze a document? (enter rank number or 'n'):  n\n",
      "\n",
      "ğŸ¯ Enter command:  skardu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” Ranking documents for query: 'skardu'\n",
      "\n",
      "ğŸ” Ranking documents for query: 'skardu'\n",
      "\n",
      "âœ… Ranking for: 'skardu'\n",
      "ğŸ“Š Ranked 15 document(s) by TF-IDF relevance\n",
      "================================================================================\n",
      "\n",
      " 1. ğŸ“„ Clean_Drinking_water_Clean_20Drinking_20water.txt\n",
      "    ğŸ¥‡ Rank: #1\n",
      "    ğŸ“ Length: 1,105 tokens\n",
      "    â­ TF-IDF Score: 202.3664\n",
      "    ğŸ“Š Normalized Score: 28.8744\n",
      "    ğŸ” Terms matched: 0/1\n",
      "\n",
      " 2. ğŸ“„ SMC_No032009_Clean_20Drinking_20water.txt\n",
      "    ğŸ¥‡ Rank: #2\n",
      "    ğŸ“ Length: 1,105 tokens\n",
      "    â­ TF-IDF Score: 202.3664\n",
      "    ğŸ“Š Normalized Score: 28.8744\n",
      "    ğŸ” Terms matched: 0/1\n",
      "\n",
      " 3. ğŸ“„ The_Deputy_Commissioner_Skardu_Versus_Akhond_Muham_the_Deputy_Commissioner_Skardu___others_versus_Akhond_Muhammad___others.txt\n",
      "    ğŸ¥‡ Rank: #3\n",
      "    ğŸ“ Length: 1,270 tokens\n",
      "    â­ TF-IDF Score: 141.6565\n",
      "    ğŸ“Š Normalized Score: 19.8189\n",
      "    ğŸ” Terms matched: 0/1\n",
      "\n",
      " 4. ğŸ“„ Civil_Appeal_No_152015_in_CPLA_No_492015_the_Deputy_Commissioner_Skardu___others_versus_Akhond_Muhammad___others.txt\n",
      "    ğŸ¥‡ Rank: #4\n",
      "    ğŸ“ Length: 1,270 tokens\n",
      "    â­ TF-IDF Score: 141.6565\n",
      "    ğŸ“Š Normalized Score: 19.8189\n",
      "    ğŸ” Terms matched: 0/1\n",
      "\n",
      " 5. ğŸ“„ CPLA_NO_12011_Collector_20Skardu_20Versus_20Muhammad_20Akber.txt\n",
      "    ğŸ¥‡ Rank: #5\n",
      "    ğŸ“ Length: 676 tokens\n",
      "    â­ TF-IDF Score: 121.4199\n",
      "    ğŸ“Š Normalized Score: 18.6293\n",
      "    ğŸ” Terms matched: 0/1\n",
      "\n",
      " 6. ğŸ“„ Collector_Skardu_Versus_Muhammad_Akbar_Collector_20Skardu_20Versus_20Muhammad_20Akber.txt\n",
      "    ğŸ¥‡ Rank: #6\n",
      "    ğŸ“ Length: 676 tokens\n",
      "    â­ TF-IDF Score: 121.4199\n",
      "    ğŸ“Š Normalized Score: 18.6293\n",
      "    ğŸ” Terms matched: 0/1\n",
      "\n",
      " 7. ğŸ“„ Civil_Petition_for_Leave_to_Appeal_No_882016_Pro_35._20final_20judgment_20of_20wazir_20hassan_20_1_.txt\n",
      "    ğŸ¥‡ Rank: #7\n",
      "    ğŸ“ Length: 1,490 tokens\n",
      "    â­ TF-IDF Score: 131.5382\n",
      "    ğŸ“Š Normalized Score: 18.0012\n",
      "    ğŸ” Terms matched: 0/1\n",
      "\n",
      " 8. ğŸ“„ COMPENSATION_FOR_DAMAGES_CAUSED_BY_THE_OVER_FLOW_A_SMC._10-2010_2016_04_20_12_07_02_356.txt\n",
      "    ğŸ¥‡ Rank: #8\n",
      "    ğŸ“ Length: 617 tokens\n",
      "    â­ TF-IDF Score: 111.3015\n",
      "    ğŸ“Š Normalized Score: 17.3192\n",
      "    ğŸ” Terms matched: 0/1\n",
      "\n",
      " 9. ğŸ“„ SMC_N0_102010_SMC._10-2010_2016_04_20_12_07_02_356.txt\n",
      "    ğŸ¥‡ Rank: #9\n",
      "    ğŸ“ Length: 617 tokens\n",
      "    â­ TF-IDF Score: 111.3015\n",
      "    ğŸ“Š Normalized Score: 17.3192\n",
      "    ğŸ” Terms matched: 0/1\n",
      "\n",
      "10. ğŸ“„ CrPLA_NO_022014_CrPLA_No._02-2014_State_v._Asif_Abbas.txt\n",
      "    ğŸ¥‡ Rank: #10\n",
      "    ğŸ“ Length: 301 tokens\n",
      "    â­ TF-IDF Score: 91.0649\n",
      "    ğŸ“Š Normalized Score: 15.9471\n",
      "    ğŸ” Terms matched: 0/1\n",
      "\n",
      "11. ğŸ“„ The_State_Versus_Asif_Abbass_so_Musa_and_others_CrPLA_No._02-2014_State_v._Asif_Abbas.txt\n",
      "    ğŸ¥‡ Rank: #11\n",
      "    ğŸ“ Length: 301 tokens\n",
      "    â­ TF-IDF Score: 91.0649\n",
      "    ğŸ“Š Normalized Score: 15.9471\n",
      "    ğŸ” Terms matched: 0/1\n",
      "\n",
      "12. ğŸ“„ CPLA_No282019_Provincial_Govt_Versus_Attaullah_23._20final_20judgment_20of_20attaullah2.txt\n",
      "    ğŸ¥‡ Rank: #12\n",
      "    ğŸ“ Length: 631 tokens\n",
      "    â­ TF-IDF Score: 101.1832\n",
      "    ğŸ“Š Normalized Score: 15.6900\n",
      "    ğŸ” Terms matched: 0/1\n",
      "\n",
      "13. ğŸ“„ CPLA_No442020_NHA_through_Chairman_Versus_Affec_19._20NHA_20through_20Chairman_20Vs._20Affectees_20of_20JSR.txt\n",
      "    ğŸ¥‡ Rank: #13\n",
      "    ğŸ“ Length: 1,638 tokens\n",
      "    â­ TF-IDF Score: 101.1832\n",
      "    ğŸ“Š Normalized Score: 13.6700\n",
      "    ğŸ” Terms matched: 0/1\n",
      "\n",
      "14. ğŸ“„ Cr_Appeal_No_082016_In_Cr_PLA_NO_072016_Haleema_20Sadia_20versus_20Shakeel_20Ahmed_20__20others.txt\n",
      "    ğŸ¥‡ Rank: #14\n",
      "    ğŸ“ Length: 1,772 tokens\n",
      "    â­ TF-IDF Score: 101.1832\n",
      "    ğŸ“Š Normalized Score: 13.5264\n",
      "    ğŸ” Terms matched: 0/1\n",
      "\n",
      "15. ğŸ“„ Haleema_Sadia_versus_Shakeel_Ahmed_others_Haleema_20Sadia_20versus_20Shakeel_20Ahmed_20__20others.txt\n",
      "    ğŸ¥‡ Rank: #15\n",
      "    ğŸ“ Length: 1,772 tokens\n",
      "    â­ TF-IDF Score: 101.1832\n",
      "    ğŸ“Š Normalized Score: 13.5264\n",
      "    ğŸ” Terms matched: 0/1\n",
      "\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Analyze a document? (enter rank number or 'n'):  n#\n",
      "\n",
      "ğŸ¯ Enter command:  respondant\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” Ranking documents for query: 'respondant'\n",
      "\n",
      "ğŸ” Ranking documents for query: 'respondant'\n",
      "\n",
      "âŒ No relevant documents found\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List, Dict, Set, Tuple\n",
    "import math\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer as SklearnTfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import scipy.sparse as sp\n",
    "\n",
    "class SupremeCourtTFIDFRanking:\n",
    "    \"\"\"\n",
    "    TF-IDF Ranking System for Supreme Court Documents\n",
    "    Properly ranks documents based on TF-IDF relevance scores\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, corpus_folder: str):\n",
    "        \"\"\"\n",
    "        Initialize TF-IDF Ranking System\n",
    "        \n",
    "        Args:\n",
    "            corpus_folder: Path to the cleaned_corpus folder\n",
    "        \"\"\"\n",
    "        self.corpus_folder = corpus_folder\n",
    "        self.index_folder = os.path.join(corpus_folder, \"tfidf_ranking\")\n",
    "        \n",
    "        # Create index folder if it doesn't exist\n",
    "        if not os.path.exists(self.index_folder):\n",
    "            os.makedirs(self.index_folder)\n",
    "        \n",
    "        # Data structures\n",
    "        self.documents = {}  # doc_id -> document info\n",
    "        self.doc_texts = {}  # doc_id -> full text\n",
    "        self.doc_tokens = {}  # doc_id -> list of tokens\n",
    "        \n",
    "        # TF-IDF components\n",
    "        self.tfidf_matrix = None  # Document-term TF-IDF matrix\n",
    "        self.query_tfidf_matrix = None  # Query-term TF-IDF matrix (for ranking)\n",
    "        self.feature_names = []  # Vocabulary\n",
    "        self.vectorizer = None   # Scikit-learn vectorizer\n",
    "        \n",
    "        # IDF values for ranking\n",
    "        self.idf_values = {}\n",
    "        \n",
    "        # Statistics\n",
    "        self.stats = {\n",
    "            'total_documents': 0,\n",
    "            'vocabulary_size': 0,\n",
    "            'total_terms': 0\n",
    "        }\n",
    "        \n",
    "        # Document mapping\n",
    "        self.doc_id_to_index = {}\n",
    "        self.index_to_doc_id = {}\n",
    "        self.doc_names = []\n",
    "    \n",
    "    def load_documents(self):\n",
    "        \"\"\"Load documents from the corpus\"\"\"\n",
    "        print(\"ğŸ“‚ Loading documents...\")\n",
    "        \n",
    "        # Try to find document tokens file\n",
    "        doc_tokens_file = os.path.join(self.corpus_folder, \"document_tokens.json\")\n",
    "        \n",
    "        if not os.path.exists(doc_tokens_file):\n",
    "            print(f\"âŒ Document tokens file not found: {doc_tokens_file}\")\n",
    "            \n",
    "            # Try alternative locations\n",
    "            alt_locations = [\n",
    "                os.path.join(self.corpus_folder, \"..\", \"document_tokens.json\"),\n",
    "                os.path.join(self.corpus_folder, \"statistics\", \"document_tokens.json\"),\n",
    "            ]\n",
    "            \n",
    "            for location in alt_locations:\n",
    "                if os.path.exists(location):\n",
    "                    doc_tokens_file = location\n",
    "                    print(f\"âœ… Found at: {doc_tokens_file}\")\n",
    "                    break\n",
    "        \n",
    "        if not os.path.exists(doc_tokens_file):\n",
    "            print(\"âŒ Could not find document tokens file\")\n",
    "            return False\n",
    "        \n",
    "        # Load document tokens\n",
    "        try:\n",
    "            with open(doc_tokens_file, 'r', encoding='utf-8') as f:\n",
    "                doc_data = json.load(f)\n",
    "            \n",
    "            doc_id = 0\n",
    "            for doc_name, doc_info in doc_data.items():\n",
    "                doc_id += 1\n",
    "                doc_key = f\"doc_{doc_id:05d}\"\n",
    "                \n",
    "                tokens = doc_info.get('tokens', [])\n",
    "                token_count = doc_info.get('token_count', 0)\n",
    "                \n",
    "                if tokens and token_count > 0:\n",
    "                    self.documents[doc_key] = {\n",
    "                        'name': doc_name,\n",
    "                        'token_count': token_count,\n",
    "                        'tokens': tokens\n",
    "                    }\n",
    "                    \n",
    "                    # Store tokens as text for vectorization\n",
    "                    self.doc_texts[doc_key] = \" \".join(tokens)\n",
    "                    self.doc_tokens[doc_key] = tokens\n",
    "            \n",
    "            self.stats['total_documents'] = len(self.documents)\n",
    "            self.stats['total_terms'] = sum(doc['token_count'] for doc in self.documents.values())\n",
    "            \n",
    "            print(f\"âœ… Loaded {self.stats['total_documents']} documents\")\n",
    "            print(f\"ğŸ“Š Total terms: {self.stats['total_terms']:,}\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error loading documents: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def calculate_tf_idf_scores(self, max_features: int = 10000, **kwargs):\n",
    "        \"\"\"\n",
    "        Calculate TF-IDF scores and build ranking matrix\n",
    "        \n",
    "        Args:\n",
    "            max_features: Maximum number of features to keep\n",
    "            **kwargs: Additional parameters for TfidfVectorizer\n",
    "        \"\"\"\n",
    "        print(\"ğŸ”¨ Calculating TF-IDF Scores for Ranking...\")\n",
    "        \n",
    "        if not self.doc_texts:\n",
    "            print(\"âŒ No documents loaded. Loading documents first...\")\n",
    "            if not self.load_documents():\n",
    "                return False\n",
    "        \n",
    "        # Prepare document texts\n",
    "        doc_ids = sorted(self.doc_texts.keys())\n",
    "        doc_names = [self.documents[doc_id]['name'] for doc_id in doc_ids]\n",
    "        doc_texts = [self.doc_texts[doc_id] for doc_id in doc_ids]\n",
    "        \n",
    "        # Configure TF-IDF vectorizer\n",
    "        vectorizer_kwargs = {\n",
    "            'max_features': max_features,\n",
    "            'min_df': 2,  # Ignore terms that appear in less than 2 documents\n",
    "            'max_df': 0.95,  # Ignore terms that appear in more than 95% of documents\n",
    "            'stop_words': 'english',\n",
    "            'ngram_range': (1, 2),  # Use unigrams and bigrams\n",
    "            'sublinear_tf': False,  # Use raw TF for ranking\n",
    "            'norm': None,  # No normalization for ranking\n",
    "            'use_idf': True,\n",
    "            'smooth_idf': True,\n",
    "            **kwargs\n",
    "        }\n",
    "        \n",
    "        print(f\"ğŸ“Š Vectorizer parameters: {vectorizer_kwargs}\")\n",
    "        \n",
    "        # Create and fit vectorizer\n",
    "        self.vectorizer = SklearnTfidfVectorizer(**vectorizer_kwargs)\n",
    "        self.tfidf_matrix = self.vectorizer.fit_transform(doc_texts)\n",
    "        \n",
    "        # Get feature names and IDF values\n",
    "        self.feature_names = self.vectorizer.get_feature_names_out()\n",
    "        self.idf_values = dict(zip(self.feature_names, self.vectorizer.idf_))\n",
    "        \n",
    "        # Store document mapping\n",
    "        self.doc_id_to_index = {doc_id: idx for idx, doc_id in enumerate(doc_ids)}\n",
    "        self.index_to_doc_id = {idx: doc_id for idx, doc_id in enumerate(doc_ids)}\n",
    "        self.doc_names = doc_names\n",
    "        \n",
    "        # Update statistics\n",
    "        self.stats['vocabulary_size'] = len(self.feature_names)\n",
    "        \n",
    "        # Save index\n",
    "        self.save_index()\n",
    "        \n",
    "        print(f\"\\nâœ… TF-IDF Scores calculated successfully!\")\n",
    "        print(f\"   Documents: {self.stats['total_documents']:,}\")\n",
    "        print(f\"   Vocabulary: {self.stats['vocabulary_size']:,}\")\n",
    "        print(f\"   TF-IDF matrix shape: {self.tfidf_matrix.shape}\")\n",
    "        \n",
    "        # Show TF-IDF statistics\n",
    "        self.show_tfidf_statistics()\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def show_tfidf_statistics(self):\n",
    "        \"\"\"Show TF-IDF statistics\"\"\"\n",
    "        if self.tfidf_matrix is None:\n",
    "            return\n",
    "        \n",
    "        # Convert to dense for analysis (sample only)\n",
    "        sample_matrix = self.tfidf_matrix[:10].toarray()\n",
    "        \n",
    "        print(f\"\\nğŸ“ˆ TF-IDF STATISTICS:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"Average TF-IDF score per document: {sample_matrix.mean():.4f}\")\n",
    "        print(f\"Maximum TF-IDF score: {sample_matrix.max():.4f}\")\n",
    "        print(f\"Minimum TF-IDF score: {sample_matrix.min():.4f}\")\n",
    "        \n",
    "        # Show terms with highest IDF (most discriminating)\n",
    "        sorted_idf = sorted(self.idf_values.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "        print(f\"\\nğŸ“Š Top 10 terms by IDF (most discriminating):\")\n",
    "        for i, (term, idf) in enumerate(sorted_idf, 1):\n",
    "            print(f\"  {i:2d}. {term:<20} IDF: {idf:.3f}\")\n",
    "        \n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    def save_index(self):\n",
    "        \"\"\"Save TF-IDF index to disk\"\"\"\n",
    "        if self.vectorizer is None or self.tfidf_matrix is None:\n",
    "            print(\"âŒ No index to save\")\n",
    "            return\n",
    "        \n",
    "        # Save vectorizer and matrix\n",
    "        index_data = {\n",
    "            'vectorizer': self.vectorizer,\n",
    "            'tfidf_matrix': self.tfidf_matrix,\n",
    "            'documents': self.documents,\n",
    "            'doc_texts': self.doc_texts,\n",
    "            'doc_tokens': self.doc_tokens,\n",
    "            'feature_names': self.feature_names,\n",
    "            'idf_values': self.idf_values,\n",
    "            'stats': self.stats,\n",
    "            'doc_id_to_index': self.doc_id_to_index,\n",
    "            'index_to_doc_id': self.index_to_doc_id,\n",
    "            'doc_names': self.doc_names\n",
    "        }\n",
    "        \n",
    "        index_file = os.path.join(self.index_folder, \"tfidf_ranking.pkl\")\n",
    "        with open(index_file, 'wb') as f:\n",
    "            pickle.dump(index_data, f)\n",
    "        \n",
    "        # Also save a human-readable version\n",
    "        readable_file = os.path.join(self.index_folder, \"tfidf_ranking_stats.json\")\n",
    "        readable_data = {\n",
    "            'stats': self.stats,\n",
    "            'matrix_shape': self.tfidf_matrix.shape,\n",
    "            'top_terms_by_idf': sorted(self.idf_values.items(), key=lambda x: x[1], reverse=True)[:50],\n",
    "            'document_count': len(self.documents)\n",
    "        }\n",
    "        \n",
    "        with open(readable_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(readable_data, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"\\nğŸ’¾ Index saved to: {index_file}\")\n",
    "        print(f\"ğŸ“Š Statistics saved to: {readable_file}\")\n",
    "    \n",
    "    def load_index(self):\n",
    "        \"\"\"Load TF-IDF index from disk\"\"\"\n",
    "        index_file = os.path.join(self.index_folder, \"tfidf_ranking.pkl\")\n",
    "        \n",
    "        if not os.path.exists(index_file):\n",
    "            print(f\"âŒ Index not found at: {index_file}\")\n",
    "            print(\"Building new index...\")\n",
    "            return self.calculate_tf_idf_scores()\n",
    "        \n",
    "        try:\n",
    "            print(f\"ğŸ“‚ Loading TF-IDF ranking index from: {index_file}\")\n",
    "            with open(index_file, 'rb') as f:\n",
    "                index_data = pickle.load(f)\n",
    "            \n",
    "            self.vectorizer = index_data['vectorizer']\n",
    "            self.tfidf_matrix = index_data['tfidf_matrix']\n",
    "            self.documents = index_data['documents']\n",
    "            self.doc_texts = index_data['doc_texts']\n",
    "            self.doc_tokens = index_data['doc_tokens']\n",
    "            self.feature_names = index_data['feature_names']\n",
    "            self.idf_values = index_data['idf_values']\n",
    "            self.stats = index_data['stats']\n",
    "            self.doc_id_to_index = index_data['doc_id_to_index']\n",
    "            self.index_to_doc_id = index_data['index_to_doc_id']\n",
    "            self.doc_names = index_data['doc_names']\n",
    "            \n",
    "            print(f\"âœ… TF-IDF Ranking Index loaded successfully!\")\n",
    "            print(f\"   Documents: {self.stats['total_documents']:,}\")\n",
    "            print(f\"   Vocabulary: {self.stats['vocabulary_size']:,}\")\n",
    "            print(f\"   Matrix shape: {self.tfidf_matrix.shape}\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error loading index: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return self.calculate_tf_idf_scores()\n",
    "    \n",
    "    def rank_documents_by_query(self, query: str, top_k: int = 20) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Rank documents by TF-IDF relevance to query\n",
    "        \n",
    "        Args:\n",
    "            query: Search query string\n",
    "            top_k: Number of top results to return\n",
    "            \n",
    "        Returns:\n",
    "            List of ranked documents with TF-IDF scores\n",
    "        \"\"\"\n",
    "        if self.vectorizer is None or self.tfidf_matrix is None:\n",
    "            print(\"âŒ TF-IDF index not loaded\")\n",
    "            return []\n",
    "        \n",
    "        print(f\"\\nğŸ” Ranking documents for query: '{query}'\")\n",
    "        \n",
    "        # Transform query to TF-IDF vector\n",
    "        query_vector = self.vectorizer.transform([query])\n",
    "        \n",
    "        # Calculate dot product (TF-IDF similarity) between query and documents\n",
    "        # This gives us the sum of TF-IDF scores for query terms in each document\n",
    "        relevance_scores = (query_vector * self.tfidf_matrix.T).toarray().flatten()\n",
    "        \n",
    "        # Get top K documents by relevance score\n",
    "        top_indices = relevance_scores.argsort()[::-1][:top_k]\n",
    "        \n",
    "        results = []\n",
    "        for rank, idx in enumerate(top_indices, 1):\n",
    "            if relevance_scores[idx] <= 0:\n",
    "                continue  # Skip documents with zero relevance\n",
    "                \n",
    "            doc_id = self.index_to_doc_id[idx]\n",
    "            doc_info = self.documents[doc_id]\n",
    "            score = relevance_scores[idx]\n",
    "            \n",
    "            # Get query terms present in document with their TF-IDF scores\n",
    "            query_terms = query.lower().split()\n",
    "            term_scores = self.get_query_term_scores(doc_id, query_terms)\n",
    "            \n",
    "            # Calculate document length normalization factor\n",
    "            doc_length = doc_info['token_count']\n",
    "            norm_factor = math.log(1 + doc_length) if doc_length > 0 else 1\n",
    "            \n",
    "            # Normalized score (adjust for document length)\n",
    "            normalized_score = score / norm_factor if norm_factor > 0 else score\n",
    "            \n",
    "            results.append({\n",
    "                'doc_id': doc_id,\n",
    "                'name': doc_info['name'],\n",
    "                'token_count': doc_info['token_count'],\n",
    "                'tfidf_score': score,\n",
    "                'normalized_score': normalized_score,\n",
    "                'rank': rank,\n",
    "                'query_terms': query_terms,\n",
    "                'term_scores': term_scores,\n",
    "                'terms_found': len([t for t in term_scores if t['tfidf'] > 0]),\n",
    "                'total_terms': len(query_terms)\n",
    "            })\n",
    "        \n",
    "        # Sort by normalized score (higher is better)\n",
    "        results.sort(key=lambda x: x['normalized_score'], reverse=True)\n",
    "        \n",
    "        # Re-rank\n",
    "        for i, result in enumerate(results, 1):\n",
    "            result['rank'] = i\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_query_term_scores(self, doc_id: str, query_terms: List[str]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Get TF-IDF scores for query terms in a specific document\n",
    "        \n",
    "        Args:\n",
    "            doc_id: Document ID\n",
    "            query_terms: List of query terms\n",
    "            \n",
    "        Returns:\n",
    "            List of term score dictionaries\n",
    "        \"\"\"\n",
    "        if doc_id not in self.doc_id_to_index:\n",
    "            return []\n",
    "        \n",
    "        doc_idx = self.doc_id_to_index[doc_id]\n",
    "        doc_vector = self.tfidf_matrix[doc_idx]\n",
    "        \n",
    "        # Convert to dense for easier access\n",
    "        if sp.issparse(doc_vector):\n",
    "            doc_vector = doc_vector.toarray().flatten()\n",
    "        \n",
    "        term_scores = []\n",
    "        for term in query_terms:\n",
    "            # Find term index in vocabulary\n",
    "            term_idx = None\n",
    "            for i, feature in enumerate(self.feature_names):\n",
    "                if term in feature.lower():\n",
    "                    term_idx = i\n",
    "                    break\n",
    "            \n",
    "            if term_idx is not None and term_idx < len(doc_vector):\n",
    "                tfidf_score = doc_vector[term_idx]\n",
    "                idf_score = self.idf_values.get(self.feature_names[term_idx], 0)\n",
    "                \n",
    "                # Get raw term frequency from document\n",
    "                doc_tokens = self.doc_tokens[doc_id]\n",
    "                term_freq = doc_tokens.count(term.lower())\n",
    "                \n",
    "                term_scores.append({\n",
    "                    'term': term,\n",
    "                    'tf': term_freq,\n",
    "                    'idf': idf_score,\n",
    "                    'tfidf': tfidf_score,\n",
    "                    'in_vocabulary': True\n",
    "                })\n",
    "            else:\n",
    "                term_scores.append({\n",
    "                    'term': term,\n",
    "                    'tf': 0,\n",
    "                    'idf': 0,\n",
    "                    'tfidf': 0,\n",
    "                    'in_vocabulary': False\n",
    "                })\n",
    "        \n",
    "        return term_scores\n",
    "    \n",
    "    def rank_documents_by_keywords(self, keywords: List[str], top_k: int = 20) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Rank documents by keyword relevance using TF-IDF\n",
    "        \n",
    "        Args:\n",
    "            keywords: List of keywords\n",
    "            top_k: Number of top results to return\n",
    "            \n",
    "        Returns:\n",
    "            List of ranked documents\n",
    "        \"\"\"\n",
    "        query = \" \".join(keywords)\n",
    "        return self.rank_documents_by_query(query, top_k)\n",
    "    \n",
    "    def get_top_tfidf_terms_for_document(self, doc_name: str, top_n: int = 15) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Get top TF-IDF terms for a specific document\n",
    "        \n",
    "        Args:\n",
    "            doc_name: Name of the document\n",
    "            top_n: Number of top terms to return\n",
    "            \n",
    "        Returns:\n",
    "            List of term dictionaries with TF-IDF scores\n",
    "        \"\"\"\n",
    "        if self.tfidf_matrix is None:\n",
    "            print(\"âŒ TF-IDF matrix not loaded\")\n",
    "            return []\n",
    "        \n",
    "        # Find the document\n",
    "        target_doc_id = None\n",
    "        for doc_id, info in self.documents.items():\n",
    "            if info['name'] == doc_name:\n",
    "                target_doc_id = doc_id\n",
    "                break\n",
    "        \n",
    "        if target_doc_id is None:\n",
    "            print(f\"âŒ Document not found: {doc_name}\")\n",
    "            return []\n",
    "        \n",
    "        # Get document index\n",
    "        if target_doc_id not in self.doc_id_to_index:\n",
    "            print(f\"âŒ Document not in TF-IDF matrix: {doc_name}\")\n",
    "            return []\n",
    "        \n",
    "        doc_idx = self.doc_id_to_index[target_doc_id]\n",
    "        \n",
    "        # Get document vector\n",
    "        doc_vector = self.tfidf_matrix[doc_idx]\n",
    "        \n",
    "        # Convert to dense if sparse\n",
    "        if sp.issparse(doc_vector):\n",
    "            doc_vector = doc_vector.toarray().flatten()\n",
    "        \n",
    "        # Get top term indices\n",
    "        top_indices = doc_vector.argsort()[::-1][:top_n]\n",
    "        \n",
    "        # Get term details\n",
    "        top_terms = []\n",
    "        for idx in top_indices:\n",
    "            score = doc_vector[idx]\n",
    "            if score > 0:\n",
    "                term = self.feature_names[idx]\n",
    "                idf = self.idf_values.get(term, 0)\n",
    "                \n",
    "                # Calculate term frequency in document\n",
    "                doc_tokens = self.doc_tokens[target_doc_id]\n",
    "                term_lower = term.lower()\n",
    "                tf = sum(1 for token in doc_tokens if token == term_lower or term_lower in token)\n",
    "                \n",
    "                top_terms.append({\n",
    "                    'term': term,\n",
    "                    'tf': tf,\n",
    "                    'idf': idf,\n",
    "                    'tfidf': score,\n",
    "                    'rank': len(top_terms) + 1\n",
    "                })\n",
    "        \n",
    "        return top_terms\n",
    "    \n",
    "    def show_document_tfidf_analysis(self, doc_name: str):\n",
    "        \"\"\"Show comprehensive TF-IDF analysis for a document\"\"\"\n",
    "        print(f\"\\nğŸ“Š TF-IDF ANALYSIS FOR: {doc_name}\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # Get document info\n",
    "        doc_info = None\n",
    "        doc_id = None\n",
    "        for d_id, info in self.documents.items():\n",
    "            if info['name'] == doc_name:\n",
    "                doc_info = info\n",
    "                doc_id = d_id\n",
    "                break\n",
    "        \n",
    "        if not doc_info:\n",
    "            print(f\"âŒ Document not found: {doc_name}\")\n",
    "            return\n",
    "        \n",
    "        print(f\"ğŸ“„ Document: {doc_name}\")\n",
    "        print(f\"ğŸ“ Tokens: {doc_info['token_count']:,}\")\n",
    "        \n",
    "        # Get top terms\n",
    "        top_terms = self.get_top_tfidf_terms_for_document(doc_name, top_n=20)\n",
    "        \n",
    "        if top_terms:\n",
    "            print(f\"\\nğŸ† TOP 20 TERMS BY TF-IDF SCORE:\")\n",
    "            print(\"-\" * 80)\n",
    "            print(f\"{'Rank':<6} {'Term':<25} {'TF':<8} {'IDF':<8} {'TF-IDF':<10}\")\n",
    "            print(\"-\" * 80)\n",
    "            \n",
    "            for term_info in top_terms:\n",
    "                print(f\"{term_info['rank']:<6} {term_info['term'][:24]:<25} \"\n",
    "                      f\"{term_info['tf']:<8} {term_info['idf']:<8.3f} {term_info['tfidf']:<10.4f}\")\n",
    "        \n",
    "        print(\"=\" * 80)\n",
    "    \n",
    "    def show_ranking_statistics(self):\n",
    "        \"\"\"Show ranking system statistics\"\"\"\n",
    "        print(f\"\\nğŸ“Š TF-IDF RANKING SYSTEM STATISTICS\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"Documents: {self.stats['total_documents']:,}\")\n",
    "        print(f\"Vocabulary: {self.stats['vocabulary_size']:,}\")\n",
    "        print(f\"Total terms: {self.stats['total_terms']:,}\")\n",
    "        \n",
    "        if self.tfidf_matrix is not None:\n",
    "            print(f\"TF-IDF matrix shape: {self.tfidf_matrix.shape}\")\n",
    "            \n",
    "            # Calculate average document score\n",
    "            if self.tfidf_matrix.shape[0] > 0:\n",
    "                avg_score = self.tfidf_matrix.sum() / (self.tfidf_matrix.shape[0] * self.tfidf_matrix.shape[1])\n",
    "                print(f\"Average TF-IDF score per term: {avg_score:.6f}\")\n",
    "        \n",
    "        # Show most discriminating terms (highest IDF)\n",
    "        sorted_idf = sorted(self.idf_values.items(), key=lambda x: x[1], reverse=True)[:15]\n",
    "        print(f\"\\nğŸ” TOP 15 MOST DISCRIMINATING TERMS (Highest IDF):\")\n",
    "        for i, (term, idf) in enumerate(sorted_idf, 1):\n",
    "            print(f\"  {i:2d}. {term:<25} IDF: {idf:.3f}\")\n",
    "        \n",
    "        # Show most common terms (lowest IDF)\n",
    "        sorted_idf_low = sorted(self.idf_values.items(), key=lambda x: x[1])[:15]\n",
    "        print(f\"\\nğŸ“‰ TOP 15 MOST COMMON TERMS (Lowest IDF):\")\n",
    "        for i, (term, idf) in enumerate(sorted_idf_low, 1):\n",
    "            print(f\"  {i:2d}. {term:<25} IDF: {idf:.3f}\")\n",
    "        \n",
    "        print(\"=\" * 80)\n",
    "    \n",
    "    def interactive_ranking(self):\n",
    "        \"\"\"Interactive TF-IDF ranking interface\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"ğŸ¥‡ TF-IDF DOCUMENT RANKING SYSTEM\")\n",
    "        print(\"=\" * 80)\n",
    "        print(\"\\nğŸ“‹ Available Commands:\")\n",
    "        print(\"  â€¢ rank <query>           - Rank documents by query relevance\")\n",
    "        print(\"  â€¢ keywords <word1 word2> - Rank documents by keywords\")\n",
    "        print(\"  â€¢ terms <doc_name>       - Show top TF-IDF terms for document\")\n",
    "        print(\"  â€¢ analyze <doc_name>     - Show TF-IDF analysis for document\")\n",
    "        print(\"  â€¢ stats                  - Show ranking statistics\")\n",
    "        print(\"  â€¢ rebuild                - Recalculate TF-IDF scores\")\n",
    "        print(\"  â€¢ quit                   - Exit\")\n",
    "        print(\"\\nğŸ“ Example commands:\")\n",
    "        print(\"  â€¢ rank murder evidence\")\n",
    "        print(\"  â€¢ keywords supreme court appeal\")\n",
    "        print(\"  â€¢ terms 2025LHC7277.txt\")\n",
    "        print(\"  â€¢ analyze 2025LHC7389.txt\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        while True:\n",
    "            user_input = input(\"\\nğŸ¯ Enter command: \").strip()\n",
    "            \n",
    "            if not user_input:\n",
    "                continue\n",
    "            \n",
    "            if user_input.lower() == 'quit':\n",
    "                print(\"ğŸ‘‹ Goodbye!\")\n",
    "                break\n",
    "            \n",
    "            elif user_input.lower() == 'stats':\n",
    "                self.show_ranking_statistics()\n",
    "            \n",
    "            elif user_input.lower() == 'rebuild':\n",
    "                confirm = input(\"âš ï¸  Recalculate TF-IDF scores? This may take time. (y/n): \").strip().lower()\n",
    "                if confirm == 'y':\n",
    "                    max_features = input(\"Enter max features (default 10000): \").strip()\n",
    "                    max_features = int(max_features) if max_features.isdigit() else 10000\n",
    "                    self.calculate_tf_idf_scores(max_features=max_features)\n",
    "            \n",
    "            elif user_input.lower().startswith('rank '):\n",
    "                query = user_input[5:].strip()\n",
    "                if query:\n",
    "                    print(f\"\\nğŸ” Ranking documents for query: '{query}'\")\n",
    "                    results = self.rank_documents_by_query(query, top_k=20)\n",
    "                    self.display_ranking_results(results, f\"Ranking for: '{query}'\")\n",
    "                else:\n",
    "                    print(\"âŒ Please enter a query\")\n",
    "            \n",
    "            elif user_input.lower().startswith('keywords '):\n",
    "                keywords = user_input[9:].strip().split()\n",
    "                if keywords:\n",
    "                    print(f\"\\nğŸ” Ranking documents for keywords: {keywords}\")\n",
    "                    results = self.rank_documents_by_keywords(keywords, top_k=20)\n",
    "                    self.display_ranking_results(results, f\"Keywords: {', '.join(keywords)}\")\n",
    "                else:\n",
    "                    print(\"âŒ Please enter keywords\")\n",
    "            \n",
    "            elif user_input.lower().startswith('terms '):\n",
    "                doc_name = user_input[6:].strip()\n",
    "                if doc_name:\n",
    "                    print(f\"\\nğŸ”¤ Top TF-IDF terms for: '{doc_name}'\")\n",
    "                    top_terms = self.get_top_tfidf_terms_for_document(doc_name, top_n=15)\n",
    "                    \n",
    "                    if top_terms:\n",
    "                        print(\"\\n\" + \"=\" * 80)\n",
    "                        print(f\"ğŸ† TOP TF-IDF TERMS FOR: {doc_name}\")\n",
    "                        print(\"=\" * 80)\n",
    "                        \n",
    "                        for term_info in top_terms:\n",
    "                            print(f\"{term_info['rank']:2d}. {term_info['term']:<25} \"\n",
    "                                  f\"TF-IDF: {term_info['tfidf']:.4f} \"\n",
    "                                  f\"(TF: {term_info['tf']}, IDF: {term_info['idf']:.3f})\")\n",
    "                        \n",
    "                        print(\"=\" * 80)\n",
    "                    else:\n",
    "                        print(\"âŒ No terms found or document not in index\")\n",
    "                else:\n",
    "                    print(\"âŒ Please enter a document name\")\n",
    "            \n",
    "            elif user_input.lower().startswith('analyze '):\n",
    "                doc_name = user_input[8:].strip()\n",
    "                if doc_name:\n",
    "                    self.show_document_tfidf_analysis(doc_name)\n",
    "                else:\n",
    "                    print(\"âŒ Please enter a document name\")\n",
    "            \n",
    "            else:\n",
    "                # Try as a ranking query\n",
    "                print(f\"\\nğŸ” Ranking documents for query: '{user_input}'\")\n",
    "                results = self.rank_documents_by_query(user_input, top_k=15)\n",
    "                self.display_ranking_results(results, f\"Ranking for: '{user_input}'\")\n",
    "    \n",
    "    def display_ranking_results(self, results: List[Dict], title: str):\n",
    "        \"\"\"Display ranking results\"\"\"\n",
    "        if not results:\n",
    "            print(f\"\\nâŒ No relevant documents found\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\nâœ… {title}\")\n",
    "        print(f\"ğŸ“Š Ranked {len(results)} document(s) by TF-IDF relevance\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        for i, result in enumerate(results[:15], 1):  # Show top 15\n",
    "            print(f\"\\n{i:2d}. ğŸ“„ {result['name']}\")\n",
    "            print(f\"    ğŸ¥‡ Rank: #{result['rank']}\")\n",
    "            print(f\"    ğŸ“ Length: {result['token_count']:,} tokens\")\n",
    "            print(f\"    â­ TF-IDF Score: {result['tfidf_score']:.4f}\")\n",
    "            print(f\"    ğŸ“Š Normalized Score: {result['normalized_score']:.4f}\")\n",
    "            \n",
    "            if result['term_scores']:\n",
    "                # Show top contributing terms\n",
    "                top_terms = sorted([t for t in result['term_scores'] if t['tfidf'] > 0], \n",
    "                                 key=lambda x: x['tfidf'], reverse=True)[:3]\n",
    "                \n",
    "                if top_terms:\n",
    "                    print(f\"    ğŸ” Top contributing terms:\")\n",
    "                    for term_info in top_terms:\n",
    "                        print(f\"       â€¢ {term_info['term']}: TF-IDF={term_info['tfidf']:.4f} \"\n",
    "                              f\"(TF={term_info['tf']}, IDF={term_info['idf']:.3f})\")\n",
    "            \n",
    "            print(f\"    ğŸ” Terms matched: {result['terms_found']}/{result['total_terms']}\")\n",
    "        \n",
    "        if len(results) > 15:\n",
    "            print(f\"\\n... and {len(results) - 15} more ranked documents\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        \n",
    "        # Ask for analysis\n",
    "        if results:\n",
    "            choice = input(\"\\nğŸ“Š Analyze a document? (enter rank number or 'n'): \").strip()\n",
    "            if choice.lower() != 'n' and choice.isdigit():\n",
    "                idx = int(choice) - 1\n",
    "                if 0 <= idx < len(results):\n",
    "                    self.show_document_tfidf_analysis(results[idx]['name'])\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function\"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"ğŸ¥‡ SUPREME COURT - TF-IDF DOCUMENT RANKING SYSTEM\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Set corpus folder path\n",
    "    corpus_folder = r\"C:\\Users\\Armaghan Rafique\\Desktop\\AI Project\\cleaned_corpus\"\n",
    "    \n",
    "    # Check if folder exists\n",
    "    if not os.path.exists(corpus_folder):\n",
    "        print(f\"âŒ Corpus folder not found: {corpus_folder}\")\n",
    "        \n",
    "        # Try alternative locations\n",
    "        alt_folders = [\n",
    "            r\"C:\\Users\\Armaghan Rafique\\Desktop\\AI Project\\supreme_court_judgements_txt\",\n",
    "            r\"C:\\Users\\Armaghan Rafique\\Desktop\\AI Project\",\n",
    "            os.path.join(os.path.expanduser(\"~\"), \"Desktop\", \"AI Project\", \"cleaned_corpus\")\n",
    "        ]\n",
    "        \n",
    "        found = False\n",
    "        for folder in alt_folders:\n",
    "            if os.path.exists(folder):\n",
    "                corpus_folder = folder\n",
    "                print(f\"âœ… Using folder: {corpus_folder}\")\n",
    "                found = True\n",
    "                break\n",
    "        \n",
    "        if not found:\n",
    "            corpus_folder = input(\"ğŸ“ Enter corpus folder path: \").strip()\n",
    "            if not os.path.exists(corpus_folder):\n",
    "                print(f\"âŒ Folder does not exist: {corpus_folder}\")\n",
    "                return\n",
    "    \n",
    "    print(f\"\\nğŸ“ Using corpus folder: {corpus_folder}\")\n",
    "    \n",
    "    # Create TF-IDF ranking system\n",
    "    ranking_system = SupremeCourtTFIDFRanking(corpus_folder)\n",
    "    \n",
    "    # Load or build index\n",
    "    print(\"\\nğŸ“‚ Initializing TF-IDF ranking system...\")\n",
    "    if not ranking_system.load_index():\n",
    "        print(\"âŒ Failed to initialize TF-IDF ranking system\")\n",
    "        return\n",
    "    \n",
    "    # Show system info\n",
    "    print(f\"\\nğŸ“Š TF-IDF RANKING SYSTEM READY\")\n",
    "    print(f\"   Documents: {ranking_system.stats['total_documents']:,}\")\n",
    "    print(f\"   Vocabulary: {ranking_system.stats['vocabulary_size']:,}\")\n",
    "    print(f\"   Total terms: {ranking_system.stats['total_terms']:,}\")\n",
    "    \n",
    "    # Start interactive ranking\n",
    "    ranking_system.interactive_ranking()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a763d8-8bfe-42c8-9a92-bdc0fd093a33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
