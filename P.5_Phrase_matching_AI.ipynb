{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48022377-1674-4a37-a62a-2436016dae55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üî§ N-GRAM PHRASE SEARCH SYSTEM (up to 5-grams)\n",
      "================================================================================\n",
      "\n",
      "üìÅ Corpus folder: C:\\Users\\Armaghan Rafique\\Desktop\\AI Project\\cleaned_corpus\n",
      "‚ùå Index not found at: C:\\Users\\Armaghan Rafique\\Desktop\\AI Project\\cleaned_corpus\\ngram_index_5\\ngram_index.pkl\n",
      "Building new index...\n",
      "üî® Building N-gram Search Index (up to 5-grams)...\n",
      "‚úÖ Loaded 1460 documents\n",
      "   Processing document 100/1460...\n",
      "   Processing document 200/1460...\n",
      "   Processing document 300/1460...\n",
      "   Processing document 400/1460...\n",
      "   Processing document 500/1460...\n",
      "   Processing document 600/1460...\n",
      "   Processing document 700/1460...\n",
      "   Processing document 800/1460...\n",
      "   Processing document 900/1460...\n",
      "   Processing document 1000/1460...\n",
      "   Processing document 1100/1460...\n",
      "   Processing document 1200/1460...\n",
      "   Processing document 1300/1460...\n",
      "   Processing document 1400/1460...\n",
      "\n",
      "üíæ Index saved to: C:\\Users\\Armaghan Rafique\\Desktop\\AI Project\\cleaned_corpus\\ngram_index_5\\ngram_index.pkl\n",
      "üìä Statistics saved to: C:\\Users\\Armaghan Rafique\\Desktop\\AI Project\\cleaned_corpus\\ngram_index_5\\index_stats.json\n",
      "\n",
      "‚úÖ N-gram index built successfully!\n",
      "   Documents indexed: 1,460\n",
      "   Document tokens file: C:\\Users\\Armaghan Rafique\\Desktop\\AI Project\\cleaned_corpus\\document_tokens.json\n",
      "   2-grams: 122,248 unique, 326.3 avg per doc\n",
      "   3-grams: 185,544 unique, 325.3 avg per doc\n",
      "   4-grams: 213,668 unique, 324.3 avg per doc\n",
      "   5-grams: 228,449 unique, 323.3 avg per doc\n",
      "\n",
      "üìä Sample N-grams in index:\n",
      "   2-grams: page appellate, appellate gilgit, gilgit baltistan, baltistan gilgit, gilgit cpla...\n",
      "   3-grams: page appellate gilgit, appellate gilgit baltistan, gilgit baltistan gilgit, baltistan gilgit cpla, gilgit cpla chief...\n",
      "\n",
      "üéØ N-GRAM SEARCH READY\n",
      "   Documents: 1,460\n",
      "   N-gram sizes: 2 to 5\n",
      "\n",
      "================================================================================\n",
      "üî§ N-GRAM PHRASE SEARCH SYSTEM\n",
      "   Max N-gram size: 5\n",
      "================================================================================\n",
      "\n",
      "üìã Available Commands:\n",
      "  ‚Ä¢ search <phrase>     - Exact phrase search\n",
      "  ‚Ä¢ multi <phrase>      - Multi-size N-gram search\n",
      "  ‚Ä¢ smart <phrase>      - Smart search with suggestions\n",
      "  ‚Ä¢ ngrams <doc> <n>    - Show N-grams for document\n",
      "  ‚Ä¢ stats               - Show index statistics\n",
      "  ‚Ä¢ example             - Show example searches\n",
      "  ‚Ä¢ quit                - Exit\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ Enter command:  murder\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Phrase must contain at least 2 words\n",
      "\n",
      "ü§î No exact matches found, trying multi-size N-gram search...\n",
      "\n",
      "‚ùå No results found for 'murder'\n",
      "\n",
      "üí° Suggestions:\n",
      "   ‚Ä¢ No documents found. Try:\n",
      "   ‚Ä¢ 1. Check spelling of your phrase\n",
      "   ‚Ä¢ 2. Try a shorter phrase\n",
      "   ‚Ä¢ 3. Try individual important words\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ Enter command:  murder\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Phrase must contain at least 2 words\n",
      "\n",
      "ü§î No exact matches found, trying multi-size N-gram search...\n",
      "\n",
      "‚ùå No results found for 'murder'\n",
      "\n",
      "üí° Suggestions:\n",
      "   ‚Ä¢ No documents found. Try:\n",
      "   ‚Ä¢ 1. Check spelling of your phrase\n",
      "   ‚Ä¢ 2. Try a shorter phrase\n",
      "   ‚Ä¢ 3. Try individual important words\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ Enter command:  murder execute\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Searching for phrase: 'murder execute'\n",
      "   Query tokens (2): ['murder', 'execute']\n",
      "   Using 2-gram matching\n",
      "   Query 2-grams (1): ['murder execute']\n",
      "   ‚úó No documents contain all 2-grams\n",
      "   N-gram frequency: [0]\n",
      "\n",
      "ü§î No exact matches found, trying multi-size N-gram search...\n",
      "\n",
      "üîç Multi-size N-gram search: 'murder execute'\n",
      "   Trying 2-gram search...\n",
      "   ‚úó No documents found with 2-grams\n",
      "   ‚úì Total unique documents: 0\n",
      "\n",
      "‚ùå No results found for 'murder execute'\n",
      "\n",
      "üí° Suggestions:\n",
      "   ‚Ä¢ No documents found. Try:\n",
      "   ‚Ä¢ 1. Check spelling of your phrase\n",
      "   ‚Ä¢ 2. Try a shorter phrase\n",
      "   ‚Ä¢ 3. Try individual important words\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ Enter command:  district section passed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Searching for phrase: 'district section passed'\n",
      "   Query tokens (3): ['district', 'section', 'passed']\n",
      "   Using 2-gram matching\n",
      "   Query 2-grams (2): ['district section', 'section passed']\n",
      "   ‚úó No documents contain all 2-grams\n",
      "   N-gram frequency: [0]\n",
      "\n",
      "ü§î No exact matches found, trying multi-size N-gram search...\n",
      "\n",
      "üîç Multi-size N-gram search: 'district section passed'\n",
      "   Trying 3-gram search...\n",
      "   ‚úó No documents found with 3-grams\n",
      "   Trying 2-gram search...\n",
      "   ‚úó No documents found with 2-grams\n",
      "   ‚úì Total unique documents: 0\n",
      "\n",
      "‚ùå No results found for 'district section passed'\n",
      "\n",
      "üí° Suggestions:\n",
      "   ‚Ä¢ No documents found. Try:\n",
      "   ‚Ä¢ 1. Check spelling of your phrase\n",
      "   ‚Ä¢ 2. Try a shorter phrase\n",
      "   ‚Ä¢ 3. Try individual important words\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ Enter command:  motorcycle bearing registration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Searching for phrase: 'motorcycle bearing registration'\n",
      "   Query tokens (3): ['motorcycle', 'bearing', 'registration']\n",
      "   Using 3-gram matching\n",
      "   Query 3-grams (1): ['motorcycle bearing registration']\n",
      "   ‚úì Candidate documents: 1\n",
      "   N-gram frequencies: [1]\n",
      "   ‚úì Documents with exact phrase: 1\n",
      "\n",
      "‚úÖ Search: 'motorcycle bearing registration'\n",
      "üìä Found 1 document(s)\n",
      "================================================================================\n",
      "\n",
      " 1. üìÑ 2025LHC7266.txt\n",
      "    üìè Length: 2,025 tokens\n",
      "    ‚≠ê Score: 23.8\n",
      "    üîç Exact matches: 1\n",
      "    üìù Context: muhammad fazil deceased returning home mouza mangan [ motorcycle bearing registration ] -jgk reached near bhaini dera allah ditta\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "üìñ Preview a document? (enter number or 'n'):  n\n",
      "\n",
      "üéØ Enter command:  against the accused\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Searching for phrase: 'against the accused'\n",
      "   Query tokens (3): ['against', 'the', 'accused']\n",
      "   Using 2-gram matching\n",
      "   Query 2-grams (2): ['against the', 'the accused']\n",
      "   ‚úó No documents contain all 2-grams\n",
      "   N-gram frequency: [0]\n",
      "\n",
      "ü§î No exact matches found, trying multi-size N-gram search...\n",
      "\n",
      "üîç Multi-size N-gram search: 'against the accused'\n",
      "   Trying 3-gram search...\n",
      "   ‚úó No documents found with 3-grams\n",
      "   Trying 2-gram search...\n",
      "   ‚úó No documents found with 2-grams\n",
      "   ‚úì Total unique documents: 0\n",
      "\n",
      "‚ùå No results found for 'against the accused'\n",
      "\n",
      "üí° Suggestions:\n",
      "   ‚Ä¢ No documents found. Try:\n",
      "   ‚Ä¢ 1. Check spelling of your phrase\n",
      "   ‚Ä¢ 2. Try a shorter phrase\n",
      "   ‚Ä¢ 3. Try individual important words\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ Enter command:  supreme court og gilgit-baltistan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Searching for phrase: 'supreme court og gilgit-baltistan'\n",
      "   Query tokens (4): ['supreme', 'court', 'og', 'gilgit-baltistan']\n",
      "   Using 2-gram matching\n",
      "   Query 2-grams (3): ['supreme court', 'court og', 'og gilgit-baltistan']\n",
      "   ‚úó No documents contain all 2-grams\n",
      "   N-gram frequency: [0]\n",
      "\n",
      "ü§î No exact matches found, trying multi-size N-gram search...\n",
      "\n",
      "üîç Multi-size N-gram search: 'supreme court og gilgit-baltistan'\n",
      "   Trying 4-gram search...\n",
      "   ‚úó No documents found with 4-grams\n",
      "   Trying 3-gram search...\n",
      "   ‚úó No documents found with 3-grams\n",
      "   Trying 2-gram search...\n",
      "   ‚úó No documents found with 2-grams\n",
      "   ‚úì Total unique documents: 0\n",
      "\n",
      "‚ùå No results found for 'supreme court og gilgit-baltistan'\n",
      "\n",
      "üí° Suggestions:\n",
      "   ‚Ä¢ No documents found. Try:\n",
      "   ‚Ä¢ 1. Check spelling of your phrase\n",
      "   ‚Ä¢ 2. Try a shorter phrase\n",
      "   ‚Ä¢ 3. Try individual important words\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ Enter command:  supreme court\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Searching for phrase: 'supreme court'\n",
      "   Query tokens (2): ['supreme', 'court']\n",
      "   Using 2-gram matching\n",
      "   Query 2-grams (1): ['supreme court']\n",
      "   ‚úó No documents contain all 2-grams\n",
      "   N-gram frequency: [0]\n",
      "\n",
      "ü§î No exact matches found, trying multi-size N-gram search...\n",
      "\n",
      "üîç Multi-size N-gram search: 'supreme court'\n",
      "   Trying 2-gram search...\n",
      "   ‚úó No documents found with 2-grams\n",
      "   ‚úì Total unique documents: 0\n",
      "\n",
      "‚ùå No results found for 'supreme court'\n",
      "\n",
      "üí° Suggestions:\n",
      "   ‚Ä¢ No documents found. Try:\n",
      "   ‚Ä¢ 1. Check spelling of your phrase\n",
      "   ‚Ä¢ 2. Try a shorter phrase\n",
      "   ‚Ä¢ 3. Try individual important words\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ Enter command:  availability of teachers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Searching for phrase: 'availability of teachers'\n",
      "   Query tokens (3): ['availability', 'of', 'teachers']\n",
      "   Using 2-gram matching\n",
      "   Query 2-grams (2): ['availability of', 'of teachers']\n",
      "   ‚úó No documents contain all 2-grams\n",
      "   N-gram frequency: [0]\n",
      "\n",
      "ü§î No exact matches found, trying multi-size N-gram search...\n",
      "\n",
      "üîç Multi-size N-gram search: 'availability of teachers'\n",
      "   Trying 3-gram search...\n",
      "   ‚úó No documents found with 3-grams\n",
      "   Trying 2-gram search...\n",
      "   ‚úó No documents found with 2-grams\n",
      "   ‚úì Total unique documents: 0\n",
      "\n",
      "‚ùå No results found for 'availability of teachers'\n",
      "\n",
      "üí° Suggestions:\n",
      "   ‚Ä¢ No documents found. Try:\n",
      "   ‚Ä¢ 1. Check spelling of your phrase\n",
      "   ‚Ä¢ 2. Try a shorter phrase\n",
      "   ‚Ä¢ 3. Try individual important words\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ Enter command:  sho ali muhammad\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Searching for phrase: 'sho ali muhammad'\n",
      "   Query tokens (3): ['sho', 'ali', 'muhammad']\n",
      "   Using 2-gram matching\n",
      "   Query 2-grams (2): ['sho ali', 'ali muhammad']\n",
      "   ‚úó No documents contain all 2-grams\n",
      "   N-gram frequency: [0]\n",
      "\n",
      "ü§î No exact matches found, trying multi-size N-gram search...\n",
      "\n",
      "üîç Multi-size N-gram search: 'sho ali muhammad'\n",
      "   Trying 3-gram search...\n",
      "   ‚úó No documents found with 3-grams\n",
      "   Trying 2-gram search...\n",
      "   ‚úì Found 35 documents with 2-grams\n",
      "   ‚úì Total unique documents: 35\n",
      "\n",
      "‚úÖ Search: 'sho ali muhammad'\n",
      "üìä Found 35 document(s)\n",
      "================================================================================\n",
      "\n",
      " 1. üìÑ Officer_Commanding_182_Petroleum_Storage_Platoon_A_Muhammad_Ali_vs_commanding_ofvicer.txt\n",
      "    üìè Length: 839 tokens\n",
      "    ‚≠ê Score: 33.3\n",
      "    üìà N-gram coverage: 50.0%\n",
      "    üî§ Best N-gram size: 2\n",
      "\n",
      " 2. üìÑ The_State_Versus_Sufi_Ali_so_Abdul_Karim_and_othe__10_._Sufi_Ali_.txt\n",
      "    üìè Length: 1,803 tokens\n",
      "    ‚≠ê Score: 33.3\n",
      "    üìà N-gram coverage: 50.0%\n",
      "    üî§ Best N-gram size: 2\n",
      "\n",
      " 3. üìÑ Cr_PLA_No102007_Qalb_Ali_Versus_State.txt\n",
      "    üìè Length: 1,133 tokens\n",
      "    ‚≠ê Score: 33.3\n",
      "    üìà N-gram coverage: 50.0%\n",
      "    üî§ Best N-gram size: 2\n",
      "\n",
      " 4. üìÑ Government_of_Gilgit-Baltistan_through_Chief_Secre_Provincial_Govt.__others_versus_Sonia.txt\n",
      "    üìè Length: 571 tokens\n",
      "    ‚≠ê Score: 33.3\n",
      "    üìà N-gram coverage: 50.0%\n",
      "    üî§ Best N-gram size: 2\n",
      "\n",
      " 5. üìÑ Hussain_others_Versus_Syed_Muhammad_Ali_and_othe_hussain_20versus_20Syed_20Muhammad_20Ali_20shah.txt\n",
      "    üìè Length: 864 tokens\n",
      "    ‚≠ê Score: 33.3\n",
      "    üìà N-gram coverage: 50.0%\n",
      "    üî§ Best N-gram size: 2\n",
      "\n",
      " 6. üìÑ Khalid_Ali_Versus_The_State_Cr_MISC_03-2010_Khalid_Ali_Vs_state.txt\n",
      "    üìè Length: 56 tokens\n",
      "    ‚≠ê Score: 33.3\n",
      "    üìà N-gram coverage: 50.0%\n",
      "    üî§ Best N-gram size: 2\n",
      "\n",
      " 7. üìÑ SMC_No_122011_smc_20no._2012-2011.txt\n",
      "    üìè Length: 220 tokens\n",
      "    ‚≠ê Score: 33.3\n",
      "    üìà N-gram coverage: 50.0%\n",
      "    üî§ Best N-gram size: 2\n",
      "\n",
      " 8. üìÑ Cr_PLA_No_152015_The_State_versus_Ehsan_Ali_Advocate___others.txt\n",
      "    üìè Length: 248 tokens\n",
      "    ‚≠ê Score: 33.3\n",
      "    üìà N-gram coverage: 50.0%\n",
      "    üî§ Best N-gram size: 2\n",
      "\n",
      " 9. üìÑ CPLA_NO222009_Muhammad_Ali_vs_commanding_ofvicer.txt\n",
      "    üìè Length: 839 tokens\n",
      "    ‚≠ê Score: 33.3\n",
      "    üìà N-gram coverage: 50.0%\n",
      "    üî§ Best N-gram size: 2\n",
      "\n",
      "10. üìÑ Muhammad2_Hussain_sso_Abdus_Salam_Versus_Abdur_Muhammad_vs_abdur_Rehaman.txt\n",
      "    üìè Length: 94 tokens\n",
      "    ‚≠ê Score: 33.3\n",
      "    üìà N-gram coverage: 50.0%\n",
      "    üî§ Best N-gram size: 2\n",
      "\n",
      "11. üìÑ The_water_and_Power_Development_Authority_WAPDA_smc_20no._2012-2011.txt\n",
      "    üìè Length: 220 tokens\n",
      "    ‚≠ê Score: 33.3\n",
      "    üìà N-gram coverage: 50.0%\n",
      "    üî§ Best N-gram size: 2\n",
      "\n",
      "12. üìÑ CrMisc_NO_032010_Cr_MISC_03-2010_Khalid_Ali_Vs_state.txt\n",
      "    üìè Length: 56 tokens\n",
      "    ‚≠ê Score: 33.3\n",
      "    üìà N-gram coverage: 50.0%\n",
      "    üî§ Best N-gram size: 2\n",
      "\n",
      "13. üìÑ Civil_Appeal_No_212015_In_CPLA_No_012014_Provincial_Govt.__others_versus_Sonia.txt\n",
      "    üìè Length: 571 tokens\n",
      "    ‚≠ê Score: 33.3\n",
      "    üìà N-gram coverage: 50.0%\n",
      "    üî§ Best N-gram size: 2\n",
      "\n",
      "14. üìÑ C_Misc_No_232013_C_Appeal_No_022011_in_CPL_Muhammad_Amin_versus_Govt._of_Gb.txt\n",
      "    üìè Length: 580 tokens\n",
      "    ‚≠ê Score: 33.3\n",
      "    üìà N-gram coverage: 50.0%\n",
      "    üî§ Best N-gram size: 2\n",
      "\n",
      "15. üìÑ Civil_Appeal_No_102016_in_CPLA_No_062015_Mst._20Hameeda_20versus_20Ali_20Mardan_20Khan.txt\n",
      "    üìè Length: 616 tokens\n",
      "    ‚≠ê Score: 33.3\n",
      "    üìà N-gram coverage: 50.0%\n",
      "    üî§ Best N-gram size: 2\n",
      "\n",
      "... and 20 more documents\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "üìñ Preview a document? (enter number or 'n'):  sho\n",
      "\n",
      "üéØ Enter command:  muhammad\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Phrase must contain at least 2 words\n",
      "\n",
      "ü§î No exact matches found, trying multi-size N-gram search...\n",
      "\n",
      "‚ùå No results found for 'muhammad'\n",
      "\n",
      "üí° Suggestions:\n",
      "   ‚Ä¢ No documents found. Try:\n",
      "   ‚Ä¢ 1. Check spelling of your phrase\n",
      "   ‚Ä¢ 2. Try a shorter phrase\n",
      "   ‚Ä¢ 3. Try individual important words\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List, Dict, Set, Tuple\n",
    "import math\n",
    "\n",
    "class NGramSearchSystem:\n",
    "    def __init__(self, corpus_folder: str, max_ngram_size: int = 5):\n",
    "        \"\"\"\n",
    "        Initialize N-gram Search System up to specified N-gram size\n",
    "        \n",
    "        Args:\n",
    "            corpus_folder: Path to the cleaned_corpus folder\n",
    "            max_ngram_size: Maximum N for N-grams (2=bigram, 3=trigram, 4=4-gram, 5=5-gram)\n",
    "        \"\"\"\n",
    "        self.corpus_folder = corpus_folder\n",
    "        self.max_ngram_size = max_ngram_size\n",
    "        self.index_folder = os.path.join(corpus_folder, f\"ngram_index_{max_ngram_size}\")\n",
    "        \n",
    "        # Create index folder if it doesn't exist\n",
    "        if not os.path.exists(self.index_folder):\n",
    "            os.makedirs(self.index_folder)\n",
    "        \n",
    "        # Data structures\n",
    "        self.documents = {}  # doc_id -> document info\n",
    "        self.doc_tokens = {}  # doc_id -> list of tokens\n",
    "        self.ngram_index = defaultdict(lambda: defaultdict(set))  # n -> ngram -> doc_ids\n",
    "        self.position_index = defaultdict(lambda: defaultdict(dict))  # n -> doc_id -> ngram -> positions\n",
    "        \n",
    "        # Statistics\n",
    "        self.stats = {\n",
    "            'total_documents': 0,\n",
    "            'total_ngrams': defaultdict(int),\n",
    "            'unique_ngrams': defaultdict(int),\n",
    "            'avg_ngrams_per_doc': defaultdict(float)\n",
    "        }\n",
    "    \n",
    "    def extract_ngrams(self, tokens: List[str], n: int) -> List[Tuple[str, int]]:\n",
    "        \"\"\"\n",
    "        Extract N-grams from tokens with their positions\n",
    "        \n",
    "        Args:\n",
    "            tokens: List of tokens\n",
    "            n: N-gram size (2 for bigram, 3 for trigram, etc.)\n",
    "        \n",
    "        Returns:\n",
    "            List of (ngram_text, start_position)\n",
    "        \"\"\"\n",
    "        ngrams = []\n",
    "        # Allow overlap by moving window by 1 token each time\n",
    "        for i in range(len(tokens) - n + 1):\n",
    "            ngram = \" \".join(tokens[i:i+n])\n",
    "            ngrams.append((ngram, i))\n",
    "        return ngrams\n",
    "    \n",
    "    def build_index(self):\n",
    "        \"\"\"Build N-gram index from corpus up to max_ngram_size\"\"\"\n",
    "        print(f\"üî® Building N-gram Search Index (up to {self.max_ngram_size}-grams)...\")\n",
    "        \n",
    "        # Load document tokens\n",
    "        doc_tokens_file = os.path.join(self.corpus_folder, \"document_tokens.json\")\n",
    "        \n",
    "        if not os.path.exists(doc_tokens_file):\n",
    "            print(\"‚ùå document_tokens.json not found!\")\n",
    "            print(\"Looking for document tokens in alternative locations...\")\n",
    "            \n",
    "            # Try to find document_tokens.json\n",
    "            for root, dirs, files in os.walk(self.corpus_folder):\n",
    "                if \"document_tokens.json\" in files:\n",
    "                    doc_tokens_file = os.path.join(root, \"document_tokens.json\")\n",
    "                    print(f\"‚úÖ Found at: {doc_tokens_file}\")\n",
    "                    break\n",
    "            \n",
    "            if not os.path.exists(doc_tokens_file):\n",
    "                print(\"‚ùå Could not find document_tokens.json\")\n",
    "                return False\n",
    "        \n",
    "        try:\n",
    "            with open(doc_tokens_file, 'r', encoding='utf-8') as f:\n",
    "                doc_data = json.load(f)\n",
    "            print(f\"‚úÖ Loaded {len(doc_data)} documents\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading document tokens: {e}\")\n",
    "            return False\n",
    "        \n",
    "        # Process each document\n",
    "        doc_id = 0\n",
    "        total_docs = len(doc_data)\n",
    "        \n",
    "        for doc_name, doc_info in doc_data.items():\n",
    "            doc_id += 1\n",
    "            tokens = doc_info.get('tokens', [])\n",
    "            token_count = doc_info.get('token_count', 0)\n",
    "            \n",
    "            if doc_id % 100 == 0:\n",
    "                print(f\"   Processing document {doc_id}/{total_docs}...\")\n",
    "            \n",
    "            if tokens and token_count >= 2:  # Need at least 2 tokens for bigrams\n",
    "                doc_key = f\"doc_{doc_id:05d}\"  # Format as doc_00001\n",
    "                \n",
    "                # Store document info\n",
    "                self.documents[doc_key] = {\n",
    "                    'name': doc_name,\n",
    "                    'token_count': token_count\n",
    "                }\n",
    "                \n",
    "                # Store tokens\n",
    "                self.doc_tokens[doc_key] = tokens\n",
    "                \n",
    "                # Extract N-grams for each size (2 to max_ngram_size)\n",
    "                for n in range(2, self.max_ngram_size + 1):\n",
    "                    if len(tokens) >= n:  # Only extract if document has enough tokens\n",
    "                        ngrams = self.extract_ngrams(tokens, n)\n",
    "                        \n",
    "                        # Update statistics\n",
    "                        self.stats['total_ngrams'][n] += len(ngrams)\n",
    "                        \n",
    "                        # Add to index\n",
    "                        positions = defaultdict(list)\n",
    "                        for ngram_text, position in ngrams:\n",
    "                            # Add to ngram index\n",
    "                            self.ngram_index[n][ngram_text].add(doc_key)\n",
    "                            \n",
    "                            # Store position\n",
    "                            positions[ngram_text].append(position)\n",
    "                        \n",
    "                        # Store positions\n",
    "                        if positions:\n",
    "                            self.position_index[n][doc_key] = positions\n",
    "        \n",
    "        # Update statistics\n",
    "        self.stats['total_documents'] = len(self.documents)\n",
    "        \n",
    "        # Calculate unique N-grams\n",
    "        for n in range(2, self.max_ngram_size + 1):\n",
    "            if n in self.ngram_index:\n",
    "                self.stats['unique_ngrams'][n] = len(self.ngram_index[n])\n",
    "                if self.stats['total_documents'] > 0:\n",
    "                    self.stats['avg_ngrams_per_doc'][n] = self.stats['total_ngrams'][n] / self.stats['total_documents']\n",
    "        \n",
    "        # Save index\n",
    "        self.save_index()\n",
    "        \n",
    "        print(f\"\\n‚úÖ N-gram index built successfully!\")\n",
    "        print(f\"   Documents indexed: {self.stats['total_documents']:,}\")\n",
    "        print(f\"   Document tokens file: {doc_tokens_file}\")\n",
    "        \n",
    "        for n in range(2, self.max_ngram_size + 1):\n",
    "            if n in self.stats['unique_ngrams']:\n",
    "                print(f\"   {n}-grams: {self.stats['unique_ngrams'][n]:,} unique, \"\n",
    "                      f\"{self.stats['avg_ngrams_per_doc'][n]:.1f} avg per doc\")\n",
    "        \n",
    "        # Show some sample N-grams\n",
    "        print(f\"\\nüìä Sample N-grams in index:\")\n",
    "        for n in range(2, min(4, self.max_ngram_size + 1)):  # Show up to trigrams\n",
    "            if n in self.ngram_index and self.ngram_index[n]:\n",
    "                sample_ngrams = list(self.ngram_index[n].keys())[:5]\n",
    "                print(f\"   {n}-grams: {', '.join(sample_ngrams)}...\")\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def save_index(self):\n",
    "        \"\"\"Save index to disk\"\"\"\n",
    "        # Convert sets to lists for serialization\n",
    "        index_data = {\n",
    "            'documents': self.documents,\n",
    "            'doc_tokens': self.doc_tokens,\n",
    "            'ngram_index': {\n",
    "                n: {k: list(v) for k, v in index.items()}\n",
    "                for n, index in self.ngram_index.items()\n",
    "            },\n",
    "            'position_index': {\n",
    "                n: {\n",
    "                    doc_id: dict(positions)\n",
    "                    for doc_id, positions in index.items()\n",
    "                }\n",
    "                for n, index in self.position_index.items()\n",
    "            },\n",
    "            'stats': self.stats,\n",
    "            'max_ngram_size': self.max_ngram_size\n",
    "        }\n",
    "        \n",
    "        index_file = os.path.join(self.index_folder, \"ngram_index.pkl\")\n",
    "        with open(index_file, 'wb') as f:\n",
    "            pickle.dump(index_data, f)\n",
    "        \n",
    "        # Also save a human-readable version\n",
    "        readable_file = os.path.join(self.index_folder, \"index_stats.json\")\n",
    "        readable_data = {\n",
    "            'stats': self.stats,\n",
    "            'max_ngram_size': self.max_ngram_size,\n",
    "            'sample_ngrams': {\n",
    "                n: list(self.ngram_index[n].keys())[:20] if n in self.ngram_index else []\n",
    "                for n in range(2, self.max_ngram_size + 1)\n",
    "            }\n",
    "        }\n",
    "        with open(readable_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(readable_data, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"\\nüíæ Index saved to: {index_file}\")\n",
    "        print(f\"üìä Statistics saved to: {readable_file}\")\n",
    "    \n",
    "    def load_index(self):\n",
    "        \"\"\"Load index from disk\"\"\"\n",
    "        index_file = os.path.join(self.index_folder, \"ngram_index.pkl\")\n",
    "        \n",
    "        if not os.path.exists(index_file):\n",
    "            print(f\"‚ùå Index not found at: {index_file}\")\n",
    "            print(\"Building new index...\")\n",
    "            return self.build_index()\n",
    "        \n",
    "        try:\n",
    "            print(f\"üìÇ Loading index from: {index_file}\")\n",
    "            with open(index_file, 'rb') as f:\n",
    "                index_data = pickle.load(f)\n",
    "            \n",
    "            self.documents = index_data['documents']\n",
    "            self.doc_tokens = index_data['doc_tokens']\n",
    "            self.max_ngram_size = index_data.get('max_ngram_size', 5)\n",
    "            self.stats = index_data['stats']\n",
    "            \n",
    "            # Reconstruct ngram_index\n",
    "            self.ngram_index = defaultdict(lambda: defaultdict(set))\n",
    "            for n, index in index_data['ngram_index'].items():\n",
    "                n_int = int(n)\n",
    "                for ngram, doc_ids in index.items():\n",
    "                    self.ngram_index[n_int][ngram] = set(doc_ids)\n",
    "            \n",
    "            # Reconstruct position_index\n",
    "            self.position_index = defaultdict(lambda: defaultdict(dict))\n",
    "            for n, index in index_data['position_index'].items():\n",
    "                n_int = int(n)\n",
    "                for doc_id, positions in index.items():\n",
    "                    self.position_index[n_int][doc_id] = positions\n",
    "            \n",
    "            print(f\"‚úÖ N-gram index loaded successfully!\")\n",
    "            print(f\"   Documents: {self.stats['total_documents']:,}\")\n",
    "            print(f\"   Max N-gram size: {self.max_ngram_size}\")\n",
    "            \n",
    "            for n in range(2, self.max_ngram_size + 1):\n",
    "                if n in self.stats['unique_ngrams']:\n",
    "                    print(f\"   {n}-grams: {self.stats['unique_ngrams'][n]:,} unique\")\n",
    "            \n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading index: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return self.build_index()\n",
    "    \n",
    "    def find_best_ngram_size(self, query_tokens: List[str]) -> int:\n",
    "        \"\"\"\n",
    "        Find the best N-gram size for the query\n",
    "        \n",
    "        Returns the largest N where:\n",
    "        1. N <= query length\n",
    "        2. N <= max_ngram_size\n",
    "        3. There are documents containing the N-grams\n",
    "        \"\"\"\n",
    "        query_length = len(query_tokens)\n",
    "        \n",
    "        # Try from largest to smallest for better precision\n",
    "        for n in range(min(self.max_ngram_size, query_length), 1, -1):\n",
    "            if query_length >= n:\n",
    "                # Check if any documents contain the first N-gram\n",
    "                first_ngram = \" \".join(query_tokens[:n])\n",
    "                if first_ngram in self.ngram_index[n]:\n",
    "                    return n\n",
    "        \n",
    "        # Fall back to bigram if nothing else works\n",
    "        return 2\n",
    "    \n",
    "    def exact_phrase_search(self, phrase: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Exact phrase search using optimal N-gram size\n",
    "        \n",
    "        Args:\n",
    "            phrase: Search phrase (e.g., \"supreme court of pakistan\")\n",
    "        \n",
    "        Returns:\n",
    "            List of matching documents with context\n",
    "        \"\"\"\n",
    "        if not phrase or not phrase.strip():\n",
    "            print(\"‚ùå Empty query\")\n",
    "            return []\n",
    "        \n",
    "        # Clean and tokenize the query\n",
    "        query_tokens = phrase.lower().strip().split()\n",
    "        query_length = len(query_tokens)\n",
    "        \n",
    "        if query_length < 2:\n",
    "            print(\"‚ùå Phrase must contain at least 2 words\")\n",
    "            return []\n",
    "        \n",
    "        print(f\"\\nüîç Searching for phrase: '{phrase}'\")\n",
    "        print(f\"   Query tokens ({query_length}): {query_tokens}\")\n",
    "        \n",
    "        # Find optimal N-gram size\n",
    "        optimal_n = self.find_best_ngram_size(query_tokens)\n",
    "        print(f\"   Using {optimal_n}-gram matching\")\n",
    "        \n",
    "        # Extract query N-grams\n",
    "        query_ngrams = self.extract_ngrams(query_tokens, optimal_n)\n",
    "        query_ngram_texts = [ngram for ngram, _ in query_ngrams]\n",
    "        \n",
    "        print(f\"   Query {optimal_n}-grams ({len(query_ngrams)}): {query_ngram_texts[:5]}\")\n",
    "        if len(query_ngrams) > 5:\n",
    "            print(f\"   ... and {len(query_ngrams) - 5} more\")\n",
    "        \n",
    "        # Step 1: Find candidate documents containing ALL query N-grams\n",
    "        candidate_docs = None\n",
    "        ngram_doc_counts = []\n",
    "        \n",
    "        for ngram_text, _ in query_ngrams:\n",
    "            docs_with_ngram = self.ngram_index[optimal_n].get(ngram_text, set())\n",
    "            ngram_doc_counts.append(len(docs_with_ngram))\n",
    "            \n",
    "            if candidate_docs is None:\n",
    "                candidate_docs = docs_with_ngram.copy()\n",
    "            else:\n",
    "                candidate_docs = candidate_docs.intersection(docs_with_ngram)\n",
    "            \n",
    "            if not candidate_docs:\n",
    "                print(f\"   ‚úó No documents contain all {optimal_n}-grams\")\n",
    "                print(f\"   N-gram frequency: {ngram_doc_counts}\")\n",
    "                return []\n",
    "        \n",
    "        print(f\"   ‚úì Candidate documents: {len(candidate_docs)}\")\n",
    "        print(f\"   N-gram frequencies: {ngram_doc_counts}\")\n",
    "        \n",
    "        # Step 2: Verify phrase occurrence in candidate documents\n",
    "        results = []\n",
    "        for doc_id in candidate_docs:\n",
    "            doc_info = self.documents[doc_id]\n",
    "            tokens = self.doc_tokens[doc_id]\n",
    "            \n",
    "            # Find all positions where the full phrase occurs\n",
    "            phrase_positions = self.find_phrase_positions_direct(\n",
    "                tokens, query_tokens\n",
    "            )\n",
    "            \n",
    "            if phrase_positions:\n",
    "                # Calculate match quality score\n",
    "                match_score = self.calculate_match_score(\n",
    "                    phrase_positions, query_length, len(tokens)\n",
    "                )\n",
    "                \n",
    "                # Get context around matches\n",
    "                contexts = []\n",
    "                for pos in phrase_positions[:2]:  # Limit to first 2 matches\n",
    "                    context = self.get_context(tokens, pos, query_length)\n",
    "                    contexts.append(context)\n",
    "                \n",
    "                results.append({\n",
    "                    'doc_id': doc_id,\n",
    "                    'name': doc_info['name'],\n",
    "                    'token_count': doc_info['token_count'],\n",
    "                    'match_score': match_score,\n",
    "                    'match_count': len(phrase_positions),\n",
    "                    'positions': phrase_positions[:3],  # First 3 positions\n",
    "                    'contexts': contexts,\n",
    "                    'matched_phrase': phrase\n",
    "                })\n",
    "        \n",
    "        # Sort by match score (descending)\n",
    "        results.sort(key=lambda x: x['match_score'], reverse=True)\n",
    "        \n",
    "        print(f\"   ‚úì Documents with exact phrase: {len(results)}\")\n",
    "        return results\n",
    "    \n",
    "    def find_phrase_positions_direct(self, tokens: List[str], \n",
    "                                    query_tokens: List[str]) -> List[int]:\n",
    "        \"\"\"\n",
    "        Direct search for phrase positions (slower but accurate)\n",
    "        \n",
    "        Returns:\n",
    "            List of start positions where phrase occurs\n",
    "        \"\"\"\n",
    "        phrase_text = \" \".join(query_tokens)\n",
    "        tokens_text = \" \".join(tokens)\n",
    "        \n",
    "        # Simple but effective: find all occurrences\n",
    "        positions = []\n",
    "        query_length = len(query_tokens)\n",
    "        \n",
    "        for i in range(len(tokens) - query_length + 1):\n",
    "            if tokens[i:i+query_length] == query_tokens:\n",
    "                positions.append(i)\n",
    "        \n",
    "        return positions\n",
    "    \n",
    "    def calculate_match_score(self, positions: List[int], \n",
    "                            query_length: int, doc_length: int) -> float:\n",
    "        \"\"\"\n",
    "        Calculate match quality score\n",
    "        \n",
    "        Higher score for:\n",
    "        1. More occurrences of the phrase\n",
    "        2. Shorter documents (relative importance)\n",
    "        3. Phrases in document beginning (higher relevance)\n",
    "        \"\"\"\n",
    "        if not positions or doc_length == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        # Base score based on number of occurrences\n",
    "        occurrence_score = min(len(positions) * 15, 60)\n",
    "        \n",
    "        # Position score (earlier occurrences are better)\n",
    "        position_score = 0\n",
    "        for pos in positions[:3]:  # Consider first 3 occurrences\n",
    "            # Normalize position (0 = beginning, 1 = end)\n",
    "            normalized_pos = 1.0 - (pos / max(doc_length, 1))\n",
    "            position_score += normalized_pos * 15\n",
    "        \n",
    "        # Density score (phrase density in document)\n",
    "        density = (len(positions) * query_length) / max(doc_length, 1)\n",
    "        density_score = min(density * 120, 40)\n",
    "        \n",
    "        # Combine scores\n",
    "        total_score = occurrence_score + position_score + density_score\n",
    "        \n",
    "        # Normalize to 0-100 scale\n",
    "        return min(total_score, 100.0)\n",
    "    \n",
    "    def get_context(self, tokens: List[str], position: int, \n",
    "                   phrase_length: int, window: int = 7) -> str:\n",
    "        \"\"\"Get context around the phrase match\"\"\"\n",
    "        start = max(0, position - window)\n",
    "        end = min(len(tokens), position + phrase_length + window)\n",
    "        \n",
    "        # Build context with highlighting\n",
    "        context_parts = []\n",
    "        for i in range(start, end):\n",
    "            if i == position:\n",
    "                context_parts.append(\"[\")\n",
    "            context_parts.append(tokens[i])\n",
    "            if i == position + phrase_length - 1:\n",
    "                context_parts.append(\"]\")\n",
    "        \n",
    "        return \" \".join(context_parts)\n",
    "    \n",
    "    def multi_size_ngram_search(self, phrase: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Search using multiple N-gram sizes for better recall\n",
    "        \n",
    "        Args:\n",
    "            phrase: Search phrase\n",
    "        \n",
    "        Returns:\n",
    "            Combined results from different N-gram sizes\n",
    "        \"\"\"\n",
    "        query_tokens = phrase.lower().strip().split()\n",
    "        query_length = len(query_tokens)\n",
    "        \n",
    "        if query_length < 2:\n",
    "            return []\n",
    "        \n",
    "        print(f\"\\nüîç Multi-size N-gram search: '{phrase}'\")\n",
    "        \n",
    "        all_results = []\n",
    "        \n",
    "        # Try different N-gram sizes\n",
    "        for n in range(min(self.max_ngram_size, query_length), 1, -1):\n",
    "            if query_length >= n:\n",
    "                print(f\"   Trying {n}-gram search...\")\n",
    "                \n",
    "                # Extract N-grams\n",
    "                ngrams = self.extract_ngrams(query_tokens, n)\n",
    "                ngram_texts = [ngram for ngram, _ in ngrams]\n",
    "                \n",
    "                # Find documents containing any of these N-grams\n",
    "                docs_found = set()\n",
    "                for ngram_text in ngram_texts:\n",
    "                    docs = self.ngram_index[n].get(ngram_text, set())\n",
    "                    docs_found.update(docs)\n",
    "                \n",
    "                if docs_found:\n",
    "                    print(f\"   ‚úì Found {len(docs_found)} documents with {n}-grams\")\n",
    "                    \n",
    "                    # Score documents based on N-gram coverage\n",
    "                    for doc_id in docs_found:\n",
    "                        doc_info = self.documents[doc_id]\n",
    "                        \n",
    "                        # Count how many query N-grams appear in document\n",
    "                        ngram_count = 0\n",
    "                        for ngram_text in ngram_texts:\n",
    "                            if doc_id in self.ngram_index[n].get(ngram_text, set()):\n",
    "                                ngram_count += 1\n",
    "                        \n",
    "                        # Calculate coverage score\n",
    "                        coverage = ngram_count / len(ngram_texts)\n",
    "                        score = coverage * 100 * (n / query_length)  # Weight by N-gram size\n",
    "                        \n",
    "                        # Check for exact phrase\n",
    "                        tokens = self.doc_tokens[doc_id]\n",
    "                        exact_positions = self.find_phrase_positions_direct(\n",
    "                            tokens, query_tokens\n",
    "                        )\n",
    "                        \n",
    "                        if exact_positions:\n",
    "                            score *= 1.5  # Bonus for exact match\n",
    "                        \n",
    "                        all_results.append({\n",
    "                            'doc_id': doc_id,\n",
    "                            'name': doc_info['name'],\n",
    "                            'token_count': doc_info['token_count'],\n",
    "                            'match_score': score,\n",
    "                            'ngram_size': n,\n",
    "                            'coverage': coverage,\n",
    "                            'exact_match': len(exact_positions) > 0,\n",
    "                            'match_count': len(exact_positions) if exact_positions else 0\n",
    "                        })\n",
    "                else:\n",
    "                    print(f\"   ‚úó No documents found with {n}-grams\")\n",
    "        \n",
    "        # Remove duplicates and sort\n",
    "        unique_results = {}\n",
    "        for result in all_results:\n",
    "            doc_id = result['doc_id']\n",
    "            if doc_id not in unique_results or result['match_score'] > unique_results[doc_id]['match_score']:\n",
    "                unique_results[doc_id] = result\n",
    "        \n",
    "        final_results = list(unique_results.values())\n",
    "        final_results.sort(key=lambda x: x['match_score'], reverse=True)\n",
    "        \n",
    "        print(f\"   ‚úì Total unique documents: {len(final_results)}\")\n",
    "        return final_results\n",
    "    \n",
    "    def search_with_suggestions(self, phrase: str) -> Tuple[List[Dict], List[str]]:\n",
    "        \"\"\"\n",
    "        Search with spelling suggestions and query expansion\n",
    "        \n",
    "        Returns:\n",
    "            (results, suggestions)\n",
    "        \"\"\"\n",
    "        original_phrase = phrase\n",
    "        query_tokens = phrase.lower().strip().split()\n",
    "        \n",
    "        # First try exact search\n",
    "        results = self.exact_phrase_search(phrase)\n",
    "        \n",
    "        if results:\n",
    "            return results, [\"Exact phrase match found\"]\n",
    "        \n",
    "        # If no results, try multi-size search\n",
    "        print(\"\\nü§î No exact matches found, trying multi-size N-gram search...\")\n",
    "        results = self.multi_size_ngram_search(phrase)\n",
    "        \n",
    "        suggestions = []\n",
    "        \n",
    "        if not results:\n",
    "            # Generate suggestions\n",
    "            suggestions.append(\"No documents found. Try:\")\n",
    "            suggestions.append(\"1. Check spelling of your phrase\")\n",
    "            suggestions.append(\"2. Try a shorter phrase\")\n",
    "            suggestions.append(\"3. Try individual important words\")\n",
    "            \n",
    "            # Suggest similar phrases based on common N-grams\n",
    "            if len(query_tokens) >= 2:\n",
    "                # Look for documents containing parts of the phrase\n",
    "                for n in range(2, min(4, len(query_tokens) + 1)):\n",
    "                    ngrams = self.extract_ngrams(query_tokens, n)\n",
    "                    for ngram_text, _ in ngrams[:3]:  # Check first 3 N-grams\n",
    "                        if ngram_text in self.ngram_index[n]:\n",
    "                            docs_count = len(self.ngram_index[n][ngram_text])\n",
    "                            suggestions.append(f\"Found '{ngram_text}' in {docs_count} documents\")\n",
    "        \n",
    "        return results, suggestions\n",
    "    \n",
    "    def show_document_ngrams(self, doc_name: str, n: int = 3, limit: int = 20):\n",
    "        \"\"\"Show N-grams for a specific document\"\"\"\n",
    "        # Find document\n",
    "        doc_id = None\n",
    "        for d_id, info in self.documents.items():\n",
    "            if info['name'] == doc_name:\n",
    "                doc_id = d_id\n",
    "                break\n",
    "        \n",
    "        if not doc_id:\n",
    "            print(f\"‚ùå Document not found: {doc_name}\")\n",
    "            return\n",
    "        \n",
    "        if n not in self.position_index or doc_id not in self.position_index[n]:\n",
    "            print(f\"‚ùå No {n}-grams found for document {doc_name}\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\nüî§ {n}-GRAMS IN DOCUMENT: {doc_name}\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        ngrams_info = self.position_index[n][doc_id]\n",
    "        sorted_ngrams = sorted(ngrams_info.items(), key=lambda x: len(x[1]), reverse=True)\n",
    "        \n",
    "        print(f\"Total {n}-grams: {len(ngrams_info)}\\n\")\n",
    "        \n",
    "        for i, (ngram, positions) in enumerate(sorted_ngrams[:limit], 1):\n",
    "            freq = len(positions)\n",
    "            sample_positions = positions[:3] if len(positions) > 3 else positions\n",
    "            print(f\"{i:3d}. {ngram}\")\n",
    "            print(f\"     Frequency: {freq}, Positions: {sample_positions}\")\n",
    "            if len(positions) > 3:\n",
    "                print(f\"     ... and {len(positions) - 3} more positions\")\n",
    "            print()\n",
    "        \n",
    "        if len(sorted_ngrams) > limit:\n",
    "            print(f\"... and {len(sorted_ngrams) - limit} more {n}-grams\")\n",
    "        \n",
    "        print(\"=\" * 80)\n",
    "    \n",
    "    def interactive_search(self):\n",
    "        \"\"\"Interactive N-gram search interface\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"üî§ N-GRAM PHRASE SEARCH SYSTEM\")\n",
    "        print(f\"   Max N-gram size: {self.max_ngram_size}\")\n",
    "        print(\"=\" * 80)\n",
    "        print(\"\\nüìã Available Commands:\")\n",
    "        print(\"  ‚Ä¢ search <phrase>     - Exact phrase search\")\n",
    "        print(\"  ‚Ä¢ multi <phrase>      - Multi-size N-gram search\")\n",
    "        print(\"  ‚Ä¢ smart <phrase>      - Smart search with suggestions\")\n",
    "        print(\"  ‚Ä¢ ngrams <doc> <n>    - Show N-grams for document\")\n",
    "        print(\"  ‚Ä¢ stats               - Show index statistics\")\n",
    "        print(\"  ‚Ä¢ example             - Show example searches\")\n",
    "        print(\"  ‚Ä¢ quit                - Exit\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        while True:\n",
    "            user_input = input(\"\\nüéØ Enter command: \").strip()\n",
    "            \n",
    "            if not user_input:\n",
    "                continue\n",
    "            \n",
    "            if user_input.lower() == 'quit':\n",
    "                print(\"üëã Goodbye!\")\n",
    "                break\n",
    "            \n",
    "            elif user_input.lower() == 'stats':\n",
    "                self.show_statistics()\n",
    "            \n",
    "            elif user_input.lower() == 'example':\n",
    "                self.show_examples()\n",
    "            \n",
    "            elif user_input.lower().startswith('search '):\n",
    "                phrase = user_input[7:].strip()\n",
    "                if phrase:\n",
    "                    results = self.exact_phrase_search(phrase)\n",
    "                    self.display_results(results, f\"Exact phrase: '{phrase}'\")\n",
    "                else:\n",
    "                    print(\"‚ùå Please enter a search phrase\")\n",
    "            \n",
    "            elif user_input.lower().startswith('multi '):\n",
    "                phrase = user_input[6:].strip()\n",
    "                if phrase:\n",
    "                    results = self.multi_size_ngram_search(phrase)\n",
    "                    self.display_results(results, f\"Multi-size: '{phrase}'\")\n",
    "                else:\n",
    "                    print(\"‚ùå Please enter a search phrase\")\n",
    "            \n",
    "            elif user_input.lower().startswith('smart '):\n",
    "                phrase = user_input[6:].strip()\n",
    "                if phrase:\n",
    "                    results, suggestions = self.search_with_suggestions(phrase)\n",
    "                    if results:\n",
    "                        self.display_results(results, f\"Smart search: '{phrase}'\")\n",
    "                    else:\n",
    "                        print(f\"\\n‚ùå No results found for '{phrase}'\")\n",
    "                        if suggestions:\n",
    "                            print(\"\\nüí° Suggestions:\")\n",
    "                            for suggestion in suggestions:\n",
    "                                print(f\"   ‚Ä¢ {suggestion}\")\n",
    "                else:\n",
    "                    print(\"‚ùå Please enter a search phrase\")\n",
    "            \n",
    "            elif user_input.lower().startswith('ngrams '):\n",
    "                parts = user_input[7:].strip().split()\n",
    "                if len(parts) >= 1:\n",
    "                    doc_name = parts[0]\n",
    "                    n = int(parts[1]) if len(parts) > 1 else 3\n",
    "                    if 2 <= n <= self.max_ngram_size:\n",
    "                        self.show_document_ngrams(doc_name, n)\n",
    "                    else:\n",
    "                        print(f\"‚ùå N must be between 2 and {self.max_ngram_size}\")\n",
    "                else:\n",
    "                    print(\"‚ùå Usage: ngrams <document_name> [n]\")\n",
    "            \n",
    "            else:\n",
    "                # Default to smart search\n",
    "                results, suggestions = self.search_with_suggestions(user_input)\n",
    "                if results:\n",
    "                    self.display_results(results, f\"Search: '{user_input}'\")\n",
    "                else:\n",
    "                    print(f\"\\n‚ùå No results found for '{user_input}'\")\n",
    "                    if suggestions:\n",
    "                        print(\"\\nüí° Suggestions:\")\n",
    "                        for suggestion in suggestions:\n",
    "                            print(f\"   ‚Ä¢ {suggestion}\")\n",
    "    \n",
    "    def show_statistics(self):\n",
    "        \"\"\"Show detailed index statistics\"\"\"\n",
    "        print(f\"\\nüìä N-GRAM INDEX STATISTICS\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"Documents indexed: {self.stats['total_documents']:,}\")\n",
    "        print(f\"Max N-gram size: {self.max_ngram_size}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for n in range(2, self.max_ngram_size + 1):\n",
    "            if n in self.stats['unique_ngrams']:\n",
    "                unique = self.stats['unique_ngrams'][n]\n",
    "                total = self.stats['total_ngrams'][n]\n",
    "                avg = self.stats['avg_ngrams_per_doc'][n]\n",
    "                print(f\"{n}-grams: {unique:,} unique, {total:,} total, {avg:.1f} avg/doc\")\n",
    "        \n",
    "        # Show sample of most common N-grams\n",
    "        print(\"-\" * 80)\n",
    "        print(\"üìà MOST COMMON N-GRAMS:\")\n",
    "        \n",
    "        for n in range(2, min(4, self.max_ngram_size + 1)):\n",
    "            if n in self.ngram_index and self.ngram_index[n]:\n",
    "                # Get N-grams sorted by document frequency\n",
    "                ngram_freqs = [(ngram, len(docs)) for ngram, docs in self.ngram_index[n].items()]\n",
    "                ngram_freqs.sort(key=lambda x: x[1], reverse=True)\n",
    "                \n",
    "                print(f\"\\nTop {n}-grams:\")\n",
    "                for i, (ngram, freq) in enumerate(ngram_freqs[:10], 1):\n",
    "                    percentage = (freq / self.stats['total_documents']) * 100\n",
    "                    print(f\"  {i:2d}. {ngram:<30s} {freq:4d} docs ({percentage:.1f}%)\")\n",
    "        \n",
    "        print(\"=\" * 80)\n",
    "    \n",
    "    def show_examples(self):\n",
    "        \"\"\"Show example searches\"\"\"\n",
    "        print(\"\\nüìù EXAMPLE SEARCHES\")\n",
    "        print(\"=\" * 80)\n",
    "        print(\"\\nShort phrases (2-3 words):\")\n",
    "        print(\"  ‚Ä¢ supreme court\")\n",
    "        print(\"  ‚Ä¢ murder evidence\")\n",
    "        print(\"  ‚Ä¢ court appeal\")\n",
    "        print(\"  ‚Ä¢ criminal case\")\n",
    "        \n",
    "        print(\"\\nMedium phrases (4-5 words):\")\n",
    "        print(\"  ‚Ä¢ supreme court of pakistan\")\n",
    "        print(\"  ‚Ä¢ high court appeal case\")\n",
    "        print(\"  ‚Ä¢ evidence of murder weapon\")\n",
    "        print(\"  ‚Ä¢ constitutional right violation\")\n",
    "        \n",
    "        print(\"\\nLonger phrases:\")\n",
    "        print(\"  ‚Ä¢ in the matter of criminal appeal\")\n",
    "        print(\"  ‚Ä¢ court of appeal decision\")\n",
    "        print(\"  ‚Ä¢ murder case evidence hearing\")\n",
    "        \n",
    "        print(\"\\nüí° Tips:\")\n",
    "        print(\"  1. Use exact phrases for precise matching\")\n",
    "        print(\"  2. Longer phrases give more specific results\")\n",
    "        print(\"  3. Try different phrase lengths if no results\")\n",
    "        print(\"=\" * 80)\n",
    "    \n",
    "    def display_results(self, results: List[Dict], title: str):\n",
    "        \"\"\"Display search results\"\"\"\n",
    "        if not results:\n",
    "            print(f\"\\n‚ùå No results found\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\n‚úÖ {title}\")\n",
    "        print(f\"üìä Found {len(results)} document(s)\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        for i, result in enumerate(results[:15], 1):  # Show first 15\n",
    "            print(f\"\\n{i:2d}. üìÑ {result['name']}\")\n",
    "            print(f\"    üìè Length: {result['token_count']:,} tokens\")\n",
    "            print(f\"    ‚≠ê Score: {result['match_score']:.1f}\")\n",
    "            \n",
    "            if 'match_count' in result and result['match_count'] > 0:\n",
    "                print(f\"    üîç Exact matches: {result['match_count']}\")\n",
    "            \n",
    "            if 'coverage' in result:\n",
    "                print(f\"    üìà N-gram coverage: {result['coverage']:.1%}\")\n",
    "            \n",
    "            if 'ngram_size' in result:\n",
    "                print(f\"    üî§ Best N-gram size: {result['ngram_size']}\")\n",
    "            \n",
    "            # Show context if available\n",
    "            if result.get('contexts'):\n",
    "                print(f\"    üìù Context: {result['contexts'][0]}\")\n",
    "        \n",
    "        if len(results) > 15:\n",
    "            print(f\"\\n... and {len(results) - 15} more documents\")\n",
    "        \n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # Ask for document preview\n",
    "        if results:\n",
    "            choice = input(\"\\nüìñ Preview a document? (enter number or 'n'): \").strip()\n",
    "            if choice.lower() != 'n' and choice.isdigit():\n",
    "                idx = int(choice) - 1\n",
    "                if 0 <= idx < len(results):\n",
    "                    self.show_document_preview(results[idx]['doc_id'])\n",
    "    \n",
    "    def show_document_preview(self, doc_id: str, lines: int = 15):\n",
    "        \"\"\"Show document preview\"\"\"\n",
    "        if doc_id not in self.doc_tokens:\n",
    "            print(f\"‚ùå Document not found: {doc_id}\")\n",
    "            return\n",
    "        \n",
    "        doc_info = self.documents[doc_id]\n",
    "        tokens = self.doc_tokens[doc_id]\n",
    "        \n",
    "        print(f\"\\n\" + \"=\" * 80)\n",
    "        print(f\"üìÑ DOCUMENT: {doc_info['name']}\")\n",
    "        print(f\"üìè Tokens: {doc_info['token_count']:,}\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # Show first N tokens\n",
    "        preview_tokens = tokens[:lines*10]\n",
    "        text = \" \".join(preview_tokens)\n",
    "        \n",
    "        # Wrap text\n",
    "        words = text.split()\n",
    "        current_line = []\n",
    "        line_length = 0\n",
    "        \n",
    "        print(\"\\n\" + \"-\" * 80)\n",
    "        for word in words:\n",
    "            current_line.append(word)\n",
    "            line_length += len(word) + 1\n",
    "            \n",
    "            if line_length > 70:\n",
    "                print(\" \".join(current_line))\n",
    "                current_line = []\n",
    "                line_length = 0\n",
    "        \n",
    "        if current_line:\n",
    "            print(\" \".join(current_line))\n",
    "        \n",
    "        if len(tokens) > lines * 10:\n",
    "            print(f\"\\n... and {len(tokens) - lines*10:,} more tokens\")\n",
    "        \n",
    "        print(\"-\" * 80)\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function\"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"üî§ N-GRAM PHRASE SEARCH SYSTEM (up to 5-grams)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Try to find corpus folder\n",
    "    corpus_folder = r\"C:\\Users\\Armaghan Rafique\\Desktop\\AI Project\\cleaned_corpus\"\n",
    "    \n",
    "    if not os.path.exists(corpus_folder):\n",
    "        print(f\"‚ùå Corpus folder not found: {corpus_folder}\")\n",
    "        \n",
    "        # Try alternative\n",
    "        alt_folders = [\n",
    "            r\"C:\\Users\\Armaghan Rafique\\Desktop\\AI Project\\supreme_court_judgements_txt\",\n",
    "            r\"C:\\Users\\Armaghan Rafique\\Desktop\\AI Project\",\n",
    "            os.path.join(os.path.expanduser(\"~\"), \"Desktop\", \"AI Project\", \"cleaned_corpus\")\n",
    "        ]\n",
    "        \n",
    "        found = False\n",
    "        for folder in alt_folders:\n",
    "            if os.path.exists(folder):\n",
    "                corpus_folder = folder\n",
    "                print(f\"‚úÖ Using folder: {corpus_folder}\")\n",
    "                found = True\n",
    "                break\n",
    "        \n",
    "        if not found:\n",
    "            corpus_folder = input(\"üìÅ Enter corpus folder path: \").strip()\n",
    "            if not os.path.exists(corpus_folder):\n",
    "                print(f\"‚ùå Folder does not exist: {corpus_folder}\")\n",
    "                return\n",
    "    \n",
    "    print(f\"\\nüìÅ Corpus folder: {corpus_folder}\")\n",
    "    \n",
    "    # Create N-gram search system with 5-grams\n",
    "    ngram_system = NGramSearchSystem(corpus_folder, max_ngram_size=5)\n",
    "    \n",
    "    # Load or build index\n",
    "    if not ngram_system.load_index():\n",
    "        print(\"‚ùå Failed to initialize N-gram search system\")\n",
    "        return\n",
    "    \n",
    "    # Show welcome message\n",
    "    print(f\"\\nüéØ N-GRAM SEARCH READY\")\n",
    "    print(f\"   Documents: {ngram_system.stats['total_documents']:,}\")\n",
    "    print(f\"   N-gram sizes: 2 to {ngram_system.max_ngram_size}\")\n",
    "    \n",
    "    # Start interactive search\n",
    "    ngram_system.interactive_search()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94a4f51-8aca-4e4c-b4e8-f61cf87ce30d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
